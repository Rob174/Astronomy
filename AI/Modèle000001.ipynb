{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modèle000001.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "R4se1VpyGHLS"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rob174/Astronomy/blob/Astronomy/AI/Mod%C3%A8le000001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k_hAT1OO2lUN"
      },
      "source": [
        "# Fonctions de base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "daqyahLS1u1Z",
        "outputId": "03653c4e-756b-429c-b02c-b8a1d7f6aad4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/My Drive/TIPE'\n",
        "import os\n",
        "from tensorflow.python.client import device_lib\n",
        "print(\"Utilise le\",str(device_lib.list_local_devices()[0])[15:18])\n",
        "print()\n",
        "print()\n",
        "from google.colab import files\n",
        "import tensorflow as tf\n",
        "from tensorflow.python import debug as tf_debug\n",
        "\n",
        "from keras.layers import Layer\n",
        "import matplotlib.gridspec as gridspec\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,concatenate,Subtract,Multiply,Average, Concatenate,Reshape, Add, BatchNormalization\n",
        "from keras.layers import Reshape,Lambda\n",
        "from keras.layers.core import Activation\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras import models\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import UpSampling2D\n",
        "from keras.layers.convolutional import Convolution2D, AveragePooling2D,MaxPooling2D\n",
        "import keras.losses\n",
        "from keras.layers.core import Flatten\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "images = [\"Galaxies_resized/\"+f for f in os.listdir(\"Galaxies_resized/\")]\n",
        "noises = []"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/TIPE\n",
            "Utilise le CPU\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lyewhn0TDz-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install latest Tensorflow build\n",
        "# !pip install -q tf-nightly-2.0-preview\n",
        "# from tensorflow import summary\n",
        "# %load_ext notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUwUDspDemoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "# !unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYmeFw_vetQr",
        "colab_type": "code",
        "outputId": "a6054772-1d0c-4e79-ec68-9bf26f250f77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/usr/lib/python3.6/json/__init__.py\", line 299, in load\n",
            "    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n",
            "  File \"/usr/lib/python3.6/json/__init__.py\", line 354, in loads\n",
            "    return _default_decoder.decode(s)\n",
            "  File \"/usr/lib/python3.6/json/decoder.py\", line 339, in decode\n",
            "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
            "  File \"/usr/lib/python3.6/json/decoder.py\", line 357, in raw_decode\n",
            "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
            "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fxnCGK3I2hpP",
        "colab": {}
      },
      "source": [
        "def next_batch(batch_size, images,tailleAttendue,formatArray):#ATTENTION : pr tenter d'améliorer l'apprentissage, on augmente la taille minimale d'image prise\n",
        "    \"\"\"\n",
        "    formatArray : format de sortie des données de l'image ; utiliser numpy\n",
        "    \"\"\"\n",
        "    imageEntreeTensor = []\n",
        "    imageSortieTensor = []\n",
        "    while len(imageEntreeTensor) < batch_size:\n",
        "        try:\n",
        "            np.random.shuffle(images)#choix aléatoire de l'image\n",
        "            image = cv2.imread(images[0])#Ouvre en rgb l'image nettoyée\n",
        "            resizedImage = cv2.resize(image,(tailleAttendue,tailleAttendue))\n",
        "            imageSortieTensor.append(np.array(resizedImage,dtype=formatArray))\n",
        "            imageEntreeTensor.append(np.array(resizedImage,dtype=formatArray))\n",
        "        except:\n",
        "            print(\"Error in next_batch\")\n",
        "    imageEntreeTensor = np.array(imageEntreeTensor,formatArray)\n",
        "    return [imageEntreeTensor,imageEntreeTensor]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "49nICh8X2kOb",
        "colab": {}
      },
      "source": [
        "def next_batch_bruit_voile(batch_size, images,tailleAttendue,formatArray,facteursVoile,bruitParam,plageVal=[0,255]):#ATTENTION : pr tenter d'améliorer l'apprentissage, on augmente la taille minimale d'image prise\n",
        "    \"\"\"\n",
        "    formatArray : format de sortie des données de l'image ; utiliser numpy\n",
        "    facteurVoile : liste de valeur entre 0 et 1 contenant l'atténuation pour chaque couche de l'image\n",
        "    bruitParam : liste avec dans l'ordre moyenne et écart type\n",
        "    \"\"\"\n",
        "    assert plageVal[0] <= plageVal[1]\n",
        "    assert plageVal[0] <= bruitParam[0] <= plageVal[1]\n",
        "    assert plageVal[0] <= bruitParam[0]-bruitParam[1] <= plageVal[1]\n",
        "    assert plageVal[0] <= bruitParam[0]+bruitParam[1] <= plageVal[1]\n",
        "    \n",
        "    imageEntreeTensor,imageSortieTensor = next_batch(batch_size,  images,tailleAttendue,formatArray)\n",
        "    imageSortieTensorCopy = np.array(imageSortieTensor,dtype=np.float32)\n",
        "    for image in range(imageSortieTensorCopy.shape[0]):\n",
        "        for rgbIndex in range(3):\n",
        "            imageSortieTensorCopy[image,:,:,rgbIndex] *= facteursVoile[rgbIndex]\n",
        "    imageSortieTensorCopy = np.clip(imageSortieTensorCopy + np.random.normal(bruitParam[0],bruitParam[1],imageSortieTensorCopy.shape),plageVal[0],plageVal[1])\n",
        "    return [imageEntreeTensor,np.array(imageSortieTensorCopy,dtype=formatArray)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KZrLh69g3Eel",
        "colab": {}
      },
      "source": [
        "def next_batch_bruit_voile_2(batch_size, images,tailleAttendue,formatArray,facteursVoile,bruitParam,plageVal=[0,255]):#ATTENTION : pr tenter d'améliorer l'apprentissage, on augmente la taille minimale d'image prise\n",
        "    \"\"\"\n",
        "    La versison 2 fait les  modification sélectives de couleurs après avoir ajouté le bruit\n",
        "    formatArray : format de sortie des données de l'image ; utiliser numpy\n",
        "    facteurVoile : liste de valeur entre 0 et 1 contenant l'atténuation pour chaque couche de l'image\n",
        "    bruitParam : liste avec dans l'ordre moyenne et écart type\n",
        "    \"\"\"\n",
        "    assert plageVal[0] <= plageVal[1]\n",
        "    assert plageVal[0] <= bruitParam[0] <= plageVal[1]\n",
        "    assert plageVal[0] <= bruitParam[0]-bruitParam[1] <= plageVal[1]\n",
        "    assert plageVal[0] <= bruitParam[0]+bruitParam[1] <= plageVal[1]\n",
        "    \n",
        "    imageEntreeTensor,imageSortieTensor = next_batch(batch_size,  images,tailleAttendue,formatArray)\n",
        "    imageSortieTensorCopy = np.array(imageSortieTensor,dtype=np.float32)\n",
        "    for image in range(imageSortieTensorCopy.shape[0]):\n",
        "        for rgbIndex in range(3):\n",
        "            imageSortieTensorCopy[image,:,:,rgbIndex] += np.random.normal(bruitParam[0],bruitParam[1])\n",
        "            imageSortieTensorCopy[image,:,:,rgbIndex] *= facteursVoile[rgbIndex]\n",
        "    imageSortieTensorCopy /= np.max(imageSortieTensorCopy)\n",
        "    imageSortieTensorCopy *= plageVal[1]\n",
        "    imageSortieTensorCopy = np.clip(imageSortieTensorCopy,plageVal[0],plageVal[1])\n",
        "    return [imageEntreeTensor,np.array(imageSortieTensorCopy,dtype=formatArray)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6nuvaDIBDp3G",
        "colab": {}
      },
      "source": [
        "def normalisation(arrayL,plageEntree=[0,255],plageSortie=[0,1]):\n",
        "    assert plageEntree != plageSortie\n",
        "    assert plageEntree[1]>0 and plageSortie[1] > 0\n",
        "    formatArray = [array.dtype for array in arrayL]\n",
        "    L = [np.array(array,dtype=np.float) for array in arrayL]\n",
        "    for i in range(len(L)):\n",
        "        L[i] = np.array((L[i]-plageEntree[0])/(plageEntree[1]-plageEntree[0])*(plageSortie[1]-plageSortie[0])+plageSortie[0],formatArray[i])\n",
        "    return L"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AO_NSKJdVoS9",
        "colab": {}
      },
      "source": [
        "def LarrayFloatToUint(L):\n",
        "    return [np.array(array,np.uint) for array in L]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOZZ2bGdMLEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tauxApprentissage(epoch,ampl,tau,lim):\n",
        "    taux = ampl*10**-((epoch)/tau)\n",
        "    return taux if taux > lim else lim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wSOGJdQj6dR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def differenceAcceptee(epoch,ampl,tau,lim):\n",
        "    taux = ampl*10**-((epoch)/tau)\n",
        "    return taux if taux > lim else lim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drdUov_iYAZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convertToUint(array):\n",
        "    return np.array(normalisation(array,[0,1],[0,255]),dtype=np.uint8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hp8DhcxaEKU-",
        "colab": {}
      },
      "source": [
        "def convertToUintL(L):\n",
        "    Lresult = []\n",
        "    print(\"Entree : \",len(L))\n",
        "    for i in range(len(L)):\n",
        "        Lresult.append(np.array(normalisation(L[i],[0,1],[0,255]),dtype=np.uint8))\n",
        "    print(\"Sortie : \",len(Lresult))\n",
        "    return Lresult"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF62Ds8TNw4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = np.arange(0,len(images))\n",
        "np.random.shuffle(index)\n",
        "trainingData = [images[i] for i in index[:int(0.6*len(images))]]\n",
        "evalData = [images[i] for i in index[int(0.6*len(images))-1:]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXZasifPjmid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# source : https://stackoverflow.com/questions/46418373/how-to-resize-interpolate-a-tensor-in-keras\n",
        "def interpolation(h,w,inputTensor):\n",
        "    def resize_like(inputTensor,h,w):\n",
        "        return tf.image.resize_nearest_neighbor(inputTensor, [h, w])\n",
        "\n",
        "    return Lambda(resize_like, arguments={'h':h,'w':w})(inputTensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exgFsJIgpdt3",
        "colab_type": "code",
        "outputId": "0102935d-0186-4833-fdeb-f38a41012583",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "def SELU(x):\n",
        "    return 1.0507*K.elu(x,alpha=1.67326)\n",
        "\n",
        "get_custom_objects().update({'custom_activation': Activation(SELU)})\n",
        "\n",
        "# A mettre pour le modèle : Activation(SELU)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0801 13:14:49.996535 139904404195200 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDebcyPfqaow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LRN2D(Layer):#Normalisation de réponse locale\n",
        "    \"\"\"\n",
        "    This code is adapted from pylearn2.\n",
        "    License at: https://github.com/lisa-lab/pylearn2/blob/master/LICENSE.txt\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=1e-4, k=2, beta=0.75, n=5, **kwargs):\n",
        "        if n % 2 == 0:\n",
        "            raise NotImplementedError(\"LRN2D only works with odd n. n provided: \" + str(n))\n",
        "        super(LRN2D, self).__init__(**kwargs)\n",
        "        self.alpha = alpha\n",
        "        self.k = k\n",
        "        self.beta = beta\n",
        "        self.n = n\n",
        "\n",
        "    def get_output(self, train):\n",
        "        X = self.get_input(train)\n",
        "        b, ch, r, c = K.shape(X)\n",
        "        half_n = self.n // 2\n",
        "        input_sqr = K.square(X)\n",
        "        extra_channels = K.zeros((b, ch + 2 * half_n, r, c))\n",
        "        input_sqr = K.concatenate([extra_channels[:, :half_n, :, :],\n",
        "                                   input_sqr,\n",
        "                                   extra_channels[:, half_n + ch:, :, :]],\n",
        "                                  axis=1)\n",
        "        scale = self.k\n",
        "        for i in range(self.n):\n",
        "            scale += self.alpha * input_sqr[:, i:i + ch, :, :]\n",
        "        scale = scale ** self.beta\n",
        "        return X / scale\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\"name\": self.__class__.__name__,\n",
        "                  \"alpha\": self.alpha,\n",
        "                  \"k\": self.k,\n",
        "                  \"beta\": self.beta,\n",
        "                  \"n\": self.n}\n",
        "        base_config = super(LRN2D, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwjD7SOoFiTy",
        "colab_type": "text"
      },
      "source": [
        "# Discriminateur"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOnNAG-mFpBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminateur():\n",
        "    inpt = Input(shape = (199, 199, 3),name='Image')\n",
        "    Lsummary = []\n",
        "    with K.name_scope('Bruit'):\n",
        "        bruit = Convolution2D(filters=1,kernel_size=(2,2),activation=None,strides=(1,1),padding='SAME')(inpt)\n",
        "        bruit = BatchNormalization()(bruit)\n",
        "        bruit = Activation(SELU)(bruit)\n",
        "        bruit = Convolution2D(filters=3,kernel_size=(2,2),activation=None,strides=(1,1),padding='SAME')(bruit)\n",
        "        bruit = BatchNormalization()(bruit)\n",
        "        bruit = Activation(SELU)(bruit)\n",
        "        bruit = Subtract()([inpt,bruit])\n",
        "        bruit = MaxPooling2D(pool_size=3,padding='VALID')(bruit)\n",
        "        bruit1 = Convolution2D(filters=1,kernel_size=(2,2),activation=None,strides=(1,1),padding='SAME')(bruit)\n",
        "        bruit1 = BatchNormalization()(bruit)\n",
        "        bruit1 = Activation(SELU)(bruit)\n",
        "        bruit1 = Convolution2D(filters=3,kernel_size=(2,2),activation=None,strides=(1,1),padding='SAME')(bruit)\n",
        "        bruit1 = BatchNormalization()(bruit)\n",
        "        bruit1 = Activation(SELU)(bruit)\n",
        "        bruit1 = Subtract()([bruit,bruit1])\n",
        "        bruit = MaxPooling2D(pool_size=3,padding='VALID')(bruit1)\n",
        "    \n",
        "    \n",
        "    def inception(prevShapes):\n",
        "        import time\n",
        "        LsummaryInception = []\n",
        "        inpt = Input(shape = prevShapes)\n",
        "        coucheT0 = Convolution2D(filters=100,kernel_size=(1,1),activation=None,strides=(1,1),padding='SAME')(inpt)\n",
        "\n",
        "        coucheT1 = Convolution2D(filters=100,kernel_size=(1,1),activation=None,strides=(1,1),padding='SAME')(inpt)\n",
        "        coucheT1 = Convolution2D(filters=100,kernel_size=(1,3),activation=None,strides=(1,1),padding='SAME')(coucheT1)\n",
        "        coucheT1 = Convolution2D(filters=100,kernel_size=(3,1),activation=None,strides=(1,1),padding='SAME')(coucheT1)\n",
        "\n",
        "        couche = Concatenate(axis=-1)([coucheT0,coucheT1])\n",
        "        couche = Convolution2D(filters=100,kernel_size=(1,1),activation=None,strides=(1,1),padding='SAME')(couche)\n",
        "        couche = Add()([couche,inpt])\n",
        "        \n",
        "        return Model(input=inpt,output=couche,name='Inception_%f'%(time.time()))\n",
        "    \n",
        "    with K.name_scope('Traitement_image'):\n",
        "        coucheAdaptation = Convolution2D(filters=100,kernel_size=(1,1),activation=None,strides=(1,1),padding='SAME')(inpt)\n",
        "        inceptionModel = inception(coucheAdaptation.get_shape().as_list()[1:])\n",
        "        image = inceptionModel(coucheAdaptation)\n",
        "        image = LRN2D(n=21,k=2,alpha=10**-4,beta=0.75)(image)\n",
        "        image = BatchNormalization()(image)\n",
        "        image = Activation(SELU)(image)\n",
        "        image = MaxPooling2D(pool_size=3,padding='VALID')(image)\n",
        "        inceptionModel = inception(image.get_shape().as_list()[1:])\n",
        "        image = inceptionModel(image)\n",
        "        image = LRN2D(n=21,k=2,alpha=10**-4,beta=0.75)(image)\n",
        "        image = BatchNormalization()(image)\n",
        "        image = Activation(SELU)(image)\n",
        "        image = MaxPooling2D(pool_size=3,padding='VALID')(image)\n",
        "        inceptionModel = inception(image.get_shape().as_list()[1:])\n",
        "        image = inceptionModel(image)\n",
        "        image = LRN2D(n=21,k=2,alpha=10**-4,beta=0.75)(image)\n",
        "        image = BatchNormalization()(image)\n",
        "        image = Activation(SELU)(image)\n",
        "    \n",
        "    resultatAnalyse = Concatenate(axis=-1)([bruit,image])\n",
        "    resultatAnalyse = Flatten()(resultatAnalyse)\n",
        "    resultatAnalyse = Dense(500)(resultatAnalyse)\n",
        "    resultatAnalyse = Dropout(rate=0.25)(resultatAnalyse)\n",
        "    resultatAnalyse = Activation(SELU)(resultatAnalyse)\n",
        "    probabilite = Dense(1)(resultatAnalyse)\n",
        "    probabilite = Activation('sigmoid')(probabilite)\n",
        "    return Model(input=inpt,output=probabilite,name='Discriminateur')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkYZQGhzV3ff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cf https://stackoverflow.com/questions/43784921/how-to-display-custom-images-in-tensorboard-using-keras?noredirect=1#comment85726690_43784921\n",
        "def make_image(tensor):\n",
        "    \"\"\"\n",
        "    Convert an numpy representation image to Image protobuf.\n",
        "    Copied from https://github.com/lanpa/tensorboard-pytorch/\n",
        "    \"\"\"\n",
        "    from PIL import Image\n",
        "    height, width = tensor.shape\n",
        "    image = Image.fromarray(tensor).convert('RGB')\n",
        "    import io\n",
        "    output = io.BytesIO()\n",
        "    image.save(output, format='PNG')\n",
        "    image_string = output.getvalue()\n",
        "    output.close()\n",
        "    CHANNEL = 1\n",
        "    return tf.Summary.Image(height=height,\n",
        "                         width=width,\n",
        "                         colorspace=CHANNEL,\n",
        "                         encoded_image_string=image_string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKN6TQe5jr9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sauvegardeModele(entree,model,iteration_entrainement,summary_writer,batch_size=7):\n",
        "    list_layers = [[layer.name,layer.get_output_at(-1),layer] for layer in model.layers][1:]\n",
        "    for layer_element in list_layers:\n",
        "        layer_name,layer_output,layer = layer_element\n",
        "        model_calcul_image = Model(input=model.input,output=layer_output)\n",
        "        sortie_couche = model_calcul_image.predict(entree, batch_size=batch_size)\n",
        "        dim_sortie = sortie_couche.shape\n",
        "        if len(dim_sortie) == 4:\n",
        "            for canal_image in range(dim_sortie[-1]):\n",
        "                summary_image = tf.Summary(value=[tf.Summary.Value(tag=layer_name+'_canal_'+str(canal_image), \n",
        "                                         image=make_image(sortie_couche[0,:,:,canal_image]))])\n",
        "                summary_writer.add_summary(summary_image,iteration_entrainement)\n",
        "    summary_writer.close()\n",
        "    summary_writer.reopen()\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKie888JIZTk",
        "colab_type": "code",
        "outputId": "4f8e8ca8-a297-43ed-f9e3-dfbf12e0e019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def train(images,eval_data,pathTmpSave,indexModele):\n",
        "    pathTmpSave = pathTmpSave + 'Modele'+str(indexModele)+'_Disc.h5'\n",
        "    d_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
        "    disc = discriminateur()\n",
        "    print(disc.summary())\n",
        "    Lloss = []\n",
        "    Lresult = []\n",
        "    disc.compile(loss='logcosh', optimizer=d_optim)\n",
        "    from keras.utils import plot_model\n",
        "    plot_model(disc, to_file='Modele'+str(indexModele)+'_Disc',show_shapes=True,show_layer_names=True)\n",
        "    \n",
        "    summary_writer = tf.summary.FileWriter(logdir='./logs/Modele'+str(indexModele)+'_Disc',graph=tf.get_default_graph())\n",
        "    \n",
        "    nbEpoch = 200\n",
        "    i = 0\n",
        "    rep = 2\n",
        "    while i < nbEpoch:\n",
        "        disc.save_weights(pathTmpSave)\n",
        "        image2,imageBruitee2 = normalisation(next_batch_bruit_voile_2(7,images,199,np.float32,[1,1,1],[50,50]),[0,255],[0,1])\n",
        "        \n",
        "        current_learning_rate = tauxApprentissage(i,10**-4,10,10**-5)\n",
        "        K.set_value(disc.optimizer.lr, current_learning_rate)\n",
        "        \n",
        "        entree = np.concatenate((image2, imageBruitee2))\n",
        "        sortie = np.array(7* [1] + 7*[0])\n",
        "        loss = disc.train_on_batch(entree,sortie)\n",
        "        Lloss.append(loss)\n",
        "        if np.isnan(loss) == True and i < 10:\n",
        "            disc = discriminateur()\n",
        "            disc.compile(loss='logcosh', optimizer=d_optim)\n",
        "            i = 0\n",
        "            print(\"Reset\")\n",
        "        elif np.isnan(loss) == True and rep > 0:\n",
        "            disc.load_weights(pathTmpSave)\n",
        "            nbEpoch += 1\n",
        "            rep -= 1\n",
        "            Lresult = []\n",
        "        elif np.isnan(loss) == True and rep == 0:\n",
        "            break\n",
        "            \n",
        "        else:\n",
        "            rep = 10\n",
        "        print(\"Epoch %i, loss %f\"%(i,loss))\n",
        "        summary_loss = tf.Summary(value=[tf.Summary.Value(tag=\"Erreur : Logcosh\", \n",
        "                                             simple_value=loss) ])\n",
        "        summary_writer.add_summary(summary_loss,i)\n",
        "        if i % 49 == 0:\n",
        "            sauvegardeModele(entree,disc,i,summary_writer,batch_size=7)\n",
        "        Lloss.append(loss)\n",
        "        i += 1\n",
        "    \n",
        "    return Lloss\n",
        "Lloss = train(trainingData, evalData,'./Restore/',1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"Inception_1563549897.521545\", inputs=Tensor(\"Tr..., outputs=Tensor(\"Tr...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"Inception_1563549897.708283\", inputs=Tensor(\"Tr..., outputs=Tensor(\"Tr...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"Inception_1563549897.901590\", inputs=Tensor(\"Tr..., outputs=Tensor(\"Tr...)`\n",
            "W0719 15:24:58.047202 140609842673536 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"Discriminateur\", inputs=Tensor(\"Im..., outputs=Tensor(\"ac...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Image (InputLayer)              (None, 199, 199, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_185 (Conv2D)             (None, 199, 199, 100 400         Image[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "Inception_1563549897.521545 (Mo (None, 199, 199, 100 100500      conv2d_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lr_n2d_28 (LRN2D)               (None, 199, 199, 100 0           Inception_1563549897.521545[1][0]\n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 199, 199, 100 400         lr_n2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_181 (Conv2D)             (None, 199, 199, 1)  13          Image[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 199, 199, 100 0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 199, 199, 1)  4           conv2d_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_39 (MaxPooling2D) (None, 66, 66, 100)  0           activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 199, 199, 1)  0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Inception_1563549897.708283 (Mo (None, 66, 66, 100)  100500      max_pooling2d_39[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_182 (Conv2D)             (None, 199, 199, 3)  15          activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lr_n2d_29 (LRN2D)               (None, 66, 66, 100)  0           Inception_1563549897.708283[1][0]\n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 199, 199, 3)  12          conv2d_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 66, 66, 100)  400         lr_n2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 199, 199, 3)  0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 66, 66, 100)  0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "subtract_19 (Subtract)          (None, 199, 199, 3)  0           Image[0][0]                      \n",
            "                                                                 activation_75[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_40 (MaxPooling2D) (None, 22, 22, 100)  0           activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_37 (MaxPooling2D) (None, 66, 66, 3)    0           subtract_19[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Inception_1563549897.901590 (Mo (None, 22, 22, 100)  100500      max_pooling2d_40[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 66, 66, 3)    0           max_pooling2d_37[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "lr_n2d_30 (LRN2D)               (None, 22, 22, 100)  0           Inception_1563549897.901590[1][0]\n",
            "__________________________________________________________________________________________________\n",
            "subtract_20 (Subtract)          (None, 66, 66, 3)    0           max_pooling2d_37[0][0]           \n",
            "                                                                 activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 22, 22, 100)  400         lr_n2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_38 (MaxPooling2D) (None, 22, 22, 3)    0           subtract_20[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 22, 22, 100)  0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_40 (Concatenate)    (None, 22, 22, 103)  0           max_pooling2d_38[0][0]           \n",
            "                                                                 activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_10 (Flatten)            (None, 49852)        0           concatenate_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_19 (Dense)                (None, 500)          24926500    flatten_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 500)          0           dense_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 500)          0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_20 (Dense)                (None, 1)            501         activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 1)            0           dense_20[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 25,230,145\n",
            "Trainable params: 25,229,537\n",
            "Non-trainable params: 608\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 0, loss 0.162549\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"Im..., outputs=Tensor(\"Tr...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"Im..., outputs=Tensor(\"Br...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"Im..., outputs=Tensor(\"co...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"Im..., outputs=Tensor(\"fl...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"Im..., outputs=Tensor(\"de...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"Im..., outputs=Tensor(\"dr...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"Im..., outputs=Tensor(\"ac...)`\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, loss 0.162400\n",
            "Epoch 2, loss 0.193431\n",
            "Epoch 3, loss 0.182119\n",
            "Epoch 4, loss 0.140434\n",
            "Epoch 5, loss 0.112518\n",
            "Epoch 6, loss 0.130877\n",
            "Epoch 7, loss 0.104991\n",
            "Epoch 8, loss 0.148660\n",
            "Epoch 9, loss 0.100008\n",
            "Epoch 10, loss 0.041876\n",
            "Epoch 11, loss 0.118191\n",
            "Epoch 12, loss 0.029929\n",
            "Epoch 13, loss 0.091597\n",
            "Epoch 14, loss 0.115697\n",
            "Epoch 15, loss 0.100794\n",
            "Epoch 16, loss 0.068363\n",
            "Epoch 17, loss 0.109097\n",
            "Epoch 18, loss 0.077566\n",
            "Epoch 19, loss 0.044812\n",
            "Epoch 20, loss 0.040720\n",
            "Epoch 21, loss 0.103823\n",
            "Epoch 22, loss 0.047190\n",
            "Epoch 23, loss 0.053478\n",
            "Epoch 24, loss 0.079487\n",
            "Epoch 25, loss 0.072721\n",
            "Epoch 26, loss 0.085456\n",
            "Epoch 27, loss 0.082768\n",
            "Epoch 28, loss 0.040946\n",
            "Epoch 29, loss 0.065589\n",
            "Epoch 30, loss 0.040821\n",
            "Epoch 31, loss 0.068556\n",
            "Epoch 32, loss 0.050092\n",
            "Epoch 33, loss 0.035736\n",
            "Epoch 34, loss 0.050636\n",
            "Epoch 35, loss 0.063486\n",
            "Epoch 36, loss 0.029868\n",
            "Epoch 37, loss 0.027689\n",
            "Epoch 38, loss 0.109195\n",
            "Epoch 39, loss 0.094627\n",
            "Epoch 40, loss 0.076842\n",
            "Epoch 41, loss 0.052171\n",
            "Epoch 42, loss 0.090465\n",
            "Epoch 43, loss 0.048154\n",
            "Epoch 44, loss 0.106710\n",
            "Epoch 45, loss 0.029430\n",
            "Epoch 46, loss 0.030663\n",
            "Epoch 47, loss 0.020831\n",
            "Epoch 48, loss 0.046251\n",
            "Epoch 49, loss 0.107904\n",
            "Epoch 50, loss 0.048141\n",
            "Epoch 51, loss 0.028918\n",
            "Epoch 52, loss 0.053598\n",
            "Epoch 53, loss 0.061098\n",
            "Epoch 54, loss 0.022007\n",
            "Epoch 55, loss 0.073722\n",
            "Epoch 56, loss 0.064321\n",
            "Epoch 57, loss 0.038093\n",
            "Epoch 58, loss 0.079255\n",
            "Epoch 59, loss 0.104119\n",
            "Epoch 60, loss 0.081285\n",
            "Epoch 61, loss 0.037735\n",
            "Epoch 62, loss 0.047956\n",
            "Epoch 63, loss 0.055477\n",
            "Epoch 64, loss 0.060186\n",
            "Epoch 65, loss 0.054783\n",
            "Epoch 66, loss 0.087210\n",
            "Epoch 67, loss 0.039794\n",
            "Epoch 68, loss 0.034708\n",
            "Epoch 69, loss 0.036894\n",
            "Epoch 70, loss 0.020715\n",
            "Epoch 71, loss 0.093414\n",
            "Epoch 72, loss 0.077253\n",
            "Epoch 73, loss 0.023072\n",
            "Epoch 74, loss 0.029082\n",
            "Epoch 75, loss 0.088235\n",
            "Epoch 76, loss 0.046193\n",
            "Epoch 77, loss 0.024860\n",
            "Epoch 78, loss 0.053863\n",
            "Epoch 79, loss 0.092727\n",
            "Epoch 80, loss 0.056554\n",
            "Epoch 81, loss 0.038999\n",
            "Epoch 82, loss 0.011970\n",
            "Epoch 83, loss 0.025060\n",
            "Epoch 84, loss 0.025786\n",
            "Epoch 85, loss 0.016573\n",
            "Epoch 86, loss 0.059219\n",
            "Epoch 87, loss 0.040702\n",
            "Epoch 88, loss 0.033070\n",
            "Epoch 89, loss 0.073169\n",
            "Epoch 90, loss 0.047956\n",
            "Epoch 91, loss 0.021529\n",
            "Epoch 92, loss 0.034262\n",
            "Epoch 93, loss 0.055522\n",
            "Epoch 94, loss 0.036996\n",
            "Epoch 95, loss 0.036439\n",
            "Epoch 96, loss 0.034863\n",
            "Epoch 97, loss 0.063061\n",
            "Epoch 98, loss 0.038765\n",
            "Epoch 99, loss 0.037872\n",
            "Epoch 100, loss 0.075269\n",
            "Epoch 101, loss 0.066189\n",
            "Epoch 102, loss 0.043904\n",
            "Epoch 103, loss 0.072896\n",
            "Epoch 104, loss 0.026358\n",
            "Epoch 105, loss 0.097385\n",
            "Epoch 106, loss 0.017074\n",
            "Epoch 107, loss 0.114451\n",
            "Epoch 108, loss 0.055549\n",
            "Epoch 109, loss 0.028812\n",
            "Epoch 110, loss 0.045120\n",
            "Epoch 111, loss 0.028839\n",
            "Epoch 112, loss 0.075358\n",
            "Epoch 113, loss 0.010868\n",
            "Epoch 114, loss 0.094022\n",
            "Epoch 115, loss 0.037146\n",
            "Epoch 116, loss 0.085845\n",
            "Epoch 117, loss 0.044083\n",
            "Epoch 118, loss 0.032499\n",
            "Epoch 119, loss 0.028644\n",
            "Epoch 120, loss 0.031172\n",
            "Epoch 121, loss 0.020444\n",
            "Epoch 122, loss 0.036473\n",
            "Epoch 123, loss 0.054391\n",
            "Epoch 124, loss 0.034870\n",
            "Epoch 125, loss 0.019515\n",
            "Epoch 126, loss 0.062393\n",
            "Epoch 127, loss 0.070483\n",
            "Epoch 128, loss 0.035362\n",
            "Epoch 129, loss 0.086377\n",
            "Epoch 130, loss 0.088879\n",
            "Epoch 131, loss 0.059427\n",
            "Epoch 132, loss 0.024631\n",
            "Epoch 133, loss 0.047055\n",
            "Epoch 134, loss 0.067600\n",
            "Epoch 135, loss 0.086589\n",
            "Epoch 136, loss 0.052368\n",
            "Epoch 137, loss 0.016663\n",
            "Epoch 138, loss 0.047813\n",
            "Epoch 139, loss 0.015745\n",
            "Epoch 140, loss 0.044892\n",
            "Epoch 141, loss 0.074779\n",
            "Epoch 142, loss 0.006760\n",
            "Epoch 143, loss 0.098608\n",
            "Epoch 144, loss 0.047691\n",
            "Epoch 145, loss 0.088756\n",
            "Epoch 146, loss 0.054749\n",
            "Epoch 147, loss 0.077583\n",
            "Epoch 148, loss 0.064101\n",
            "Epoch 149, loss 0.066182\n",
            "Epoch 150, loss 0.050597\n",
            "Epoch 151, loss 0.056155\n",
            "Epoch 152, loss 0.063666\n",
            "Epoch 153, loss 0.050858\n",
            "Epoch 154, loss 0.053354\n",
            "Epoch 155, loss 0.110775\n",
            "Epoch 156, loss 0.039058\n",
            "Epoch 157, loss 0.054725\n",
            "Epoch 158, loss 0.015975\n",
            "Epoch 159, loss 0.058159\n",
            "Epoch 160, loss 0.018288\n",
            "Epoch 161, loss 0.047929\n",
            "Epoch 162, loss 0.078382\n",
            "Epoch 163, loss 0.028372\n",
            "Epoch 164, loss 0.042082\n",
            "Epoch 165, loss 0.031072\n",
            "Epoch 166, loss 0.029816\n",
            "Epoch 167, loss 0.045918\n",
            "Epoch 168, loss 0.035661\n",
            "Epoch 169, loss 0.047517\n",
            "Epoch 170, loss 0.038920\n",
            "Epoch 171, loss 0.020639\n",
            "Epoch 172, loss 0.074638\n",
            "Epoch 173, loss 0.081850\n",
            "Epoch 174, loss 0.076259\n",
            "Epoch 175, loss 0.050076\n",
            "Epoch 176, loss 0.050840\n",
            "Epoch 177, loss 0.003050\n",
            "Epoch 178, loss 0.002255\n",
            "Epoch 179, loss 0.046645\n",
            "Epoch 180, loss 0.043853\n",
            "Epoch 181, loss 0.065636\n",
            "Epoch 182, loss 0.032774\n",
            "Epoch 183, loss 0.039989\n",
            "Epoch 184, loss 0.060241\n",
            "Epoch 185, loss 0.064881\n",
            "Epoch 186, loss 0.014226\n",
            "Epoch 187, loss 0.008538\n",
            "Epoch 188, loss 0.025294\n",
            "Epoch 189, loss 0.012154\n",
            "Epoch 190, loss 0.049590\n",
            "Epoch 191, loss 0.018934\n",
            "Epoch 192, loss 0.007230\n",
            "Epoch 193, loss 0.005185\n",
            "Epoch 194, loss 0.076936\n",
            "Epoch 195, loss 0.093447\n",
            "Epoch 196, loss 0.045258\n",
            "Epoch 197, loss 0.029913\n",
            "Epoch 198, loss 0.025516\n",
            "Epoch 199, loss 0.046019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwWO20K1Cbew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateur():\n",
        "    inpt = Input(shape = (199, 199, 3),name='ImageATraiter')\n",
        "    couche = Dense(500)(inpt)\n",
        "    couche = Dropout(rate=0.25)(couche)\n",
        "    couche = LRN2D(n=21,k=2,alpha=10**-4,beta=0.75)(couche)\n",
        "    couche = Activation(SELU)(couche)\n",
        "    couche = Dense(100)(couche)\n",
        "    couche = BatchNormalization()(couche)\t\n",
        "    couche = Activation(SELU)(couche)\n",
        "    couche = Dense(3)(couche)\n",
        "    couche = BatchNormalization()(couche)\t\n",
        "    couche = Activation(SELU)(couche)\n",
        "    couche = Add()([inpt,couche])\n",
        "    return Model(input=inpt,output=couche,name='Generateur')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZffLfY0WMaPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def top_10(gen,x,y,indexModele):\n",
        "    Lloss = []\n",
        "    orig_backup = \"_tmp_Modele\"+str(indexModele)+\"_gen_orig.h5\"\n",
        "    gen.save_weights(orig_backup)\n",
        "    for i in range(10):\n",
        "        backup_path = \"_tmp_Modele\"+str(indexModele)+\"_gen_\"+str(i)+\".h5\"\n",
        "        gen.load_weights(orig_backup)\n",
        "        loss = gen.train_on_batch(x,y)\n",
        "        gen.save_weights(backup_path)\n",
        "        Lloss.append([loss,backup_path])\n",
        "    Lloss.sort(key=lambda elem:elem[0])\n",
        "    gen.load_weights(Lloss[0][1])\n",
        "    return Lloss[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaGwnkGthU2X",
        "colab_type": "code",
        "outputId": "b640081e-dad5-4913-9abc-f9c669416967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        }
      },
      "source": [
        "def preTrainGenerateur(images,eval_data,pathTmpSave,indexModele):\n",
        "    gen = generateur()\n",
        "    optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
        "    gen.compile(loss='mean_absolute_error', optimizer=optim)\n",
        "    print(gen.summary())\n",
        "    from keras.utils import plot_model\n",
        "    plot_model(gen, to_file='modele_gereteur_Modele%i.png'%(indexModele),show_shapes=True,show_layer_names=True)\n",
        "    i = 0\n",
        "    Lanalyse = []\n",
        "    Lloss = []\n",
        "    nbEpoch = 500\n",
        "    repetition_limit = 10\n",
        "    repetition_state = 0\n",
        "    while i < nbEpoch:\n",
        "        image,imageBruitee = normalisation(next_batch_bruit_voile_2(7,images,199,np.float32,[1,1,1],[50,50]),[0,255],[0,1])\n",
        "        current_learning_rate = tauxApprentissage(i,10**-4,10,10**-7)\n",
        "        K.set_value(gen.optimizer.lr, current_learning_rate)\n",
        "        if i % 10 == 0:\n",
        "            predictedImage = convertToUint(gen.predict(image))\n",
        "            predictedImageBruitee = convertToUint(gen.predict(imageBruitee))\n",
        "            Lanalyse.append([image,predictedImage,imageBruitee,predictedImageBruitee])\n",
        "        x = np.concatenate((image,imageBruitee))\n",
        "        y = np.concatenate((image,image))\n",
        "        loss = top_10(gen,x,y,indexModele)\n",
        "        print(\"Loss %i : %f\"%(i,loss))\n",
        "        Lloss.append(loss)\n",
        "        i += 1\n",
        "    return Lloss,Lanalyse\n",
        "Lloss,Lanalyse = preTrainGenerateur(trainingData, evalData,'./Restore/',1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0801 13:09:18.800787 139683353831296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0801 13:09:18.813323 139683353831296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0801 13:09:18.843512 139683353831296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0801 13:09:18.855220 139683353831296 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0801 13:09:18.879354 139683353831296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3217: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0801 13:09:18.921240 139683353831296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0801 13:09:19.157670 139683353831296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"Generateur\", inputs=Tensor(\"Im..., outputs=Tensor(\"ad...)`\n",
            "  \n",
            "W0801 13:09:19.506540 139683353831296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "ImageATraiter (InputLayer)      (None, 199, 199, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 199, 199, 500 2000        ImageATraiter[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 199, 199, 500 0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lr_n2d_1 (LRN2D)                (None, 199, 199, 500 0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 199, 199, 500 0           lr_n2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 199, 199, 100 50100       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 199, 199, 100 400         dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 199, 199, 100 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 199, 199, 3)  303         activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 199, 199, 3)  12          dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 199, 199, 3)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 199, 199, 3)  0           ImageATraiter[0][0]              \n",
            "                                                                 activation_4[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 52,815\n",
            "Trainable params: 52,609\n",
            "Non-trainable params: 206\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Error in next_batch\n",
            "Error in next_batch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhqtnxiwhrJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Llayers = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bNEj3euhpwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateur():\n",
        "    global Llayers\n",
        "    inpt = Input(shape = (199, 199, 3),name='ImageATraiter')\n",
        "    Llayers.append([inpt,'input_generateur'])\n",
        "    couche = Dense(500)(inpt)\n",
        "    couche = Dropout(rate=0.25)(couche)\n",
        "    couche = LRN2D(n=21,k=2,alpha=10**-4,beta=0.75)(couche)\n",
        "    couche = Activation(SELU)(couche)\n",
        "    Llayers.append([couche,\"gen_dense_500\"])\n",
        "    couche = Dense(100)(couche)\n",
        "    couche = BatchNormalization()(couche)\n",
        "    couche = Activation(SELU)(couche)\n",
        "    Llayers.append([couche,\"gen_dense_100\"])\n",
        "    couche = Dense(3)(couche)\n",
        "    couche = BatchNormalization()(couche)\t\n",
        "    couche = Activation(SELU)(couche)\n",
        "    Llayers.append([couche,\"gen_dense_3\"])\n",
        "    couche = Add()([inpt,couche])\n",
        "    Llayers.append([couche,'gen_dense_add'])\n",
        "    return Model(input=inpt,output=couche,name='Generateur')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92l9iUdEhm2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminateur():\n",
        "    global Llayers\n",
        "    inpt = Input(shape = (199, 199, 3),name='Image')\n",
        "    with K.name_scope('Bruit'):\n",
        "        bruit = Convolution2D(filters=1,kernel_size=(2,2),activation=None,strides=(1,1),padding='SAME')(inpt)\n",
        "        bruit = BatchNormalization()(bruit)\n",
        "        bruit = Activation(SELU)(bruit)\n",
        "        bruit = Convolution2D(filters=3,kernel_size=(2,2),activation=None,strides=(1,1),padding='SAME')(bruit)\n",
        "        bruit = BatchNormalization()(bruit)\n",
        "        bruit = Activation(SELU)(bruit)\n",
        "        bruit = Subtract()([inpt,bruit])\n",
        "        Llayers.append([bruit,'disc_subt_bruit_1'])\n",
        "        bruit = MaxPooling2D(pool_size=3,padding='VALID')(bruit)\n",
        "        bruit1 = Convolution2D(filters=1,kernel_size=(2,2),activation=None,strides=(1,1),padding='SAME')(bruit)\n",
        "        bruit1 = BatchNormalization()(bruit)\n",
        "        bruit1 = Activation(SELU)(bruit)\n",
        "        bruit1 = Convolution2D(filters=3,kernel_size=(2,2),activation=None,strides=(1,1),padding='SAME')(bruit)\n",
        "        bruit1 = BatchNormalization()(bruit)\n",
        "        bruit1 = Activation(SELU)(bruit)\n",
        "        bruit = Subtract()([bruit,bruit1])\n",
        "        Llayers.append([bruit,'disc_subt_bruit_2'])\n",
        "        layer = MaxPooling2D(pool_size=3,padding='VALID')\n",
        "        Llayers.append([layer,'disc_bruit_3'])\n",
        "        bruit = layer(bruit)\n",
        "    \n",
        "    \n",
        "    def inception(prevShapes):\n",
        "        import time\n",
        "        LsummaryInception = []\n",
        "        inpt = Input(shape = prevShapes)\n",
        "        coucheT0 = Convolution2D(filters=100,kernel_size=(1,1),activation=None,strides=(1,1),padding='SAME')(inpt)\n",
        "\n",
        "        coucheT1 = Convolution2D(filters=100,kernel_size=(1,1),activation=None,strides=(1,1),padding='SAME')(inpt)\n",
        "        coucheT1 = Convolution2D(filters=100,kernel_size=(1,3),activation=None,strides=(1,1),padding='SAME')(coucheT1)\n",
        "        coucheT1 = Convolution2D(filters=100,kernel_size=(3,1),activation=None,strides=(1,1),padding='SAME')(coucheT1)\n",
        "\n",
        "        couche = Concatenate(axis=-1)([coucheT0,coucheT1])\n",
        "        couche = Convolution2D(filters=100,kernel_size=(1,1),activation=None,strides=(1,1),padding='SAME')(couche)\n",
        "        couche = Add()([couche,inpt])\n",
        "        \n",
        "        return Model(input=inpt,output=couche,name='Inception_%f'%(time.time()))\n",
        "    \n",
        "    with K.name_scope('Traitement_image'):\n",
        "        coucheAdaptation = Convolution2D(filters=100,kernel_size=(1,1),activation=None,strides=(1,1),padding='SAME')(inpt)\n",
        "        inceptionModel = inception(coucheAdaptation.get_shape().as_list()[1:])\n",
        "        image = inceptionModel(coucheAdaptation)\n",
        "        image = LRN2D(n=21,k=2,alpha=10**-4,beta=0.75)(image)\n",
        "        image = BatchNormalization()(image)\n",
        "        image = Activation(SELU)(image)\n",
        "        Llayers.append([image,'disc_inception_img_1'])\n",
        "        image = MaxPooling2D(pool_size=3,padding='VALID')(image)\n",
        "        inceptionModel = inception(image.get_shape().as_list()[1:])\n",
        "        image = inceptionModel(image)\n",
        "        image = LRN2D(n=21,k=2,alpha=10**-4,beta=0.75)(image)\n",
        "        image = BatchNormalization()(image)\n",
        "        image = Activation(SELU)(image)\n",
        "        Llayers.append([image,'disc_inception_img_2'])\n",
        "        image = MaxPooling2D(pool_size=3,padding='VALID')(image)\n",
        "        inceptionModel = inception(image.get_shape().as_list()[1:])\n",
        "        image = inceptionModel(image)\n",
        "        image = LRN2D(n=21,k=2,alpha=10**-4,beta=0.75)(image)\n",
        "        image = BatchNormalization()(image)\n",
        "        image = Activation(SELU)(image)\n",
        "        Llayers.append([image,'disc_inception_img_3'])\n",
        "    \n",
        "    resultatAnalyse = Concatenate(axis=-1)([bruit,image])\n",
        "    resultatAnalyse = Flatten()(resultatAnalyse)\n",
        "    resultatAnalyse = Dense(500)(resultatAnalyse)\n",
        "    resultatAnalyse = Dropout(rate=0.25)(resultatAnalyse)\n",
        "    resultatAnalyse = Activation(SELU)(resultatAnalyse)\n",
        "    probabilite = Dense(1)(resultatAnalyse)\n",
        "    probabilite = Activation('sigmoid')(probabilite)\n",
        "    return Model(input=inpt,output=probabilite,name='Discriminateur')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO4ay9Zj-ZqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_gen_disc_top_9(disc,gen_disc,x,indexModele):\n",
        "    disc_orig_backup = \"Modele\"+str(indexModele)+\"_global_disc_orig.h5\"\n",
        "    disc_gen_orig_backup = \"Modele\"+str(indexModele)+\"_global_gen_disc_orig.h5\"\n",
        "    # On commence par faire 3 essais avec le gen_disc\n",
        "    Lloss_gen_disc = []\n",
        "    Lloss_tot = []\n",
        "    disc.save_weights(disc_orig_backup)\n",
        "    gen_disc.save_weights(disc_gen_orig_backup)\n",
        "    y = [1]*x.shape[0]\n",
        "    for i in range(3):\n",
        "        try:\n",
        "            gen_disc.load_weights(disc_gen_orig_backup)\n",
        "        except:\n",
        "            gen_disc.load_weights(disc_gen_orig_backup)\n",
        "        loss = gen_disc.train_on_batch(x,y)\n",
        "        gen_disc_backup = \"Modele\"+str(indexModele)+\"_global_gen_disc_%i.h5\"%(i)\n",
        "        try:\n",
        "            gen_disc.save_weights(gen_disc_backup)\n",
        "        except:\n",
        "            gen_disc.save_weights(gen_disc_backup)\n",
        "        Lloss_gen_disc.append([loss,gen_disc_backup])\n",
        "    y = [1]*(x.shape[0]//2)+[0]*(x.shape[0]//2)\n",
        "    for index_gen_disc,loss_pathBackup in enumerate(Lloss_gen_disc):\n",
        "        for i in range(3):\n",
        "            print(\"loss_pathBackup[1]\",loss_pathBackup[1])\n",
        "            try:\n",
        "                gen_disc.load_weights(loss_pathBackup[1])\n",
        "            except:\n",
        "                gen_disc.load_weights(loss_pathBackup[1])\n",
        "            loss = disc.train_on_batch(x,y)\n",
        "            disc_backup = \"Modele\"+str(indexModele)+\"_global_disc_branche_%i_essai_%i.h5\"%(index_gen_disc,i)\n",
        "            try:\n",
        "                disc.save_weights(disc_backup)\n",
        "            except:\n",
        "                disc.save_weights(disc_backup)\n",
        "            Lloss_tot.append([loss_pathBackup[0],loss_pathBackup[1],loss,disc_backup,np.mean([loss_pathBackup[0],loss])])\n",
        "    Lloss_tot.sort(key=lambda elem:elem[-1])\n",
        "    try:\n",
        "        gen_disc.load_weights(Lloss_tot[0][1])\n",
        "        disc.load_weights(Lloss_tot[0][-2])\n",
        "    except:\n",
        "        gen_disc.load_weights(Lloss_tot[0][1])\n",
        "        disc.load_weights(Lloss_tot[0][-2])\n",
        "        \n",
        "    print(\"Resultat top 9 : \",Lloss_tot)\n",
        "    return Lloss_tot[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o352RNtxhvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sauvegardeModele(entree_pure,entree_deterioree,model,iteration_entrainement,summary_writer,batch_size=7):\n",
        "    global Llayers\n",
        "    list_layers = Llayers\n",
        "    print(\"list_layers \",list_layers)\n",
        "    for p,entree in enumerate([entree_pure,entree_deterioree]):\n",
        "        layer_outputs,layer_names = [list_layers[i][0] for i in range(1,len(list_layers))],[list_layers[i][1] for i in range(len(list_layers))]\n",
        "        print(\"layer_outputs : \",layer_outputs)\n",
        "        model_calcul_image = Model(input=model.input,outputs=[model.output]+layer_outputs)\n",
        "        sorties_couches = [entree]+model_calcul_image.predict(entree, batch_size=batch_size)[1:]\n",
        "        for index_couche,sortie_couche in enumerate(sorties_couches):\n",
        "            layer_name = layer_names[index_couche]\n",
        "            dim_sortie = sortie_couche.shape\n",
        "            if len(dim_sortie) == 4:\n",
        "                for canal_image in range(dim_sortie[-1]):\n",
        "                    tag = layer_name\n",
        "                    tag += 'pure' if p == 0 else 'deterioree'\n",
        "                    summary_image = tf.Summary(value=[tf.Summary.Value(tag=tag, \n",
        "                                            image=make_image(sortie_couche[0,:,:,canal_image]))])\n",
        "                    summary_writer.add_summary(summary_image,iteration_entrainement)\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iFvzVmO84qz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_disc_model(gen,disc):\n",
        "    inpt = Input(shape = (199, 199, 3))\n",
        "    couche = gen(inpt)\n",
        "    couche = disc(couche)\n",
        "    return Model(input=inpt, output=couche)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e5YM7WEs6sw",
        "colab_type": "code",
        "outputId": "84368e92-b2a5-4b0f-8345-f3720dd362f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def train_gen_disc(images,eval_data,indexModele):\n",
        "    gen = generateur()\n",
        "    gen_backup = \"_tmp_Modele\"+str(indexModele-3)+\"_gen_orig.h5\"\n",
        "    gen.load_weights(gen_backup)\n",
        "    gen_disc_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
        "    disc = discriminateur()\n",
        "    disc_backup = \"./Restore/\" + 'Modele'+str(indexModele-3)+'_Disc.h5'\n",
        "    disc_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
        "    disc.compile(loss='binary_crossentropy', optimizer=disc_optim)\n",
        "    disc.load_weights(disc_backup)\n",
        "    gen_disc = gen_disc_model(gen,disc)\n",
        "    gen_disc.compile(loss='binary_crossentropy', optimizer=gen_disc_optim)\n",
        "    print(\"Layers : \",Llayers)\n",
        "    print(gen_disc.summary())\n",
        "    print(disc.summary())\n",
        "    print(Llayers)\n",
        "    Lloss = []\n",
        "    iterations = 500\n",
        "    index = 0\n",
        "    import datetime\n",
        "    chaine_date = datetime.datetime.today().strftime('%Y-%m-%d_%Hh%Mmin%Ss') #cf http://strftime.org/ et https://www.science-emergence.com/Articles/Obtenir-la-date-daujourdhui-au-format-YYYY-MM-DD-avec-python/\n",
        "    summary_writer = tf.summary.FileWriter(logdir='./logs/Modele'+str(indexModele)+'_gen_disc_'+chaine_date,graph=tf.get_default_graph())\n",
        "    while index < iterations:\n",
        "        image,imageBruitee = normalisation(next_batch_bruit_voile_2(7,images,199,np.float32,[1,1,1],[50,50]),[0,255],[0,1])\n",
        "        current_learning_rate = tauxApprentissage(index,10**-4,10,10**-7)\n",
        "        K.set_value(disc.optimizer.lr, current_learning_rate)\n",
        "        K.set_value(gen_disc.optimizer.lr, current_learning_rate)\n",
        "        x = np.concatenate((image,imageBruitee))\n",
        "        loss = mean_gen_disc_top_9(disc,gen_disc,x,indexModele)\n",
        "        Lloss.append(loss)\n",
        "        print(\"Iteration %i : gen_disc, erreur : %f ; disc, erreur : %f ; erreur moyenne : %f\"%(index,loss[0],loss[2],loss[-1]))\n",
        "        summary_loss = tf.Summary(value=[tf.Summary.Value(tag=\"Erreur gen-disc : binary-crossentropy\", \n",
        "                                             simple_value=loss[0]) ])\n",
        "        summary_writer.add_summary(summary_loss,index)\n",
        "        summary_loss = tf.Summary(value=[tf.Summary.Value(tag=\"Erreur disc : binary-crossentropy\", \n",
        "                                             simple_value=loss[2]) ])\n",
        "        summary_writer.add_summary(summary_loss,index)\n",
        "        summary_loss = tf.Summary(value=[tf.Summary.Value(tag=\"Erreur moyenne : binary-crossentropy\", \n",
        "                                             simple_value=loss[-1]) ])\n",
        "        summary_writer.add_summary(summary_loss,index)\n",
        "        if index % 5 == 0:\n",
        "            sauvegardeModele(image,imageBruitee,gen_disc,index,summary_writer)\n",
        "            p_gen_disc = gen_disc.predict(x)[0]\n",
        "            p_disc = disc.predict(x)[0]\n",
        "            summary_loss = tf.Summary(value=[tf.Summary.Value(tag=\"Probabilité gen-disc img 0 (orig pure) pure : \", \n",
        "                                                simple_value=p_gen_disc) ])\n",
        "            summary_writer.add_summary(summary_loss,index)\n",
        "            summary_loss = tf.Summary(value=[tf.Summary.Value(tag=\"Probabilité disc img 0 (orig pure) pure : \", \n",
        "                                                simple_value=p_disc) ])\n",
        "            summary_writer.add_summary(summary_loss,index)\n",
        "            x_proba = np.concatenate((imageBruitee,imageBruitee))\n",
        "            p_gen_disc = gen_disc.predict(x_proba)[0]\n",
        "            p_disc = disc.predict(x_proba)[0]\n",
        "            summary_loss = tf.Summary(value=[tf.Summary.Value(tag=\"Probabilité gen-disc img 0 (orig détériorée) pure : \", \n",
        "                                                simple_value=p_gen_disc) ])\n",
        "            summary_writer.add_summary(summary_loss,index)\n",
        "            summary_loss = tf.Summary(value=[tf.Summary.Value(tag=\"Probabilité disc img 0 (orig détériorée) pure : \", \n",
        "                                                simple_value=p_disc) ])\n",
        "            summary_writer.add_summary(summary_loss,index)\n",
        "            summary_writer.close()\n",
        "            summary_writer.reopen()\n",
        "        index += 1\n",
        "Lloss = train_gen_disc(images,evalData,4)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"Generateur\", inputs=Tensor(\"Im..., outputs=Tensor(\"ad...)`\n",
            "W0801 13:15:37.843936 139904404195200 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"Inception_1564665338.109349\", inputs=Tensor(\"Tr..., outputs=Tensor(\"Tr...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"Inception_1564665338.269454\", inputs=Tensor(\"Tr..., outputs=Tensor(\"Tr...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"Inception_1564665338.431779\", inputs=Tensor(\"Tr..., outputs=Tensor(\"Tr...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:73: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"Discriminateur\", inputs=Tensor(\"Im..., outputs=Tensor(\"ac...)`\n",
            "W0801 13:15:38.740683 139904404195200 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"Di...)`\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Layers :  [[<tf.Tensor 'ImageATraiter:0' shape=(?, 199, 199, 3) dtype=float32>, 'input_generateur'], [<tf.Tensor 'activation_2/mul_1:0' shape=(?, 199, 199, 500) dtype=float32>, 'gen_dense_500'], [<tf.Tensor 'ImageATraiter_1:0' shape=(?, 199, 199, 3) dtype=float32>, 'input_generateur'], [<tf.Tensor 'activation_4/mul_1:0' shape=(?, 199, 199, 500) dtype=float32>, 'gen_dense_500'], [<tf.Tensor 'activation_5/mul_1:0' shape=(?, 199, 199, 100) dtype=float32>, 'gen_dense_100'], [<tf.Tensor 'activation_6/mul_1:0' shape=(?, 199, 199, 3) dtype=float32>, 'gen_dense_3'], [<tf.Tensor 'add_1/add:0' shape=(?, 199, 199, 3) dtype=float32>, 'gen_dense_add'], [<tf.Tensor 'Bruit/subtract_1/sub:0' shape=(?, 199, 199, 3) dtype=float32>, 'disc_subt_bruit_1'], [<tf.Tensor 'Bruit/subtract_2/sub:0' shape=(?, 66, 66, 3) dtype=float32>, 'disc_subt_bruit_2'], [<keras.layers.pooling.MaxPooling2D object at 0x7f3dba011da0>, 'disc_bruit_3'], [<tf.Tensor 'Traitement_image/activation_11/mul_1:0' shape=(?, 199, 199, 100) dtype=float32>, 'disc_inception_img_1'], [<tf.Tensor 'Traitement_image/activation_12/mul_1:0' shape=(?, 66, 66, 100) dtype=float32>, 'disc_inception_img_2'], [<tf.Tensor 'Traitement_image/activation_13/mul_1:0' shape=(?, 22, 22, 100) dtype=float32>, 'disc_inception_img_3']]\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 199, 199, 3)       0         \n",
            "_________________________________________________________________\n",
            "Generateur (Model)           (None, 199, 199, 3)       52815     \n",
            "_________________________________________________________________\n",
            "Discriminateur (Model)       (None, 1)                 25230145  \n",
            "=================================================================\n",
            "Total params: 25,282,960\n",
            "Trainable params: 25,282,146\n",
            "Non-trainable params: 814\n",
            "_________________________________________________________________\n",
            "None\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Image (InputLayer)              (None, 199, 199, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 199, 199, 100 400         Image[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "Inception_1564665338.109349 (Mo (None, 199, 199, 100 100500      conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lr_n2d_3 (LRN2D)                (None, 199, 199, 100 0           Inception_1564665338.109349[1][0]\n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 199, 199, 100 400         lr_n2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 199, 199, 1)  13          Image[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 199, 199, 100 0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 199, 199, 1)  4           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 66, 66, 100)  0           activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 199, 199, 1)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Inception_1564665338.269454 (Mo (None, 66, 66, 100)  100500      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 199, 199, 3)  15          activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lr_n2d_4 (LRN2D)                (None, 66, 66, 100)  0           Inception_1564665338.269454[1][0]\n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 199, 199, 3)  12          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 66, 66, 100)  400         lr_n2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 199, 199, 3)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 66, 66, 100)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "subtract_1 (Subtract)           (None, 199, 199, 3)  0           Image[0][0]                      \n",
            "                                                                 activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 22, 22, 100)  0           activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 66, 66, 3)    0           subtract_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "Inception_1564665338.431779 (Mo (None, 22, 22, 100)  100500      max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 66, 66, 3)    0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "lr_n2d_5 (LRN2D)                (None, 22, 22, 100)  0           Inception_1564665338.431779[1][0]\n",
            "__________________________________________________________________________________________________\n",
            "subtract_2 (Subtract)           (None, 66, 66, 3)    0           max_pooling2d_1[0][0]            \n",
            "                                                                 activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 22, 22, 100)  400         lr_n2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 22, 22, 3)    0           subtract_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 22, 22, 100)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 22, 22, 103)  0           max_pooling2d_2[0][0]            \n",
            "                                                                 activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 49852)        0           concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 500)          24926500    flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 500)          0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 500)          0           dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1)            501         activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 1)            0           dense_7[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 25,230,145\n",
            "Trainable params: 25,229,537\n",
            "Non-trainable params: 608\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "[[<tf.Tensor 'ImageATraiter:0' shape=(?, 199, 199, 3) dtype=float32>, 'input_generateur'], [<tf.Tensor 'activation_2/mul_1:0' shape=(?, 199, 199, 500) dtype=float32>, 'gen_dense_500'], [<tf.Tensor 'ImageATraiter_1:0' shape=(?, 199, 199, 3) dtype=float32>, 'input_generateur'], [<tf.Tensor 'activation_4/mul_1:0' shape=(?, 199, 199, 500) dtype=float32>, 'gen_dense_500'], [<tf.Tensor 'activation_5/mul_1:0' shape=(?, 199, 199, 100) dtype=float32>, 'gen_dense_100'], [<tf.Tensor 'activation_6/mul_1:0' shape=(?, 199, 199, 3) dtype=float32>, 'gen_dense_3'], [<tf.Tensor 'add_1/add:0' shape=(?, 199, 199, 3) dtype=float32>, 'gen_dense_add'], [<tf.Tensor 'Bruit/subtract_1/sub:0' shape=(?, 199, 199, 3) dtype=float32>, 'disc_subt_bruit_1'], [<tf.Tensor 'Bruit/subtract_2/sub:0' shape=(?, 66, 66, 3) dtype=float32>, 'disc_subt_bruit_2'], [<keras.layers.pooling.MaxPooling2D object at 0x7f3dba011da0>, 'disc_bruit_3'], [<tf.Tensor 'Traitement_image/activation_11/mul_1:0' shape=(?, 199, 199, 100) dtype=float32>, 'disc_inception_img_1'], [<tf.Tensor 'Traitement_image/activation_12/mul_1:0' shape=(?, 66, 66, 100) dtype=float32>, 'disc_inception_img_2'], [<tf.Tensor 'Traitement_image/activation_13/mul_1:0' shape=(?, 22, 22, 100) dtype=float32>, 'disc_inception_img_3']]\n",
            "loss_pathBackup[1] Modele4_global_gen_disc_0.h5\n",
            "loss_pathBackup[1] Modele4_global_gen_disc_0.h5\n",
            "loss_pathBackup[1] Modele4_global_gen_disc_0.h5\n",
            "loss_pathBackup[1] Modele4_global_gen_disc_1.h5\n",
            "loss_pathBackup[1] Modele4_global_gen_disc_1.h5\n",
            "loss_pathBackup[1] Modele4_global_gen_disc_1.h5\n",
            "loss_pathBackup[1] Modele4_global_gen_disc_2.h5\n",
            "loss_pathBackup[1] Modele4_global_gen_disc_2.h5\n",
            "loss_pathBackup[1] Modele4_global_gen_disc_2.h5\n",
            "Resultat top 9 :  [[0.7565689, 'Modele4_global_gen_disc_2.h5', 0.851517, 'Modele4_global_disc_branche_2_essai_0.h5', 0.80404294], [0.7565689, 'Modele4_global_gen_disc_2.h5', 0.9012748, 'Modele4_global_disc_branche_2_essai_2.h5', 0.82892185], [0.7565689, 'Modele4_global_gen_disc_2.h5', 0.9280138, 'Modele4_global_disc_branche_2_essai_1.h5', 0.84229136], [1.1127689, 'Modele4_global_gen_disc_1.h5', 0.81873477, 'Modele4_global_disc_branche_1_essai_0.h5', 0.9657518], [1.2184708, 'Modele4_global_gen_disc_0.h5', 0.87954026, 'Modele4_global_disc_branche_0_essai_1.h5', 1.0490055], [1.2184708, 'Modele4_global_gen_disc_0.h5', 0.89365345, 'Modele4_global_disc_branche_0_essai_0.h5', 1.0560621], [1.1127689, 'Modele4_global_gen_disc_1.h5', 1.0114415, 'Modele4_global_disc_branche_1_essai_2.h5', 1.0621052], [1.1127689, 'Modele4_global_gen_disc_1.h5', 1.049742, 'Modele4_global_disc_branche_1_essai_1.h5', 1.0812554], [1.2184708, 'Modele4_global_gen_disc_0.h5', 0.9785628, 'Modele4_global_disc_branche_0_essai_2.h5', 1.0985168]]\n",
            "Iteration 0 : gen_disc, erreur : 0.756569 ; disc, erreur : 0.851517 ; erreur moyenne : 0.804043\n",
            "list_layers  [[<tf.Tensor 'ImageATraiter:0' shape=(?, 199, 199, 3) dtype=float32>, 'input_generateur'], [<tf.Tensor 'activation_2/mul_1:0' shape=(?, 199, 199, 500) dtype=float32>, 'gen_dense_500'], [<tf.Tensor 'ImageATraiter_1:0' shape=(?, 199, 199, 3) dtype=float32>, 'input_generateur'], [<tf.Tensor 'activation_4/mul_1:0' shape=(?, 199, 199, 500) dtype=float32>, 'gen_dense_500'], [<tf.Tensor 'activation_5/mul_1:0' shape=(?, 199, 199, 100) dtype=float32>, 'gen_dense_100'], [<tf.Tensor 'activation_6/mul_1:0' shape=(?, 199, 199, 3) dtype=float32>, 'gen_dense_3'], [<tf.Tensor 'add_1/add:0' shape=(?, 199, 199, 3) dtype=float32>, 'gen_dense_add'], [<tf.Tensor 'Bruit/subtract_1/sub:0' shape=(?, 199, 199, 3) dtype=float32>, 'disc_subt_bruit_1'], [<tf.Tensor 'Bruit/subtract_2/sub:0' shape=(?, 66, 66, 3) dtype=float32>, 'disc_subt_bruit_2'], [<keras.layers.pooling.MaxPooling2D object at 0x7f3dba011da0>, 'disc_bruit_3'], [<tf.Tensor 'Traitement_image/activation_11/mul_1:0' shape=(?, 199, 199, 100) dtype=float32>, 'disc_inception_img_1'], [<tf.Tensor 'Traitement_image/activation_12/mul_1:0' shape=(?, 66, 66, 100) dtype=float32>, 'disc_inception_img_2'], [<tf.Tensor 'Traitement_image/activation_13/mul_1:0' shape=(?, 22, 22, 100) dtype=float32>, 'disc_inception_img_3']]\n",
            "layer_outputs :  [<tf.Tensor 'activation_2/mul_1:0' shape=(?, 199, 199, 500) dtype=float32>, <tf.Tensor 'ImageATraiter_1:0' shape=(?, 199, 199, 3) dtype=float32>, <tf.Tensor 'activation_4/mul_1:0' shape=(?, 199, 199, 500) dtype=float32>, <tf.Tensor 'activation_5/mul_1:0' shape=(?, 199, 199, 100) dtype=float32>, <tf.Tensor 'activation_6/mul_1:0' shape=(?, 199, 199, 3) dtype=float32>, <tf.Tensor 'add_1/add:0' shape=(?, 199, 199, 3) dtype=float32>, <tf.Tensor 'Bruit/subtract_1/sub:0' shape=(?, 199, 199, 3) dtype=float32>, <tf.Tensor 'Bruit/subtract_2/sub:0' shape=(?, 66, 66, 3) dtype=float32>, <keras.layers.pooling.MaxPooling2D object at 0x7f3dba011da0>, <tf.Tensor 'Traitement_image/activation_11/mul_1:0' shape=(?, 199, 199, 100) dtype=float32>, <tf.Tensor 'Traitement_image/activation_12/mul_1:0' shape=(?, 66, 66, 100) dtype=float32>, <tf.Tensor 'Traitement_image/activation_13/mul_1:0' shape=(?, 22, 22, 100) dtype=float32>]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=Tensor(\"in...)`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-3b4b9ff2cb5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0msummary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mLloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gen_disc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevalData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-3b4b9ff2cb5b>\u001b[0m in \u001b[0;36mtrain_gen_disc\u001b[0;34m(images, eval_data, indexModele)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0msummary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0msauvegardeModele\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimageBruitee\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgen_disc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msummary_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mp_gen_disc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_disc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mp_disc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-d972f4b398ec>\u001b[0m in \u001b[0;36msauvegardeModele\u001b[0;34m(entree_pure, entree_deterioree, model, iteration_entrainement, summary_writer, batch_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"layer_outputs : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmodel_calcul_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlayer_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0msorties_couches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mentree\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel_calcul_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex_couche\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msortie_couche\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorties_couches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m    186\u001b[0m                                  \u001b[0;34m'the output of a Keras `Layer` '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                                  \u001b[0;34m'(thus holding past layer metadata). '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                                  'Found: ' + str(x))\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         self._compute_previous_mask = (\n",
            "\u001b[0;31mValueError\u001b[0m: Output tensors to a Model must be the output of a Keras `Layer` (thus holding past layer metadata). Found: <keras.layers.pooling.MaxPooling2D object at 0x7f3dba011da0>"
          ]
        }
      ]
    }
  ]
}