{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modele000002.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPAkLQeF/BhPqDJFWfvn313",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rob174/Astronomy/blob/Astronomy/Modele000002.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-4ZUsJdxOlq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6d969d6b-db45-4982-bd71-95fc4b9ed5f7"
      },
      "source": [
        "\n",
        "##Python / Colab\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import os\n",
        "from IPython.display import Image as imgIPython\n",
        "from IPython.display import clear_output,display\n",
        "import IPython\n",
        "## Tensorflow keras\n",
        "try:\n",
        "  !pip install -q tf-nightly\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from tensorflow.python import debug as tf_debug\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense,Conv2D,Convolution2D,Activation,Conv2DTranspose\n",
        "from tensorflow.keras.layers import MaxPooling2D,AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout,Reshape,BatchNormalization\n",
        "from tensorflow.keras.layers import concatenate,Concatenate,Subtract,Multiply,Average,Add\n",
        "from tensorflow.keras.layers import UpSampling2D, Reshape,Flatten\n",
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.losses\n",
        "## Math libraries\n",
        "import numpy as np\n",
        "import scipy\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "##Images\n",
        "from PIL import Image\n",
        "import cv2\n",
        "## Graph\n",
        "from graphviz import render\n",
        "from graphviz import Digraph,Graph\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/My Drive/TIPE'\n",
        "#Dataset\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import random\n",
        "import pathlib"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/TIPE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMma_lMW-29B",
        "colab_type": "text"
      },
      "source": [
        "#Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oQm7uLvDnR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title { display-mode: \"form\" }\n",
        "\n",
        "dossier_TIPE = \"/content/drive/My Drive/TIPE/\"#@param {type:\"string\"}\n",
        "localisation_annexe_txt = \"list_files.txt\"#@param {type:\"string\"}\n",
        "part_train = 0.6#@param {type: \"number\"}\n",
        "part_validation = 0.3#@param {type: \"number\"}\n",
        "part_test = 0.1#@param {type: \"number\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGFNQMf4E0SZ",
        "colab_type": "text"
      },
      "source": [
        "Récupération des images et de celles issues d'entrainements précédents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1f4VUBiEzaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_clean = [dossier_TIPE+\"Galaxies_resized/\"+f for f in os.listdir(dossier_TIPE+\"Galaxies_resized/\")]\n",
        "dataset_noise = [None for i in range(len(dataset_clean))]\n",
        "#Ouverture des images issues d'entrainement précédants\n",
        "with open(localisation_annexe_txt,'r') as f:\n",
        "        for l in f:\n",
        "            dataset_clean.append(dossier_TIPE+l.split(\",\")[0].strip())\n",
        "            dataset_noise.append(dossier_TIPE+l.split(\",\")[1].strip())\n",
        "#Mélange suivant : https://stackoverflow.com/questions/23289547/shuffle-two-list-at-once-with-same-order doc *list https://stackoverflow.com/questions/12555627/python-3-starred-expression-to-unpack-a-list\n",
        "shuffle_list = list(zip(dataset_clean,dataset_noise))\n",
        "random.shuffle(shuffle_list)\n",
        "dataset_clean,dataset_noise = zip(*shuffle_list)\n",
        "lg = len(dataset_clean)\n",
        "#Création des indices limites\n",
        "parts_lim = [part_train,part_validation,part_test]\n",
        "indices_lim = [int(sum(parts_lim[:i+1])*lg) for i in range(len(parts_lim))][:-1]\n",
        "#Séparation\n",
        "xTrain, xValidation, xTest = np.split(dataset_noise,indices_lim)\n",
        "yTrain, yValidation, yTest = np.split(dataset_clean,indices_lim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac2TLlgRMArx",
        "colab_type": "text"
      },
      "source": [
        "Création des classes de dataset Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6a8nDxQRhg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_path = tf.keras.utils.get_file(\n",
        "    'Dataset',\n",
        "    dossier_TIPE+\"Galaxies_resized/\")\n",
        "dataset_path = pathlib.Path(dataset_path)\n",
        "#doc de pathlib : https://docs.python.org/3/library/pathlib.html\n",
        "list_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnJ5U9HexYRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#D'après https://stackoverflow.com/questions/54590363/create-tensorflow-dataset-from-image-local-directory\n",
        "class ArtificialDataset(tf.data.Dataset):\n",
        "    def _generator(num_samples):\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((tf.constant(image_list), tf.constant(label_list)))\n",
        "        \n",
        "        \n",
        "        for sample_idx in range(num_samples):\n",
        "            # Reading data (line, record) from the file\n",
        "            time.sleep(0.015)\n",
        "            \n",
        "            yield (sample_idx,)\n",
        "    \n",
        "    def __new__(cls, num_samples=3):\n",
        "        return tf.data.Dataset.from_generator(\n",
        "            cls._generator,\n",
        "            output_types=tf.dtypes.int64,\n",
        "            output_shapes=(1,),\n",
        "            args=(num_samples,)\n",
        "        )\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}