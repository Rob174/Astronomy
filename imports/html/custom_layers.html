<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.4" />
<title>custom_layers API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>custom_layers</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import tensorflow as tf
from tensorflow.keras.layers import Layer

##interpolation
def interpolation(h,w,inputTensor):
    # source : https://stackoverflow.com/questions/46418373/how-to-resize-interpolate-a-tensor-in-keras
    def resize_like(inputTensor,h,w):
        return tf.image.resize_nearest_neighbor(inputTensor, [h, w])

    return Lambda(resize_like, arguments={&#39;h&#39;:h,&#39;w&#39;:w})(inputTensor)

from tensorflow.keras import backend as K
from tensorflow.keras.optimizers import Optimizer


##RectifiedAdam

class RectifiedAdam(Optimizer):
    # Ported from https://github.com/LiyuanLucasLiu/RAdam/blob/master/radam.py
    &#34;&#34;&#34;RectifiedAdam optimizer.
    Default parameters follow those provided in the original paper.
    # Arguments
        lr: float &gt;= 0. Learning rate.
        final_lr: float &gt;= 0. Final learning rate.
        beta_1: float, 0 &lt; beta &lt; 1. Generally close to 1.
        beta_2: float, 0 &lt; beta &lt; 1. Generally close to 1.
        gamma: float &gt;= 0. Convergence speed of the bound function.
        epsilon: float &gt;= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.
        decay: float &gt;= 0. Learning rate decay over each update.
        weight_decay: Weight decay weight.
        amsbound: boolean. Whether to apply the AMSBound variant of this
            algorithm.
    # References
        - [On the Variance of the Adaptive Learning Rate and Beyond]
          (https://arxiv.org/abs/1908.03265)
        - [Adam - A Method for Stochastic Optimization]
          (https://arxiv.org/abs/1412.6980v8)
        - [On the Convergence of Adam and Beyond]
          (https://openreview.net/forum?id=ryQu7f-RZ)
    &#34;&#34;&#34;

    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,
                 epsilon=None, decay=0., weight_decay=0.0, **kwargs):
        super(RectifiedAdam, self).__init__(**kwargs)

        with K.name_scope(self.__class__.__name__):
            self.iterations = K.variable(0, dtype=&#39;int64&#39;, name=&#39;iterations&#39;)
            self.lr = K.variable(lr, name=&#39;lr&#39;)
            self.beta_1 = K.variable(beta_1, name=&#39;beta_1&#39;)
            self.beta_2 = K.variable(beta_2, name=&#39;beta_2&#39;)
            self.decay = K.variable(decay, name=&#39;decay&#39;)

        if epsilon is None:
            epsilon = K.epsilon()
        self.epsilon = epsilon
        self.initial_decay = decay

        self.weight_decay = float(weight_decay)

    def get_updates(self, loss, params):
        grads = self.get_gradients(loss, params)
        self.updates = [K.update_add(self.iterations, 1)]

        lr = self.lr
        if self.initial_decay &gt; 0:
            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,
                                                      K.dtype(self.decay))))

        t = K.cast(self.iterations, K.floatx()) + 1

        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]
        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]
        self.weights = [self.iterations] + ms + vs

        for p, g, m, v in zip(params, grads, ms, vs):
            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g
            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)

            beta2_t = self.beta_2 ** t
            N_sma_max = 2 / (1 - self.beta_2) - 1
            N_sma = N_sma_max - 2 * t * beta2_t / (1 - beta2_t)

            # apply weight decay
            if self.weight_decay != 0.:
                p_wd = p - self.weight_decay * lr * p
            else:
                p_wd = None

            if p_wd is None:
                p_ = p
            else:
                p_ = p_wd

            def gt_path():
                step_size = lr * K.sqrt(
                    (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max /
                    (N_sma_max - 2)) / (1 - self.beta_1 ** t)

                denom = K.sqrt(v_t) + self.epsilon
                p_t = p_ - step_size * (m_t / denom)

                return p_t

            def lt_path():
                step_size = lr / (1 - self.beta_1 ** t)
                p_t = p_ - step_size * m_t

                return p_t

            p_t = K.switch(N_sma &gt; 5, gt_path, lt_path)

            self.updates.append(K.update(m, m_t))
            self.updates.append(K.update(v, v_t))
            new_p = p_t

            # Apply constraints.
            if getattr(p, &#39;constraint&#39;, None) is not None:
                new_p = p.constraint(new_p)

            self.updates.append(K.update(p, new_p))
        return self.updates

    def get_config(self):
        config = {&#39;lr&#39;: float(K.get_value(self.lr)),
                  &#39;beta_1&#39;: float(K.get_value(self.beta_1)),
                  &#39;beta_2&#39;: float(K.get_value(self.beta_2)),
                  &#39;decay&#39;: float(K.get_value(self.decay)),
                  &#39;epsilon&#39;: self.epsilon,
                  &#39;weight_decay&#39;: self.weight_decay}
        base_config = super(RectifiedAdam, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

##SELU

from tensorflow.keras.layers import Activation
from tensorflow.keras.utils import get_custom_objects
def SELU(x):
    # A mettre pour le modèle : Activation(SELU)
    return 1.0507*K.elu(x,alpha=1.67326)

get_custom_objects().update({&#39;custom_activation&#39;: Activation(SELU)})

##LRN2D
class LRN2D(Layer):#Normalisation de réponse locale
    &#34;&#34;&#34;
    This code is adapted from pylearn2.
    License at: https://github.com/lisa-lab/pylearn2/blob/master/LICENSE.txt
    &#34;&#34;&#34;

    def __init__(self, alpha=1e-4, k=2, beta=0.75, n=5, **kwargs):
        if n % 2 == 0:
            raise NotImplementedError(&#34;LRN2D only works with odd n. n provided: &#34; + str(n))
        super(LRN2D, self).__init__(**kwargs)
        self.alpha = alpha
        self.k = k
        self.beta = beta
        self.n = n

    def get_output(self, train):
        X = self.get_input(train)
        b, ch, r, c = K.shape(X)
        half_n = self.n // 2
        input_sqr = K.square(X)
        extra_channels = K.zeros((b, ch + 2 * half_n, r, c))
        input_sqr = K.concatenate([extra_channels[:, :half_n, :, :],
                                   input_sqr,
                                   extra_channels[:, half_n + ch:, :, :]],
                                  axis=1)
        scale = self.k
        for i in range(self.n):
            scale += self.alpha * input_sqr[:, i:i + ch, :, :]
        scale = scale ** self.beta
        return X / scale

    def get_config(self):
        config = {&#34;name&#34;: self.__class__.__name__,
                  &#34;alpha&#34;: self.alpha,
                  &#34;k&#34;: self.k,
                  &#34;beta&#34;: self.beta,
                  &#34;n&#34;: self.n}
        base_config = super(LRN2D, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="custom_layers.SELU"><code class="name flex">
<span>def <span class="ident">SELU</span></span>(<span>x)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def SELU(x):
    # A mettre pour le modèle : Activation(SELU)
    return 1.0507*K.elu(x,alpha=1.67326)</code></pre>
</details>
</dd>
<dt id="custom_layers.interpolation"><code class="name flex">
<span>def <span class="ident">interpolation</span></span>(<span>h, w, inputTensor)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def interpolation(h,w,inputTensor):
    # source : https://stackoverflow.com/questions/46418373/how-to-resize-interpolate-a-tensor-in-keras
    def resize_like(inputTensor,h,w):
        return tf.image.resize_nearest_neighbor(inputTensor, [h, w])

    return Lambda(resize_like, arguments={&#39;h&#39;:h,&#39;w&#39;:w})(inputTensor)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="custom_layers.LRN2D"><code class="flex name class">
<span>class <span class="ident">LRN2D</span></span>
<span>(</span><span>alpha=0.0001, k=2, beta=0.75, n=5, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>This code is adapted from pylearn2.
License at: <a href="https://github.com/lisa-lab/pylearn2/blob/master/LICENSE.txt">https://github.com/lisa-lab/pylearn2/blob/master/LICENSE.txt</a></p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LRN2D(Layer):#Normalisation de réponse locale
    &#34;&#34;&#34;
    This code is adapted from pylearn2.
    License at: https://github.com/lisa-lab/pylearn2/blob/master/LICENSE.txt
    &#34;&#34;&#34;

    def __init__(self, alpha=1e-4, k=2, beta=0.75, n=5, **kwargs):
        if n % 2 == 0:
            raise NotImplementedError(&#34;LRN2D only works with odd n. n provided: &#34; + str(n))
        super(LRN2D, self).__init__(**kwargs)
        self.alpha = alpha
        self.k = k
        self.beta = beta
        self.n = n

    def get_output(self, train):
        X = self.get_input(train)
        b, ch, r, c = K.shape(X)
        half_n = self.n // 2
        input_sqr = K.square(X)
        extra_channels = K.zeros((b, ch + 2 * half_n, r, c))
        input_sqr = K.concatenate([extra_channels[:, :half_n, :, :],
                                   input_sqr,
                                   extra_channels[:, half_n + ch:, :, :]],
                                  axis=1)
        scale = self.k
        for i in range(self.n):
            scale += self.alpha * input_sqr[:, i:i + ch, :, :]
        scale = scale ** self.beta
        return X / scale

    def get_config(self):
        config = {&#34;name&#34;: self.__class__.__name__,
                  &#34;alpha&#34;: self.alpha,
                  &#34;k&#34;: self.k,
                  &#34;beta&#34;: self.beta,
                  &#34;n&#34;: self.n}
        base_config = super(LRN2D, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="custom_layers.LRN2D.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = {&#34;name&#34;: self.__class__.__name__,
              &#34;alpha&#34;: self.alpha,
              &#34;k&#34;: self.k,
              &#34;beta&#34;: self.beta,
              &#34;n&#34;: self.n}
    base_config = super(LRN2D, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))</code></pre>
</details>
</dd>
<dt id="custom_layers.LRN2D.get_output"><code class="name flex">
<span>def <span class="ident">get_output</span></span>(<span>self, train)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_output(self, train):
    X = self.get_input(train)
    b, ch, r, c = K.shape(X)
    half_n = self.n // 2
    input_sqr = K.square(X)
    extra_channels = K.zeros((b, ch + 2 * half_n, r, c))
    input_sqr = K.concatenate([extra_channels[:, :half_n, :, :],
                               input_sqr,
                               extra_channels[:, half_n + ch:, :, :]],
                              axis=1)
    scale = self.k
    for i in range(self.n):
        scale += self.alpha * input_sqr[:, i:i + ch, :, :]
    scale = scale ** self.beta
    return X / scale</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="custom_layers.RectifiedAdam"><code class="flex name class">
<span>class <span class="ident">RectifiedAdam</span></span>
<span>(</span><span>lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, weight_decay=0.0, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>RectifiedAdam optimizer.
Default parameters follow those provided in the original paper.</p>
<h1 id="arguments">Arguments</h1>
<pre><code>lr: float &gt;= 0. Learning rate.
final_lr: float &gt;= 0. Final learning rate.
beta_1: float, 0 &lt; beta &lt; 1. Generally close to 1.
beta_2: float, 0 &lt; beta &lt; 1. Generally close to 1.
gamma: float &gt;= 0. Convergence speed of the bound function.
epsilon: float &gt;= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.
decay: float &gt;= 0. Learning rate decay over each update.
weight_decay: Weight decay weight.
amsbound: boolean. Whether to apply the AMSBound variant of this
    algorithm.
</code></pre>
<h1 id="references">References</h1>
<pre><code>- [On the Variance of the Adaptive Learning Rate and Beyond]
  (&lt;https://arxiv.org/abs/1908.03265&gt;)
- [Adam - A Method for Stochastic Optimization]
  (&lt;https://arxiv.org/abs/1412.6980v8&gt;)
- [On the Convergence of Adam and Beyond]
  (&lt;https://openreview.net/forum?id=ryQu7f-RZ&gt;)
</code></pre>
<p>Create a new Optimizer.</p>
<p>This must be called by the constructors of subclasses.
Note that Optimizer instances should not bind to a single graph,
and so shouldn't keep Tensors as member variables. Generally
you should be able to use the _set_hyper()/state.get_hyper()
facility instead.</p>
<p>This class in stateful and thread-compatible.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>A non-empty string.
The name to use for accumulators created
for the optimizer.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>keyword arguments. Allowed to be {<code>clipnorm</code>, <code>clipvalue</code>, <code>lr</code>,
<code>decay</code>}. <code>clipnorm</code> is clip gradients by norm; <code>clipvalue</code> is clip
gradients by value, <code>decay</code> is included for backward compatibility to
allow time inverse decay of learning rate. <code>lr</code> is included for backward
compatibility, recommended to use <code>learning_rate</code> instead.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><strong><code>ValueError</code></strong></dt>
<dd>If name is malformed.</dd>
<dt><strong><code>RuntimeError</code></strong></dt>
<dd>If _create_slots has been overridden instead of
_create_vars.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RectifiedAdam(Optimizer):
    # Ported from https://github.com/LiyuanLucasLiu/RAdam/blob/master/radam.py
    &#34;&#34;&#34;RectifiedAdam optimizer.
    Default parameters follow those provided in the original paper.
    # Arguments
        lr: float &gt;= 0. Learning rate.
        final_lr: float &gt;= 0. Final learning rate.
        beta_1: float, 0 &lt; beta &lt; 1. Generally close to 1.
        beta_2: float, 0 &lt; beta &lt; 1. Generally close to 1.
        gamma: float &gt;= 0. Convergence speed of the bound function.
        epsilon: float &gt;= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.
        decay: float &gt;= 0. Learning rate decay over each update.
        weight_decay: Weight decay weight.
        amsbound: boolean. Whether to apply the AMSBound variant of this
            algorithm.
    # References
        - [On the Variance of the Adaptive Learning Rate and Beyond]
          (https://arxiv.org/abs/1908.03265)
        - [Adam - A Method for Stochastic Optimization]
          (https://arxiv.org/abs/1412.6980v8)
        - [On the Convergence of Adam and Beyond]
          (https://openreview.net/forum?id=ryQu7f-RZ)
    &#34;&#34;&#34;

    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,
                 epsilon=None, decay=0., weight_decay=0.0, **kwargs):
        super(RectifiedAdam, self).__init__(**kwargs)

        with K.name_scope(self.__class__.__name__):
            self.iterations = K.variable(0, dtype=&#39;int64&#39;, name=&#39;iterations&#39;)
            self.lr = K.variable(lr, name=&#39;lr&#39;)
            self.beta_1 = K.variable(beta_1, name=&#39;beta_1&#39;)
            self.beta_2 = K.variable(beta_2, name=&#39;beta_2&#39;)
            self.decay = K.variable(decay, name=&#39;decay&#39;)

        if epsilon is None:
            epsilon = K.epsilon()
        self.epsilon = epsilon
        self.initial_decay = decay

        self.weight_decay = float(weight_decay)

    def get_updates(self, loss, params):
        grads = self.get_gradients(loss, params)
        self.updates = [K.update_add(self.iterations, 1)]

        lr = self.lr
        if self.initial_decay &gt; 0:
            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,
                                                      K.dtype(self.decay))))

        t = K.cast(self.iterations, K.floatx()) + 1

        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]
        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]
        self.weights = [self.iterations] + ms + vs

        for p, g, m, v in zip(params, grads, ms, vs):
            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g
            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)

            beta2_t = self.beta_2 ** t
            N_sma_max = 2 / (1 - self.beta_2) - 1
            N_sma = N_sma_max - 2 * t * beta2_t / (1 - beta2_t)

            # apply weight decay
            if self.weight_decay != 0.:
                p_wd = p - self.weight_decay * lr * p
            else:
                p_wd = None

            if p_wd is None:
                p_ = p
            else:
                p_ = p_wd

            def gt_path():
                step_size = lr * K.sqrt(
                    (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max /
                    (N_sma_max - 2)) / (1 - self.beta_1 ** t)

                denom = K.sqrt(v_t) + self.epsilon
                p_t = p_ - step_size * (m_t / denom)

                return p_t

            def lt_path():
                step_size = lr / (1 - self.beta_1 ** t)
                p_t = p_ - step_size * m_t

                return p_t

            p_t = K.switch(N_sma &gt; 5, gt_path, lt_path)

            self.updates.append(K.update(m, m_t))
            self.updates.append(K.update(v, v_t))
            new_p = p_t

            # Apply constraints.
            if getattr(p, &#39;constraint&#39;, None) is not None:
                new_p = p.constraint(new_p)

            self.updates.append(K.update(p, new_p))
        return self.updates

    def get_config(self):
        config = {&#39;lr&#39;: float(K.get_value(self.lr)),
                  &#39;beta_1&#39;: float(K.get_value(self.beta_1)),
                  &#39;beta_2&#39;: float(K.get_value(self.beta_2)),
                  &#39;decay&#39;: float(K.get_value(self.decay)),
                  &#39;epsilon&#39;: self.epsilon,
                  &#39;weight_decay&#39;: self.weight_decay}
        base_config = super(RectifiedAdam, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="custom_layers.RectifiedAdam.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the config of the optimimizer.</p>
<p>An optimizer config is a Python dictionary (serializable)
containing the configuration of an optimizer.
The same optimizer can be reinstantiated later
(without any saved state) from this configuration.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = {&#39;lr&#39;: float(K.get_value(self.lr)),
              &#39;beta_1&#39;: float(K.get_value(self.beta_1)),
              &#39;beta_2&#39;: float(K.get_value(self.beta_2)),
              &#39;decay&#39;: float(K.get_value(self.decay)),
              &#39;epsilon&#39;: self.epsilon,
              &#39;weight_decay&#39;: self.weight_decay}
    base_config = super(RectifiedAdam, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))</code></pre>
</details>
</dd>
<dt id="custom_layers.RectifiedAdam.get_updates"><code class="name flex">
<span>def <span class="ident">get_updates</span></span>(<span>self, loss, params)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_updates(self, loss, params):
    grads = self.get_gradients(loss, params)
    self.updates = [K.update_add(self.iterations, 1)]

    lr = self.lr
    if self.initial_decay &gt; 0:
        lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,
                                                  K.dtype(self.decay))))

    t = K.cast(self.iterations, K.floatx()) + 1

    ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]
    vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]
    self.weights = [self.iterations] + ms + vs

    for p, g, m, v in zip(params, grads, ms, vs):
        m_t = (self.beta_1 * m) + (1. - self.beta_1) * g
        v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)

        beta2_t = self.beta_2 ** t
        N_sma_max = 2 / (1 - self.beta_2) - 1
        N_sma = N_sma_max - 2 * t * beta2_t / (1 - beta2_t)

        # apply weight decay
        if self.weight_decay != 0.:
            p_wd = p - self.weight_decay * lr * p
        else:
            p_wd = None

        if p_wd is None:
            p_ = p
        else:
            p_ = p_wd

        def gt_path():
            step_size = lr * K.sqrt(
                (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max /
                (N_sma_max - 2)) / (1 - self.beta_1 ** t)

            denom = K.sqrt(v_t) + self.epsilon
            p_t = p_ - step_size * (m_t / denom)

            return p_t

        def lt_path():
            step_size = lr / (1 - self.beta_1 ** t)
            p_t = p_ - step_size * m_t

            return p_t

        p_t = K.switch(N_sma &gt; 5, gt_path, lt_path)

        self.updates.append(K.update(m, m_t))
        self.updates.append(K.update(v, v_t))
        new_p = p_t

        # Apply constraints.
        if getattr(p, &#39;constraint&#39;, None) is not None:
            new_p = p.constraint(new_p)

        self.updates.append(K.update(p, new_p))
    return self.updates</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="custom_layers.SELU" href="#custom_layers.SELU">SELU</a></code></li>
<li><code><a title="custom_layers.interpolation" href="#custom_layers.interpolation">interpolation</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="custom_layers.LRN2D" href="#custom_layers.LRN2D">LRN2D</a></code></h4>
<ul class="">
<li><code><a title="custom_layers.LRN2D.get_config" href="#custom_layers.LRN2D.get_config">get_config</a></code></li>
<li><code><a title="custom_layers.LRN2D.get_output" href="#custom_layers.LRN2D.get_output">get_output</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="custom_layers.RectifiedAdam" href="#custom_layers.RectifiedAdam">RectifiedAdam</a></code></h4>
<ul class="">
<li><code><a title="custom_layers.RectifiedAdam.get_config" href="#custom_layers.RectifiedAdam.get_config">get_config</a></code></li>
<li><code><a title="custom_layers.RectifiedAdam.get_updates" href="#custom_layers.RectifiedAdam.get_updates">get_updates</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.4</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>