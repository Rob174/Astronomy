{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modèle000001.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rob174/Astronomy/blob/Astronomy/Mod%C3%A8le000001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k_hAT1OO2lUN"
      },
      "source": [
        "# Fonctions de base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "daqyahLS1u1Z",
        "colab": {}
      },
      "source": [
        "\n",
        "##Python / Colab\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import os\n",
        "from IPython.display import Image as imgIPython\n",
        "from IPython.display import clear_output,display\n",
        "import IPython\n",
        "## Tensorflow keras\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from tensorflow.python import debug as tf_debug\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense,Conv2D,Convolution2D,Activation,Conv2DTranspose\n",
        "from tensorflow.keras.layers import MaxPooling2D,AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout,Reshape,BatchNormalization\n",
        "from tensorflow.keras.layers import concatenate,Concatenate,Subtract,Multiply,Average,Add\n",
        "from tensorflow.keras.layers import UpSampling2D, Reshape,Flatten\n",
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.losses\n",
        "## Math libraries\n",
        "import numpy as np\n",
        "import scipy\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "##Images\n",
        "from PIL import Image\n",
        "import cv2\n",
        "## Graph\n",
        "from graphviz import render\n",
        "from graphviz import Digraph,Graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3feuWSKFaZF",
        "colab_type": "code",
        "outputId": "bcf4b04d-17e5-4cc4-deeb-408652d0677a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/My Drive/TIPE'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/TIPE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGp8Id2c0L3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/imports')\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/imports/Tensorflow')\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/imports/Graph_visualization')\n",
        "## Custom library\n",
        "from input_setup import *\n",
        "from custom_layers import * \n",
        "from graph_global import * \n",
        "from graph_graphviz import * \n",
        "from graph_keras import * "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvA9Twa4ZEXb",
        "colab_type": "text"
      },
      "source": [
        "## Test fonction bruitage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGNItN2GVxk6",
        "colab_type": "code",
        "outputId": "3437a0c3-c345-4c22-c896-49a13eeb2e85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "max_step=200\n",
        "max_it=2000\n",
        "iteration = 100\n",
        "train_data,validation_data,test_data = create_datasets()\n",
        "# image,imageBruitee = next_batch_bruit_voile_2(5,train_data,256,np.float32,\n",
        "#                                                             [[0,1.-iteration/(max_it+1)],[0,1.-iteration/(max_it+1)],[0,1.-iteration/(max_it+1)]],\n",
        "#                                                             [iteration//100*100/max_it/2,iteration//100*100/max_it/2],[0,1])\n",
        "taux_attenuation_max_log = 0.4#@param {type:'slider',min:0,max:5,step:0.1}\n",
        "taux_attenuation_min_log = 1.3#@param {type:'slider',min:0,max:5,step:0.1}\n",
        "ordre_gdr_bruit = 1.779#@param {type:'slider',min:0,max:4,step:0.001}\n",
        "taux_bruit = 10**-ordre_gdr_bruit\n",
        "plage_val = [taux_attenuation_min_log,taux_attenuation_max_log]\n",
        "print(\"Facteur voile entre %f et %f\"%(plage_val[0],plage_val[1]))\n",
        "image,imageBruitee = next_batch_bruit_voile_2(5,train_data,256,np.float32,\n",
        "                                                            [plage_val,plage_val,plage_val],\n",
        "                                                            [taux_bruit,taux_bruit],[0,1])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(30,15))\n",
        "it = 1\n",
        "for statut in range(2):\n",
        "    for img in range(5):\n",
        "        plt.subplot(2,5,it)\n",
        "        plt.imshow(image[img,:,:,:] if statut == 0 else imageBruitee[img,:,:,:])\n",
        "        it += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-97986693ffdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmax_it\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# image,imageBruitee = next_batch_bruit_voile_2(5,train_data,256,np.float32,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#                                                             [[0,1.-iteration/(max_it+1)],[0,1.-iteration/(max_it+1)],[0,1.-iteration/(max_it+1)]],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'create_datasets' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTiBXKguCD7c",
        "colab_type": "text"
      },
      "source": [
        "# Conversions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AO_NSKJdVoS9",
        "colab": {}
      },
      "source": [
        "def LarrayFloatToUint(L):\n",
        "    return [np.array(array,np.uint) for array in L]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hp8DhcxaEKU-",
        "colab": {}
      },
      "source": [
        "def convertToUintL(L):\n",
        "    Lresult = []\n",
        "    print(\"Entree : \",len(L))\n",
        "    for i in range(len(L)):\n",
        "        Lresult.append(np.array(normalisation(L[i],[0,1],[0,255]),dtype=np.uint8))\n",
        "    print(\"Sortie : \",len(Lresult))\n",
        "    return Lresult"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drdUov_iYAZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convertToUint(array):\n",
        "    return np.array(normalisation(array,[0,1],[0,255]),dtype=np.uint8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkYZQGhzV3ff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cf https://stackoverflow.com/questions/43784921/how-to-display-custom-images-in-tensorboard-using-keras?noredirect=1#comment85726690_43784921\n",
        "def make_image(tensor):\n",
        "    \"\"\"\n",
        "    Convert an numpy representation image to Image protobuf.\n",
        "    Copied from https://github.com/lanpa/tensorboard-pytorch/\n",
        "    \"\"\"\n",
        "    from PIL import Image\n",
        "    tensor = np.stack((tensor,tensor,tensor),axis=-1)\n",
        "    height, width, channel = tensor.shape # numpy.ndarray\n",
        "    image = Image.fromarray(tensor)\n",
        "    import io\n",
        "    output = io.BytesIO()\n",
        "    image.save(output, format='PNG')\n",
        "    image_string = output.getvalue()\n",
        "    output.close()\n",
        "    CHANNEL = 1\n",
        "    var = tf.Summary.Image(height=height,\n",
        "                         width=width,\n",
        "                         colorspace=CHANNEL,\n",
        "                         encoded_image_string=image_string)\n",
        "    print(\"var : \",var)\n",
        "    return var"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZODUIlFtaG3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_shape(x):\n",
        "    print(\"Shape : \",x.get_shape().as_list())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOZZ2bGdMLEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tauxApprentissage(epoch,ampl,tau,lim):\n",
        "    taux = ampl*10**-((epoch)/tau)\n",
        "    return taux if taux > lim else lim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wSOGJdQj6dR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def differenceAcceptee(epoch,ampl,tau,lim):\n",
        "    taux = ampl*10**-((epoch)/tau)\n",
        "    return taux if taux > lim else lim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7lAEUgOBRzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_img(img_array_normalized,path):\n",
        "    assert type(img_array_normalized) == np.ndarray, \"Pass a numpy array\"\n",
        "    assert len(img_array_normalized.shape) == 4, \"Pass a tensor, a batch of images with 4 dimensions\"\n",
        "    assert img_array_normalized.shape[-1] == 3, \"Pass a tensor with 3 channels at the end to build rgb images\"\n",
        "    assert np.max(img_array_normalized) <=1, \"Max tensor value must be 1 no %f\"%np.max(img_array_normalized)\n",
        "    individual_path = path\n",
        "    for i in range(img_array_normalized.shape[0]):\n",
        "        individual_path = path + \"_batchIndex_%d\"%(i)\n",
        "        cv2.imwrite(path+'.jpg', np.uint8(img_array_normalized[i,:,:,:]*255))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JpfoJ7mDnZo",
        "colab_type": "text"
      },
      "source": [
        "## Layers graph mode implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90I4K2JIEE6A",
        "colab_type": "text"
      },
      "source": [
        "### Graph, Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUIaa1v66tE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rgb_to_magn_angle(x,draw=True):\n",
        "    x,graph,index,improvements = x[0],x[1],x[2],x[3]\n",
        "    x = Lambda(lambda x:K.cast(x,\"complex64\"))(x)\n",
        "    x_list_magn = []\n",
        "    x_list_angle = []\n",
        "    for i in range(3):\n",
        "        fft = Lambda(lambda x: tf.signal.fft2d(x[:,:,:,i]), output_shape=(None,199,199))(x)\n",
        "        x_list_magn.append(Lambda(lambda fft:K.expand_dims(tf.math.abs(fft),axis=-1), output_shape=(None,199,199))(fft))\n",
        "        x_list_angle.append(Lambda(lambda fft: K.expand_dims(tf.math.angle(fft),axis=-1), output_shape=(None,199,199))(fft))\n",
        "    magn = Concatenate()(x_list_magn)\n",
        "    angle = Concatenate()(x_list_angle)\n",
        "    magn = Lambda(lambda magn: K.cast(magn,dtype=tf.float32), output_shape=(None,199,199))(magn)\n",
        "    angle = Lambda(lambda angle: K.cast(angle,dtype=tf.float32), output_shape=(None,199,199))(angle)\n",
        "    if draw == True:\n",
        "#         print(\"Draw FFT at index %s\"%g_get_current_id())\n",
        "        graph.node(g_get_current_id(),\"FFT\")\n",
        "        g_link(graph,index,g_get_current_id())\n",
        "    global index_couches\n",
        "    index_couches += 1\n",
        "    return [magn,graph,g_get_past_id(),improvements],[angle,graph,g_get_past_id(),improvements]\n",
        "def to_rgb(magn,angle,magn_train,angle_train,draw=True):\n",
        "    magn,graph,magn_index,improvements = extract_inpt(magn)\n",
        "    angle,graph,angle_index,improvements = extract_inpt(angle)\n",
        "    magn = Lambda(lambda magn: K.cast(magn,dtype=tf.complex64))(magn)\n",
        "    angle = Lambda(lambda angle:K.cast(angle,dtype=tf.complex64))(angle)\n",
        "    x_list_canal = []\n",
        "    for i in range(3):\n",
        "        complx_tensor = Lambda(lambda mang_angle: mang_angle[0][:,:,:,i]*tf.exp(1j*mang_angle[1][:,:,:,i]))([magn,angle])\n",
        "        ifft = Lambda(tf.signal.ifft2d)(complx_tensor)\n",
        "        x_list_canal.append(K.expand_dims(ifft,axis=-1))\n",
        "    images = Concatenate()(x_list_canal)\n",
        "    images = K.cast(images,dtype=tf.float32)\n",
        "    if draw == True:\n",
        "        graph.node(g_get_current_id(),\"RGB\")\n",
        "        if magn_train == True:\n",
        "            g_link(graph,magn_index,g_get_current_id())\n",
        "        if angle_train == True:\n",
        "            g_link(graph,angle_index,g_get_current_id())\n",
        "    global index_couches\n",
        "    index_couches += 1\n",
        "    return [images,graph,g_get_past_id(),improvements]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPEtd5pfNtVp",
        "colab_type": "text"
      },
      "source": [
        "###Couches composites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYfoDfstTrFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unit(inpt,k,f,dropout_r=None,activation=False,draw=False,init_index=None,identifier=False):\n",
        "    if identifier == True:\n",
        "        print(\"Identifier : \",identifier)\n",
        "    if init_index != None:\n",
        "        global index_couches\n",
        "        index_couches = init_index\n",
        "    couche = conv(inpt,k=k,f=f,s=1,draw=draw,identifier=identifier)\n",
        "    couche = b_norm(couche,draw=draw,identifier=identifier)\n",
        "    couche = lrn(couche,n=21,k=2,a=10**-4,b=0.75,draw=draw,identifier=identifier)\n",
        "    if activation == True:\n",
        "        couche = activ(couche,act_type=\"SELU\",draw=draw,identifier=identifier)\n",
        "    if dropout_r != None:\n",
        "        couche = dropout(couche,r=dropout_r,draw=draw,identifier=identifier)\n",
        "    return couche"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jueETQ6eSLbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compare_shape(t1,t2):\n",
        "    if len(t1) != len(t2):\n",
        "        return False\n",
        "    for elem1,elem2 in zip(t1,t2):\n",
        "        if elem1 != elem2:\n",
        "            return False\n",
        "    return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVQP3hmzBd8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Because we need to have both the former layer and the layer resulting of the next floor we have to split the \"etage\" in an down function and an up function that will create respectively the left and the right part \n",
        "def etage_down(inpt_before,pool_k,k,f,dropout_r=None,activation=False,draw=False,init_index=None):\n",
        "    \"\"\"We consider that a treatment has been already done on the inpt_before layer ;\n",
        "    on the other side the etage function will restitute a layer with dimensions that match with inpt_before ;\n",
        "    if we are at the last etage we will only call one / multiple units with the two ouputs layers\n",
        "    In order to keep mostly the same weights we will always keep max pool and deconv layers in the process with deconv non trainable --> in the corresponding layers\"\"\"\n",
        "    reduced = max_p(inpt_before,pool_k,draw=draw)\n",
        "    after_process = unit(reduced,k,f,dropout_r=dropout_r,activation=True,draw=draw,init_index=init_index)\n",
        "    return after_process\n",
        "\n",
        "def etage_up(inpt_before,inpt_after,after_process,k,f,dropout_r=None,activation=False,draw=False,init_index=None):\n",
        "    _,_,_,improvements_size_modif = extract_inpt(inpt_before)\n",
        "    _,_,_,improvements_processing = extract_inpt(after_process)\n",
        "    \n",
        "    inpt_after[3] = improvements_processing\n",
        "    after_process[3] = improvements_processing\n",
        "    inpt_after = concat([inpt_after,after_process],operation=False, identifier=False,draw=draw)\n",
        "    inpt_after_processed = unit(inpt_after,k,f,dropout_r=dropout_r,activation=activation,draw=draw,init_index=init_index)\n",
        "    \n",
        "    output = deconv(inpt_after_processed,k=2,f=inpt_before[0].get_shape().as_list()[-1],strides=(2,2),\n",
        "                    tmp=False,draw=draw)\n",
        "    if compare_shape(output[0].get_shape().as_list(),inpt_before[0].get_shape().as_list()) != True:\n",
        "        raise Exception(\"No appropriate deconvolution kernel found with desired shape %s\\nand input of deconvolution %s\"%(inpt_before[0].get_shape().as_list(),output[0].get_shape().as_list()))\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTnMwNPiD0zO",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2ghwubNSbaf",
        "colab_type": "text"
      },
      "source": [
        "## Global model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG6ZvYqrBhRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inception(inpt,draw=True):\n",
        "    orig_graph, inpt = begin_cluster(past_couche=inpt,name=\"Inception%s\"%(g_get_current_id()),color='red')\n",
        "    \n",
        "    T0 = unit(inpt,k=1,f=100,activation=True,dropout_r=0.5,draw=draw)\n",
        "    T1 = unit(inpt,k=1,f=100,activation=True,dropout_r=0.5,draw=draw)\n",
        "    T1 = unit(T1,k=1,f=100,activation=True,dropout_r=0.5,draw=draw)\n",
        "\n",
        "    couche = concat([T0,T1],draw=draw)\n",
        "    couche = conv(couche,k=1,f=100,s=1,draw=draw)\n",
        "    couche = add([couche,inpt],draw=draw)\n",
        "    end_cluster(last_couche=couche, orig_graph=orig_graph)\n",
        "    return couche"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o352RNtxhvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sauvegardeModele(entree_pure,entree_deterioree,model,iteration_entrainement,summary_writer,batch_size=5):\n",
        "    global Llayers\n",
        "    for canal_image in range(3):\n",
        "        summary_image = tf.Summary(value=[tf.Summary.Value(tag=\"input_pure_canal_\"+str(canal_image), \n",
        "                                image=make_image(np.array(entree_pure[0][0,:,:,canal_image]*255,dtype=np.uint8)))])\n",
        "        summary_writer.add_summary(summary_image,iteration_entrainement)\n",
        "        summary_image = tf.Summary(value=[tf.Summary.Value(tag=\"input_deterioree_canal_\"+str(canal_image), \n",
        "                                image=make_image(np.array(entree_deterioree[0][0,:,:,canal_image]*255,dtype=np.uint8)))])\n",
        "        summary_writer.add_summary(summary_image,iteration_entrainement)\n",
        "        \n",
        "    for p,entree in enumerate([entree_pure,entree_deterioree]):\n",
        "        print(\"p value %d\"%(p))\n",
        "        layer_outputs,layer_names = [Llayers[i][0] for i in range(len(Llayers))],[Llayers[i][1] for i in range(len(Llayers))]\n",
        "        model_calcul_image = Model(inputs=model.input,outputs=[model.output]+layer_outputs)\n",
        "        sorties_couches = model_calcul_image.predict(entree, batch_size=batch_size)[1:] if len(layer_outputs) > 0 else [model_calcul_image.predict(entree, batch_size=batch_size)][1:]\n",
        "        \n",
        "        for index_couche,sortie_couche in enumerate(sorties_couches):\n",
        "            layer_name = layer_names[index_couche]\n",
        "            dim_sortie = sortie_couche.shape\n",
        "            if len(dim_sortie) == 4:\n",
        "                for canal_image in range(dim_sortie[-1]):\n",
        "                    tag = layer_name\n",
        "                    tag += 'pure' if p == 0 else 'deterioree'\n",
        "                    tag += \"_canal_\" + str(canal_image)\n",
        "                    summary_image = tf.Summary(value=[tf.Summary.Value(tag=tag, \n",
        "                                            image=make_image(np.array(sortie_couche[0,:,:,canal_image]*255,dtype=np.uint8)))])\n",
        "                    summary_writer.add_summary(summary_image,iteration_entrainement)\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1xaVw_4SfiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import output\n",
        "def beep():\n",
        "    output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3fMqsjEMJTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets\n",
        "import time\n",
        "continuer = None\n",
        "def block_or_continue():\n",
        "    beep()\n",
        "    print(\"Would you resume training ? True or False ?\")\n",
        "    return bool(input())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB7xsmOEThUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backup(image,imageBruitee,gen_disc,disc,index,summary_writer):\n",
        "    x = [np.concatenate((image[0],imageBruitee[0])),np.concatenate((image[1],imageBruitee[1]))]\n",
        "    sauvegardeModele(image,imageBruitee,gen_disc,index,summary_writer)\n",
        "    p_gen_disc = gen_disc.predict(x)[0]\n",
        "    p_disc = disc.predict(x)[0]\n",
        "\n",
        "    summary_loss = tf.Summary(value=[tf.Summary.Value(tag=\"Probabilité gen-disc img 0 (orig pure) pure : \", \n",
        "                                        simple_value=p_gen_disc) ])\n",
        "    summary_writer.add_summary(summary_loss,index)\n",
        "    summary_loss = tf.Summary(value=[tf.Summary.Value(tag=\"Probabilité disc img 0 (orig pure) pure : \", \n",
        "                                        simple_value=p_disc) ])\n",
        "    summary_writer.add_summary(summary_loss,index)\n",
        "    x_proba = [np.concatenate((imageBruitee[0],imageBruitee[0])),np.concatenate((imageBruitee[1],imageBruitee[1]))]\n",
        "    p_gen_disc = gen_disc.predict(x_proba)[0]\n",
        "    p_disc = disc.predict(x_proba)[0]\n",
        "    summary_loss = tf.Summary(value=[tf.Summary.Value(tag=\"Probabilité gen-disc img 0 (orig détériorée) pure : \", \n",
        "                                        simple_value=p_gen_disc) ])\n",
        "    summary_writer.add_summary(summary_loss,index)\n",
        "    summary_loss = tf.Summary(value=[tf.Summary.Value(tag=\"Probabilité disc img 0 (orig détériorée) pure : \", \n",
        "                                        simple_value=p_disc) ])\n",
        "    summary_writer.add_summary(summary_loss,index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39XpimsoQARk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_loss(y_true,y_pred):\n",
        "    return K.mean(K.square(y_pred - y_true),axis=-1)+K.max(K.square(y_pred - y_true),axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDAX-aHtopZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_fft(img_tensor):\n",
        "    L = []\n",
        "    print(img_tensor.shape[0],img_tensor.get_shape().as_list())\n",
        "    for index_image in range(img_tensor.get_shape().as_list()[0]):\n",
        "        L.append(to_fft_single(img_tensor[index_image,:,:,:]))\n",
        "    return np.stack(L,axis=0)\n",
        "\n",
        "def to_img(fft_cplx):\n",
        "    return to_uint8_array(np.fft.ifft2(fft_cplx))\n",
        "def to_fft_single(img):\n",
        "    fft_cplx = np.fft.fft2(img)\n",
        "    fft = 20*np.log(np.abs(fft_cplx)+10**-5*np.ones(img.shape))\n",
        "    fft = fft + np.abs(np.min(fft))*np.ones(fft.shape)\n",
        "    fft = fft / (np.max(fft)*np.ones(fft.shape))*(255*np.ones(fft.shape))\n",
        "    fft_uint8 = to_uint8_array(fft)\n",
        "    print(np.max(fft_uint8),np.min(fft_uint8),fft_uint8.shape,fft_uint8.dtype)\n",
        "    return fft_cplx\n",
        "\n",
        "def to_uint8_array(array):\n",
        "    return array.astype(np.uint8)\n",
        "def next_batch_bruit_voile_fft():\n",
        "    image,imageBruitee = next_batch_bruit_voile_2(10,images,199,np.float32,[1,1,1],[50,50])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH193F6WSWyo",
        "colab_type": "text"
      },
      "source": [
        "## FFT Pretraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odSEq99jmnDl",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq4QVh3eceLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def analyse_progressive_training_model(couche,step=0,init=2,draw=True):\n",
        "    #input size has to be 2**4=16 min but we wil take 2**8=256\n",
        "    level0_in = unit(couche,k=2,f=100,activation=True,dropout_r=0.1,init_index=init,draw=draw)#Output size around 256\n",
        "    down1 = etage_down(level0_in,pool_k=2,k=2,f=100,dropout_r=0.1,activation=True,draw=draw)#Output size around 128\n",
        "    down2 = etage_down(down1,pool_k=2,k=2,f=100,dropout_r=0.1,activation=True,draw=draw)#Output size around 64\n",
        "    down3 = etage_down(down2,pool_k=2,k=2,f=100,dropout_r=0.1,activation=True,draw=draw)#Output size around 32\n",
        "    down4 = etage_down(down3,pool_k=2,k=2,f=100,dropout_r=0.1,activation=True,draw=draw)#Output size around 16\n",
        "    \n",
        "    final_analysis = unit(down4,k=2,f=100,activation=True,draw=draw)#Output size around 16\n",
        "    \n",
        "    up4 = etage_up(inpt_before=down3,inpt_after=final_analysis,after_process=down4,k=2,f=100,dropout_r=0.1,activation=True,draw=draw)#Output size around 32\n",
        "    up3 = etage_up(inpt_before=down2,inpt_after=up4,after_process=down3,k=2,f=100,dropout_r=0.1,activation=True,draw=draw)#Output size around 64\n",
        "    up2 = etage_up(inpt_before=down1,inpt_after=up3,after_process=down2,k=2,f=100,dropout_r=0.1,activation=True,draw=draw)#Output size around 128\n",
        "    up1 = etage_up(inpt_before=level0_in,inpt_after=up2,after_process=down1,k=2,f=100,dropout_r=0.1,activation=True,draw=draw)#Output size around 256\n",
        "    level0_out = concat([level0_in,up1],draw=draw)\n",
        "    level0_out[3] = couche[3]\n",
        "    level0_out = unit(level0_out,k=2,f=3,activation=True,dropout_r=0.1,draw=draw)#Bonne taille\n",
        "    return  level0_out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocq7NRIS6Egq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index_couches = -12\n",
        "def model_fft_training(step,half_batch_size,name,magn_select=True,angle_select=True):#else angle\n",
        "    global index_disc\n",
        "    global index_couches\n",
        "    index_couches = 1\n",
        "    graph = new_graph()\n",
        "    inpt = g_input(graph=graph, improvements=step, shape=(256, 256, 3),content_name=\"Image\")\n",
        "    inpt_real = g_input(graph=graph, improvements=step, shape=(256, 256, 3),content_name=\"Image_real\",draw=False)\n",
        "\n",
        "    magn,angle = rgb_to_magn_angle(inpt,draw=True)\n",
        "    r_magn,r_angle = rgb_to_magn_angle(inpt_real,draw=False)\n",
        "\n",
        "    if magn_select == True:\n",
        "        magn = analyse_progressive_training_model(magn,step=step,init=3)\n",
        "    if angle_select == True:\n",
        "        angle = analyse_progressive_training_model(angle,step=step,init=150)\n",
        "    img_rgb = to_rgb(magn=magn,angle=angle,magn_train=magn_select,angle_train=angle_select) \n",
        "    model = Model(inputs=[inpt[0],inpt_real[0]],outputs=img_rgb[0],name='gen')\n",
        "    end_graph(img_rgb[1],name=name)\n",
        "    return model, r_magn[0],r_angle[0], magn[0], angle[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsoRVtreIg6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# validation = None\n",
        "# for step in range(38,100):#21\n",
        "#     for training_L in [[False,True],[True,False],[True,True]]:\n",
        "#         base_name = \"Pretrain_test_fft_%d\"%10\n",
        "#         graph_name = base_name\n",
        "#         if training_L[0] == True:\n",
        "#             graph_name += \"_magn\"\n",
        "#         if training_L[1] == True:\n",
        "#             graph_name += \"_angle\"\n",
        "#         graph_name = graph_name + \"_%d\"%(step)\n",
        "#         backup_path = graph_name + \".h5\"\n",
        "#         global index_couches\n",
        "#         model_fft_training(step,half_batch_size=5,name=graph_name,magn_select=training_L[0],angle_select=training_L[1])\n",
        "#         print(\"Is it a correct graph for step %d angle %r et magn %r ?\"%(step,training_L[0],training_L[1]))\n",
        "#         print(\"path : %s\"%('%s.png'%(graph_name)))\n",
        "#         graph = imgIPython(filename='%s.png'%(graph_name)) \n",
        "#         display(graph)\n",
        "#         validation = input()\n",
        "#         if validation == \"e\":\n",
        "#             break\n",
        "#         if validation != \"t\":\n",
        "#             raise Exception(\"The graph is not correct\")\n",
        "    \n",
        "#     if validation == \"e\":\n",
        "#         break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PK0jE-_mijZ",
        "colab_type": "text"
      },
      "source": [
        "### Backup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuRmrZ47b9aP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def restore_modified_network(past_model,new_model):\n",
        "    layers = [layer.name for layer in new_model.layers]\n",
        "    for layer in past_model.layers:\n",
        "        if layer.name in layers:\n",
        "            print(\"Restoring layer %s\"%layer.name)\n",
        "            try:\n",
        "                new_model.get_layer(layer.name).set_weights(layer.get_weights())\n",
        "            except Exception as e:\n",
        "                print(\"Can't load weights because \")\n",
        "                print(str(e))\n",
        "                print(\"So, nothing done for the layer %s\"%(layer.name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwGgsIqMUEQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def search_restore_pt(index_model,model,magn_trained,angle_trained,base_name):\n",
        "    best_angle_training = None\n",
        "    best_magn_training = None\n",
        "    max_step_angle = 0\n",
        "    max_step_magn = 0\n",
        "    folder = \"Model%d/\"%index_model\n",
        "    if os.path.exists(folder) == False:\n",
        "        print(\"No restore point found\")\n",
        "        return None\n",
        "    for file in os.listdir(folder):\n",
        "        if file.endswith(\".h5\") and base_name in file:\n",
        "            if \"magn\" in file and int(file.split(\".\")[0].split(\"_\")[-1]) >= max_step_magn:\n",
        "                best_magn_training = folder + file\n",
        "                max_step_magn = int(file.split(\".\")[0].split(\"_\")[-1])\n",
        "            if \"angle\" in file and int(file.split(\".\")[0].split(\"_\")[-1]) >= max_step_angle:\n",
        "                best_angle_training = folder + file\n",
        "                max_step_angle = int(file.split(\".\")[0].split(\"_\")[-1])\n",
        "    # Pour cette étape d'entrainement, \n",
        "    #       - soit uniquement l'un des deux a été entrainé séparément : cas où à l'iteration step on arrive à magn = True et angle = True \n",
        "    #               -> magn déjà entrainé pour step mais angle en est encore à step-1\n",
        "    #           --> on restaure d'abord le fichier contenant angle et magn de l'étape précédante puis celui d'entrainement particulier\n",
        "    #           --> soit d'abord dans tous les cas le fichier de angle puis celui de magn\n",
        "    #       - soit les deux ont été entrainés séparément\n",
        "    #           --> on restaure les deux fichier sans ordre préférentiel car chaque fichier ne contient pas d'info sur un reseau analysant l'autre partie de l'image\n",
        "    #       - soit les deux ont été entrainés en commun \n",
        "    #           --> on restaure d'abord le fichier contenant angle et magn de l'étape précédante puis celui d'entrainement particulier\n",
        "    # Dans l'ordre on fait angle1 -> magn1 -> magn_angle1 -> angle2 -> magn2 -> magn_angle2 -> ...\n",
        "    if (angle_trained == True and magn_trained == False) or (angle_trained == False and magn_trained == True) and (best_angle_training != None and best_magn_training != None):\n",
        "        # Cas angle2 ou magn2\n",
        "        print(\"Restoring weights...\")\n",
        "        model_prec,_,_, _,_ = model_fft_training(step=max_step_magn,#To avoid problems after training angle of this step\n",
        "                                                                           half_batch_size=5,\n",
        "                                                                  name=graph_name,magn_select=True,\n",
        "                                                                  angle_select=True)\n",
        "        model_prec.load_weights(best_angle_training,by_name=True)\n",
        "        restore_modified_network(past_model=model_prec,new_model=model)\n",
        "    elif (angle_trained == True and magn_trained == True) and (best_angle_training != None and best_magn_training != None):\n",
        "        print(\"Restoring weights from models angle_magn into magn_angle...\")\n",
        "        # Cas magn_angle1 : on construit angle1 et magn1 et on met les variables dans le nouveau modèle\n",
        "        model_prec_angle,_,_, _,_ = model_fft_training(step=max_step_magn,half_batch_size=5,\n",
        "                                                                  name=base_name,magn_select=True,\n",
        "                                                                  angle_select=True)\n",
        "        model_prec_magn,_,_, _,_ = model_fft_training(step=max_step_magn,half_batch_size=5,\n",
        "                                                                  name=base_name,magn_select=True,\n",
        "                                                                  angle_select=True)\n",
        "        model_prec_angle.load_weights(best_angle_training,by_name=True)\n",
        "        restore_modified_network(past_model=model_prec_angle,new_model=model)\n",
        "        model_prec_magn.load_weights(best_magn_training,by_name=True)\n",
        "        restore_modified_network(past_model=model_prec_magn,new_model=model)\n",
        "    else:\n",
        "        print(\"%s restore point found for angle and %s restore point found for magn\"%(\"No\" if best_angle_training == None else \"One\",\"no\" if best_magn_training == None else \"one\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfvAihJ3eQwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backup_fft(num_model,model,image,imageBruitee,graph_name,step,iteration,metrics_list_names,model_state_backup_int=5):\n",
        "    if os.path.exists(\"Model%d\"%num_model) == False:\n",
        "        os.mkdir(\"Model%d\"%num_model)\n",
        "    folder = \"Model%d/\"%num_model\n",
        "    metrics_list_values_clean = model.test_on_batch([image,image],image)\n",
        "    metrics_list_values_noise = model.test_on_batch([imageBruitee,image],image)\n",
        "    if len(metrics_list_values_clean) != len(metrics_list_names) and len(metrics_list_values_noise) != len(metrics_list_names):\n",
        "        raise Exception(\"Metrics names number and model outputs not matching with %d values for model output and %d values for names\"%(len(metrics_list_values_clean),len(metrics_list_names)))\n",
        "    metrics_names = []\n",
        "    metrics_list_values = []\n",
        "    for i,name in enumerate(metrics_list_names):\n",
        "        metrics_names += [name+\"_clean\",name+\"_noise\"]\n",
        "        metrics_list_values += [metrics_list_values_clean[i],metrics_list_values_noise[i]]\n",
        "    metrics_names = [\"Iteration\"] + metrics_names\n",
        "    metrics_list_values = [iteration] + metrics_list_values\n",
        "    if iteration % model_state_backup_int == 0:\n",
        "        model.save_weights(folder + graph_name+\".h5\")\n",
        "        save_img(image,folder + graph_name+\"_%d_img\"%iteration)\n",
        "        save_img(imageBruitee,folder + graph_name+\"_%d_img_bruit\"%iteration)\n",
        "        img_gen = model.predict([image,image])\n",
        "        img_bruit_gen = model.predict([imageBruitee,image])\n",
        "        save_img(np.clip(img_gen,0,1),folder + graph_name+\"_%d_img_gen\"%iteration)\n",
        "        save_img(np.clip(img_bruit_gen,0,1),folder + graph_name+\"_%d_img_bruit_gen\"%iteration)\n",
        "\n",
        "    append_write = \"w\"\n",
        "    backup_file_path = folder + graph_name+\"_step%d_metrics.txt\"%step\n",
        "    if os.path.exists(backup_file_path):\n",
        "        append_write = 'a' # append if already exists\n",
        "    with open(backup_file_path,append_write) as file:\n",
        "        if os.stat(backup_file_path).st_size == 0:\n",
        "            file.write(\",\".join(list(map(lambda x:str(x),metrics_names)))+\"\\n\")\n",
        "        file.write(\",\".join(list(map(lambda x:str(x),metrics_list_values)))+\"\\n\")\n",
        "    print(\"\".join([metrics_names[i] + \" : \" + str(metrics_list_values[i]) + \"; \" for i in range(len(metrics_list_values))]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc6r3TZnmq1V",
        "colab_type": "text"
      },
      "source": [
        "### Loss & Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfMItCyYTF6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fft_loss(r_magn,r_angle, gen_magn,gen_angle):\n",
        "    def loss(y_true,y_pred):\n",
        "        return K.mean(K.square(r_magn - gen_magn), axis=-1) + K.mean(K.square(r_angle - gen_angle), axis=-1)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3Jha5tSOztJT",
        "colab": {}
      },
      "source": [
        "Lmetrics = []\n",
        "Lnames = []\n",
        "def generate_functions(name,type_data,input_first_function,expression):\n",
        "    global Lmetrics\n",
        "    global Lnames\n",
        "    Lnames.append(\"%s\"%(expression).replace(\"y\",\"rgb\"))\n",
        "    Lmetrics.append(\"metrics_%s_%s(%s)\"%(type_data,name,\",\".join(input_first_function)))\n",
        "    print(\"def metrics_%s_%s(%s):\\n\\tdef %s_%s(y_true,y_pred):\\n\\t\\treturn %s\\n\\treturn %s_%s\"%(type_data,name,\",\".join(input_first_function),type_data,name,expression,type_data,name))\n",
        "    print()\n",
        "for minmax in [\"min\",\"max\"]:\n",
        "    for magnangle in [\"magn\",\"angle\"]:\n",
        "        generate_functions(\"%s_%s\"%(minmax,magnangle),\"fft\",[\"r_%s\"%(magnangle)],\"K.%s(r_%s)\"%(minmax,magnangle))\n",
        "for magnangle in [\"magn\",\"angle\"]:\n",
        "    generate_functions(\"max_abs_diff_%s\"%(magnangle),\"fft\",[\"r_%s\"%(magnangle),\"gen_%s\"%(magnangle)],\"K.max(K.abs(r_%s-gen_%s))\"%(magnangle,magnangle))\n",
        "for magnangle in [\"magn\",\"angle\"]:\n",
        "    generate_functions(\"mean_abs_diff_%s\"%(magnangle),\"fft\",[\"r_%s\"%(magnangle),\"gen_%s\"%(magnangle)],\"K.mean(K.abs(r_%s-gen_%s))\"%(magnangle,magnangle))\n",
        "for magnangle in [\"magn\",\"angle\"]:\n",
        "    generate_functions(\"var_abs_diff_%s\"%(magnangle),\"fft\",[\"r_%s\"%(magnangle),\"gen_%s\"%(magnangle)],\"K.var(K.abs(r_%s-gen_%s))\"%(magnangle,magnangle))\n",
        "generate_functions(\"max_abs_diff_%s\"%(magnangle),\"rgb\",[],\"K.mean(K.abs(y_true-y_pred))\")\n",
        "print(\"[%s]\"%(\",\".join(Lmetrics)))\n",
        "print(\"[%s]\"%(\",\".join(list(map(lambda x:\"'\"+x+\"'\",Lnames)))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fBvWsQPHz0br",
        "colab": {}
      },
      "source": [
        "def metrics_fft_min_magn(r_magn):\n",
        "\tdef fft_min_magn(y_true,y_pred):\n",
        "\t\treturn K.min(r_magn)\n",
        "\treturn fft_min_magn\n",
        "\n",
        "def metrics_fft_min_angle(r_angle):\n",
        "\tdef fft_min_angle(y_true,y_pred):\n",
        "\t\treturn K.min(r_angle)\n",
        "\treturn fft_min_angle\n",
        "\n",
        "def metrics_fft_max_magn(r_magn):\n",
        "\tdef fft_max_magn(y_true,y_pred):\n",
        "\t\treturn K.max(r_magn)\n",
        "\treturn fft_max_magn\n",
        "\n",
        "def metrics_fft_max_angle(r_angle):\n",
        "\tdef fft_max_angle(y_true,y_pred):\n",
        "\t\treturn K.max(r_angle)\n",
        "\treturn fft_max_angle\n",
        "\n",
        "def metrics_fft_max_abs_diff_magn(r_magn,gen_magn):\n",
        "\tdef fft_max_abs_diff_magn(y_true,y_pred):\n",
        "\t\treturn K.max(K.abs(r_magn-gen_magn))\n",
        "\treturn fft_max_abs_diff_magn\n",
        "\n",
        "def metrics_fft_max_abs_diff_angle(r_angle,gen_angle):\n",
        "\tdef fft_max_abs_diff_angle(y_true,y_pred):\n",
        "\t\treturn K.max(K.abs(r_angle-gen_angle))\n",
        "\treturn fft_max_abs_diff_angle\n",
        "\n",
        "def metrics_fft_mean_abs_diff_magn(r_magn,gen_magn):\n",
        "\tdef fft_mean_abs_diff_magn(y_true,y_pred):\n",
        "\t\treturn K.mean(K.abs(r_magn-gen_magn))\n",
        "\treturn fft_mean_abs_diff_magn\n",
        "\n",
        "def metrics_fft_mean_abs_diff_angle(r_angle,gen_angle):\n",
        "\tdef fft_mean_abs_diff_angle(y_true,y_pred):\n",
        "\t\treturn K.mean(K.abs(r_angle-gen_angle))\n",
        "\treturn fft_mean_abs_diff_angle\n",
        "\n",
        "def metrics_fft_var_abs_diff_magn(r_magn,gen_magn):\n",
        "\tdef fft_var_abs_diff_magn(y_true,y_pred):\n",
        "\t\treturn K.var(K.abs(r_magn-gen_magn))\n",
        "\treturn fft_var_abs_diff_magn\n",
        "\n",
        "def metrics_fft_var_abs_diff_angle(r_angle,gen_angle):\n",
        "\tdef fft_var_abs_diff_angle(y_true,y_pred):\n",
        "\t\treturn K.var(K.abs(r_angle-gen_angle))\n",
        "\treturn fft_var_abs_diff_angle\n",
        "\n",
        "def metrics_rgb_max_abs_diff_angle():\n",
        "\tdef rgb_max_abs_diff_angle(y_true,y_pred):\n",
        "\t\treturn K.mean(K.abs(y_true-y_pred))\n",
        "\treturn rgb_max_abs_diff_angle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42Z12kIwmvJS",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy-tE33133y9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_progressive_model(index_model,max_step=200, max_it=2000):\n",
        "    train_data,validation_data,test_data = create_datasets()\n",
        "    max_total_it = (max_step-1)*3*max_it\n",
        "    tot_iteration = 0\n",
        "    for step in range(40,max_step):\n",
        "        magn_choice, angle_choice = True,True\n",
        "        if magn_choice == False and angle_choice == False:\n",
        "            continue\n",
        "\n",
        "        base_name = \"Pretrain_fft_%d\"%index_model\n",
        "        graph_name = base_name\n",
        "        if magn_choice == True:\n",
        "            graph_name += \"_magn\"\n",
        "        if angle_choice == True:\n",
        "            graph_name += \"_angle\"\n",
        "        graph_name = graph_name + \"_%d\"%(step)\n",
        "        backup_path = graph_name + \".h5\"\n",
        "        model,r_magn,r_angle, gen_magn,gen_angle = model_fft_training(step=step,half_batch_size=5,\n",
        "                                                                        name=graph_name,magn_select=magn_choice,angle_select=angle_choice)\n",
        "        \n",
        "        metrics_list = [metrics_fft_min_magn(r_magn),metrics_fft_min_angle(r_angle),\n",
        "                        metrics_fft_max_magn(r_magn),metrics_fft_max_angle(r_angle),\n",
        "                        metrics_fft_max_abs_diff_magn(r_magn,gen_magn),metrics_fft_max_abs_diff_angle(r_angle,gen_angle),\n",
        "                        metrics_fft_mean_abs_diff_magn(r_magn,gen_magn),metrics_fft_mean_abs_diff_angle(r_angle,gen_angle),\n",
        "                        metrics_fft_var_abs_diff_magn(r_magn,gen_magn),metrics_fft_var_abs_diff_angle(r_angle,gen_angle),\n",
        "                        metrics_rgb_max_abs_diff_angle()]\n",
        "        metrics_list_names = [\"Loss\",'K.min(r_magn)','K.min(r_angle)',\n",
        "                                'K.max(r_magn)','K.max(r_angle)',\n",
        "                                'K.max(K.abs(r_magn-gen_magn))','K.max(K.abs(r_angle-gen_angle))',\n",
        "                                'K.mean(K.abs(r_magn-gen_magn))','K.mean(K.abs(r_angle-gen_angle))',\n",
        "                                'K.var(K.abs(r_magn-gen_magn))','K.var(K.abs(r_angle-gen_angle))',\n",
        "                                'K.mean(K.abs(rgb_true-rgb_pred))']\n",
        "        optimizer = Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "        model.compile(loss=fft_loss(r_magn,r_angle, gen_magn,gen_angle),metrics=metrics_list,optimizer=\"Adam\")\n",
        "        search_restore_pt(index_model,model=model, magn_trained=magn_choice,angle_trained=angle_choice,\n",
        "                            base_name=graph_name)\n",
        "        for iteration in range(max_it):\n",
        "            if step == 40 and iteration < 1406:\n",
        "                continue\n",
        "            image,imageBruitee = next_batch_bruit_voile_2(5,train_data,256,np.float32,\n",
        "                                                            [[0,1.-iteration/(max_it+1)],[0,1.-iteration/(max_it+1)],[0,1.-iteration/(max_it+1)]],\n",
        "                                                            [iteration//100*100/max_it/2,iteration//100*100/max_it/2],[0,1])\n",
        "            x = np.concatenate((image,imageBruitee),axis=0)\n",
        "            y = np.concatenate((image,image),axis=0)\n",
        "            metrics_list_values = model.train_on_batch([x,y],y)\n",
        "            loss = metrics_list_values[0]\n",
        "            backup_fft(index_model,model,image,imageBruitee,graph_name,step,iteration,metrics_list_names,model_state_backup_int=5)\n",
        "            whose_is_training = \"\"\n",
        "            if magn_choice == True and angle_choice == True:\n",
        "                whose_is_training += \"magn et angle\" \n",
        "            elif magn_choice == True:\n",
        "                whose_is_training += \"magn\" \n",
        "            elif angle_choice == True:\n",
        "                whose_is_training += \"angle\" \n",
        "            print(\"Etape d'entrainement %d de %s iteration %d statut %d pourcent, erreur %.5e\"%(step,whose_is_training, iteration,tot_iteration/max_total_it,loss))\n",
        "            tot_iteration += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBzmhOxJ_qvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_progressive_model(11)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJGaD3wJglbR",
        "colab_type": "text"
      },
      "source": [
        "# Discriminator pretraining\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w4Y7R97jM1H",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4xgaXRpjOhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_model():\n",
        "    inpt = Input(shape=(256, 256, 3), name=\"Input\")\n",
        "    xception_model = tensorflow.keras.applications.xception.Xception(include_top=False, weights=None, input_tensor=inpt, input_shape=(256,256,3), pooling='max', classes=1)\n",
        "    filterModification = Dense(units=1,activation=None)(xception_model.output)\n",
        "    return Model(inputs=inpt,outputs=filterModification, name='disc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW_c4jsRgqJv",
        "colab_type": "text"
      },
      "source": [
        "## Losses & metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWfpXwi1eFfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Lmetrics = []\n",
        "Lnames = []\n",
        "generate_functions(\"max_abs_diff_%s\"%(magnangle),\"rgb\",[],\"K.max(K.abs(y_true-y_pred))\")\n",
        "generate_functions(\"mean_abs_diff_%s\"%(magnangle),\"rgb\",[],\"K.mean(K.abs(y_true-y_pred))\")\n",
        "generate_functions(\"std_abs_diff_%s\"%(magnangle),\"rgb\",[],\"K.std(K.abs(y_true-y_pred))\")\n",
        "print(\"[%s]\"%(\",\".join(Lmetrics)))\n",
        "print(\"[%s]\"%(\",\".join(list(map(lambda x:\"'\"+x+\"'\",Lnames)))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Xfuu33MgBXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics_rgb_max_abs_diff_angle():\n",
        "\tdef rgb_max_abs_diff_angle(y_true,y_pred):\n",
        "\t\treturn K.max(K.abs(y_true-y_pred))\n",
        "\treturn rgb_max_abs_diff_angle\n",
        "\n",
        "def metrics_rgb_mean_abs_diff_angle():\n",
        "\tdef rgb_mean_abs_diff_angle(y_true,y_pred):\n",
        "\t\treturn K.mean(K.abs(y_true-y_pred))\n",
        "\treturn rgb_mean_abs_diff_angle\n",
        "\n",
        "def metrics_rgb_std_abs_diff_angle():\n",
        "\tdef rgb_std_abs_diff_angle(y_true,y_pred):\n",
        "\t\treturn K.std(K.abs(y_true-y_pred))\n",
        "\treturn rgb_std_abs_diff_angle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC-kuitmgwe8",
        "colab_type": "text"
      },
      "source": [
        "## Backup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JuC82QjCg7pd",
        "colab": {}
      },
      "source": [
        "def search_restore_pt(index_model,model,base_name):\n",
        "    \"\"\"\n",
        "    We only have to restore previous version of the model\"\"\"\n",
        "    folder = \"Model_disc%d/\"%index_model\n",
        "    if os.path.exists(folder) == False:\n",
        "        print(\"No restore point found\")\n",
        "        return None\n",
        "    model_path = \"\"\n",
        "    for file in os.listdir(folder):\n",
        "        if file.endswith(\".h5\") and base_name in file:\n",
        "            model_path = file\n",
        "    if model_path == \"\":\n",
        "        print(\"No restore point found\")\n",
        "        return None\n",
        "    try:\n",
        "        model.load_weights(model_path,by_name=True)\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "        print(\"Skipping restoration\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HsmNpZZkRCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def append_or_write(backup_file_path):\n",
        "    mode =\"w\"\n",
        "    if os.path.exists(backup_file_path):\n",
        "        append_write = 'a'\n",
        "    return mode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haLnBU8ulD1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_to_file(backup_file_path,titles,data_list_list):\n",
        "    with open(backup_file_path,'a') as file:\n",
        "        if os.stat(backup_file_path).st_size == 0:\n",
        "            file.write(\",\".join(titles)+\"\\n\")\n",
        "        for line in data_list_list:\n",
        "            file.write(\",\".join(line)+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoRyudZnpAdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_input_output(img,imgNoise):\n",
        "    \"\"\"D'après la fonction backup_fft ci-dessus on prend comme convention que l'IA prédit la probabilité que \n",
        "    l'image d'entrée soit bruitée\n",
        "    \"\"\"\n",
        "    inpt = np.concatenate((img,imgNoise),axis=0)\n",
        "    outpt = np.zeros(shape=(inpt.shape[0]))\n",
        "    return inpt,outpt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4FdEcjErg7ph",
        "colab": {}
      },
      "source": [
        "def backup_fft(num_model,model,image,imageBruitee,graph_name,step,iteration,metrics_list_names,model_state_backup_int=5):\n",
        "    folder = \"Model_disc%d/\"%num_model\n",
        "    if os.path.exists(folder) == False:\n",
        "        os.mkdir(folder)\n",
        "    \n",
        "    outpt_clean = np.zeros(shape=(image.shape[0]))\n",
        "    outpt_noise = np.zeros(shape=(imageBruitee.shape[0]))\n",
        "\n",
        "    metrics_list_values_clean = model.test_on_batch(image,outpt_clean)\n",
        "    metrics_list_values_noise = model.test_on_batch(imageBruitee,outpt_noise)\n",
        "    if len(metrics_list_values_clean) != len(metrics_list_names) and len(metrics_list_values_noise) != len(metrics_list_names):\n",
        "        print(\"metrics_list_values_clean : \",metrics_list_values_clean)\n",
        "        print(\"metrics_list_names : \",metrics_list_names)\n",
        "        raise Exception(\"Metrics names number and model outputs not matching with %d values for model output and %d values for names\"%(len(metrics_list_values_clean),len(metrics_list_names)))\n",
        "    metrics_names = []\n",
        "    metrics_list_values = []\n",
        "    for i,name in enumerate(metrics_list_names):\n",
        "        metrics_names += [name+\"_clean\",name+\"_noise\"]\n",
        "        metrics_list_values += [metrics_list_values_clean[i],metrics_list_values_noise[i]]\n",
        "    metrics_names = [\"Iteration\"] + metrics_names\n",
        "    metrics_list_values = [iteration] + metrics_list_values\n",
        "    if iteration % model_state_backup_int == 0:\n",
        "        model.save_weights(folder + graph_name+\".h5\")\n",
        "        save_img(image,folder + graph_name+\"_%d_img\"%iteration)\n",
        "        save_img(imageBruitee,folder + graph_name+\"_%d_img_bruit\"%iteration)\n",
        "        prediction_clean_gen = model.predict(image)\n",
        "        prediction_clean_bruit_gen = model.predict(imageBruitee)\n",
        "        \n",
        "        backup_prediction_path = folder + graph_name + \"_step%d_predictions.txt\"%(step)\n",
        "        titles = [\"Prediction_bruitee_orig_clean\",\"Prediction_bruitee_orig_noise\"]\n",
        "        predictions = [str(prediction_clean_gen[0]),str(prediction_clean_bruit_gen[0])]\n",
        "        write_to_file(backup_file_path=backup_prediction_path,titles=titles,data_list_list=[predictions])\n",
        "\n",
        "    backup_file_path = folder + graph_name+\"_step%d_metrics.txt\"%step\n",
        "    titles = list(map(lambda x:str(x),metrics_names))\n",
        "    data = list(map(lambda x:str(x),metrics_list_values))\n",
        "    write_to_file(backup_file_path=backup_file_path,titles=titles,data_list_list=[data])\n",
        "    print(\"\".join([metrics_names[i] + \" : \" + str(metrics_list_values[i]) + \"; \" for i in range(len(metrics_list_values))]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etsrjWLhgu9f",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqQc-ZGyc8XR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_progressive_model(index_model,max_step=200, max_it=2000):\n",
        "    train_data,validation_data,test_data = create_datasets()\n",
        "    max_total_it = (max_step-1)*3*max_it\n",
        "    tot_iteration = 0\n",
        "    base_name = \"Pretrain_fft_disc_%d\"%index_model\n",
        "    for step in range(max_step):\n",
        "        graph_name = base_name\n",
        "        graph_name = graph_name + \"_%d\"%(step)\n",
        "        backup_path = graph_name + \".h5\"\n",
        "        model = generate_model()\n",
        "            \n",
        "        metrics_list = [metrics_rgb_max_abs_diff_angle(),metrics_rgb_mean_abs_diff_angle(),metrics_rgb_std_abs_diff_angle()]\n",
        "        metrics_list_names = ['binary_crossentropy','K.max(K.abs(rgb_true-rgb_pred))','K.mean(K.abs(rgb_true-rgb_pred))','K.std(K.abs(rgb_true-rgb_pred))']\n",
        "\n",
        "        model.compile(loss=\"binary_crossentropy\",metrics=metrics_list,optimizer=\"Adam\")\n",
        "        search_restore_pt(index_model,model=model,\n",
        "                            base_name=graph_name)\n",
        "        for iteration in range(max_it):\n",
        "            image,imageBruitee = next_batch_bruit_voile_2(5,train_data,256,np.float32,\n",
        "                                                            [[0,1.-iteration/(max_it+1)],[0,1.-iteration/(max_it+1)],[0,1.-iteration/(max_it+1)]],\n",
        "                                                            [iteration//100*100/max_it/2,iteration//100*100/max_it/2],[0,1])\n",
        "            inpt,outpt = generate_input_output(image,imageBruitee)\n",
        "            metrics_list_values = model.train_on_batch(inpt,outpt)\n",
        "            loss = metrics_list_values[0]\n",
        "            backup_fft(index_model,model,image,imageBruitee,graph_name,step,iteration,metrics_list_names,model_state_backup_int=5)\n",
        "            \n",
        "            print(\"Etape d'entrainement %d iteration %d statut %d pourcent, erreur %.5e\"%(step, iteration,tot_iteration/max_total_it,loss))\n",
        "            tot_iteration += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTF5jvnOoMlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_progressive_model(index_model=11)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8trBVnWlQuzb",
        "colab_type": "text"
      },
      "source": [
        "# GAN Pretrain FFT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4bIdpp-RB63",
        "colab_type": "text"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiuDfHs1DcXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index_couches = -12\n",
        "def model_training(step,half_batch_size,name,magn_select=True,angle_select=True):#else angle\n",
        "    global index_disc\n",
        "    global index_couches\n",
        "    index_couches = 1\n",
        "    graph = new_graph()\n",
        "    inpt = g_input(graph=graph, improvements=step, shape=(256, 256, 3),content_name=\"Image\")\n",
        "    output = analyse_progressive_training_model(inpt,step=step,init=3)\n",
        "    model = Model(inputs=[inpt[0]],outputs=output[0],name='gen')\n",
        "    end_graph(output[1],name=name)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbIGSHrHQ2Zk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "K.clear_session()\n",
        "def discriminateur():\n",
        "    return generate_model()\n",
        "def generate_models(name):\n",
        "    gen_model = model_training(step=50,half_batch_size=5,name=name)\n",
        "    gen = gen_model(gen_model.input)\n",
        "    disc = discriminateur()\n",
        "    disc_gan = disc(gen)\n",
        "    inpt_gen_output = Input(shape=(256,256,3))\n",
        "    return gen_model,disc,Model(inputs=[gen_model.input,inpt_gen_output],outputs=disc_gan),inpt_gen_output,gen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjnLQ2eFS2yX",
        "colab_type": "text"
      },
      "source": [
        "## Backup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifFaaAQPZCfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def search_restore_pt(index_model,model_gen_disc,base_name):\n",
        "    best_training = None\n",
        "    max_step = 0\n",
        "    folder = \"Model_gan_fft11/\"#%index_model\n",
        "    # if os.path.exists(folder) == False:\n",
        "    #     print(\"No restore point found\")\n",
        "    #     return None\n",
        "    for file in os.listdir(folder):\n",
        "        if file.endswith(\".h5\") and base_name in file:\n",
        "            print(\"file : \",file,\"gan\" in file, int(file.split(\".\")[0].split(\"_\")[-1]))\n",
        "            if \"gan\" in file and int(file.split(\".\")[0].split(\"_\")[-1]) >= max_step:\n",
        "                best_training = folder + file\n",
        "                max_step = int(file.split(\".\")[0].split(\"_\")[-1])\n",
        "    if (best_training == None):\n",
        "        print(\"No GAN training found ; Restoring from splited files\")\n",
        "        pathDisc = \"/content/drive/My Drive/TIPE/Model_disc11/Pretrain_fft_disc_11_4.h5\"#@param {type:\"string\"}\n",
        "        pathGen = \"/content/drive/My Drive/TIPE/Model11/Pretrain_fft_11_magn_angle_43.h5\"#@param {type:\"string\"}\n",
        "        ## Restore gen part\n",
        "        # model_prec_fft,_,_, _,_ = model_fft_training(step=43,half_batch_size=5,\n",
        "        #                                                           name=\"Pretrain_fft_11_magn_angle_4\",magn_select=True,\n",
        "        #                                                           angle_select=True)\n",
        "        # model_prec_fft.load_weights(pathGen,by_name=True)\n",
        "        # restore_modified_network(past_model=model_prec_fft,new_model=model_gen_disc)\n",
        "        ## Restore disc part\n",
        "        model_disc_prec = generate_model()\n",
        "        model_disc_prec.load_weights(pathDisc,by_name=True)\n",
        "        restore_modified_network(past_model=model_disc_prec,new_model=model_gen_disc)\n",
        "\n",
        "    elif (best_training != None):\n",
        "        print(\"Restoring weights...\")\n",
        "        model_gen_disc.load_weights(best_training)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdJicV07fswG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backup_fft(num_model,model_gen,model_disc,model_gan,image,imageBruitee,graph_name,step,iteration,metrics_list_names_gen,metrics_list_names_disc,model_state_backup_int=5):\n",
        "    \n",
        "    folder = \"Model_gan_fft%d/\"%num_model\n",
        "    if os.path.exists(folder) == False:\n",
        "        os.mkdir(folder)\n",
        "    metrics_list_names = [\"Iterations\"]\n",
        "    metrics_list_values = [iteration]\n",
        "    def format_metrics(current_metrics_list_names,metrics_list_values_clean,metrics_list_values_noise,metrics_list_names,metrics_list_values,identifiant=''):\n",
        "        for i,name in enumerate(current_metrics_list_names):\n",
        "            if identifiant != '':\n",
        "                metrics_list_names += [name+\"_%s_clean\"%identifiant,name+\"_%s_noise\"%identifiant]\n",
        "            else:\n",
        "                metrics_list_names += [name+\"_clean\",name+\"_noise\"]\n",
        "            metrics_list_values += [metrics_list_values_clean[i],metrics_list_values_noise[i]]\n",
        "        return metrics_list_names,metrics_list_values\n",
        "    metrics_list_values_clean = model_gen.test_on_batch(image,image)\n",
        "    metrics_list_values_noise = model_gen.test_on_batch(imageBruitee,image)\n",
        "    metrics_list_names, metrics_list_values = format_metrics(current_metrics_list_names=metrics_list_names_gen,metrics_list_values_clean=metrics_list_values_clean,metrics_list_values_noise=metrics_list_values_noise,metrics_list_names=metrics_list_names,metrics_list_values=metrics_list_values,identifiant='gen')\n",
        "\n",
        "    input_gen_clean = model_gen.predict(image)\n",
        "    input_gen_noise = model_gen.predict(imageBruitee)\n",
        "    oupt_disc_clean = model_disc.predict(input_gen_clean)\n",
        "    oupt_disc_noise = model_disc.predict(input_gen_noise)\n",
        "    outpt_clean = np.zeros(shape=(image.shape[0]))\n",
        "    outpt_noise = np.zeros(shape=(imageBruitee.shape[0]))\n",
        "    metrics_list_values_clean = model_disc.test_on_batch(image,outpt_clean)\n",
        "    metrics_list_values_noise = model_disc.test_on_batch(imageBruitee,outpt_noise)\n",
        "    metrics_list_names, metrics_list_values = format_metrics(current_metrics_list_names=metrics_list_names_disc,metrics_list_values_clean=metrics_list_values_clean,metrics_list_values_noise=metrics_list_values_noise,metrics_list_names=metrics_list_names,metrics_list_values=metrics_list_values,identifiant='disc')\n",
        "\n",
        "    if iteration % model_state_backup_int == 0:\n",
        "        model_gan.save_weights(folder + graph_name+\".h5\")\n",
        "        print(\"Weights saved at %s\"%(folder + graph_name+\".h5\"))\n",
        "        save_img(image,folder + graph_name+\"_%d_img\"%iteration)\n",
        "        save_img(imageBruitee,folder + graph_name+\"_%d_img_bruit\"%iteration)\n",
        "        print(\"Original images saved at %s\"%(folder + graph_name+\"_%d_img\"%iteration))\n",
        "        prediction_clean= model_gen.predict(image)\n",
        "        prediction_clean *= 1/np.max(prediction_clean)\n",
        "        prediction_noise = model_gen.predict(imageBruitee)\n",
        "        prediction_noise *= 1/np.max(prediction_noise)\n",
        "        save_img(prediction_clean,folder + graph_name+\"_%d_img_fromclean\"%iteration)\n",
        "        save_img(prediction_noise,folder + graph_name+\"_%d_img_fromnoise\"%iteration)\n",
        "\n",
        "        \n",
        "        backup_prediction_path = folder + graph_name + \"_step%d_predictions.txt\"%(step)\n",
        "        titles = [\"Prediction_bruitee_orig_clean\",\"Prediction_bruitee_orig_noise\"]\n",
        "        predictions = [str(oupt_disc_clean),str(oupt_disc_noise)]\n",
        "        write_to_file(backup_file_path=backup_prediction_path,titles=titles,data_list_list=[predictions])\n",
        "\n",
        "    backup_file_path = folder + graph_name+\"_step%d_metrics.txt\"%step\n",
        "    titles = list(map(lambda x:str(x),metrics_list_names))\n",
        "    data = list(map(lambda x:str(x),metrics_list_values))\n",
        "    write_to_file(backup_file_path=backup_file_path,titles=titles,data_list_list=[data])\n",
        "    print(\"\".join([metrics_list_names[i] + \" : \" + str(metrics_list_values[i]) + \"; \" for i in range(len(metrics_list_values))]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX4TGSxZPcLy",
        "colab_type": "text"
      },
      "source": [
        "## Losses and metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQXVPSkjcDSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def generate_functions(name,type_data,input_first_function,expression):\n",
        "    global Lmetrics\n",
        "    global Lnames\n",
        "    Lnames.append(\"%s\"%(expression).replace(\"y\",\"rgb\"))\n",
        "    Lmetrics.append(\"metrics_%s_%s(%s)\"%(type_data,name,\",\".join(input_first_function)))\n",
        "    print(\"def metrics_%s_%s(%s):\\n\\tdef %s_%s(y_true,y_pred):\\n\\t\\treturn %s\\n\\treturn %s_%s\"%(type_data,name,\",\".join(input_first_function),type_data,name,expression,type_data,name))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl9R5a7nPfJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Lmetrics = []\n",
        "Lnames = []\n",
        "type_metrique = ['probability'] #@param [\"['probability']\", \"['rgb']\", \"['fft']\", \"['rgb']\",\"['rgb','probability']\",\"['rgb','fft']\",\"['rgb','fft','probability']\"] {type:\"raw\"}\n",
        "for typeData in type_metrique:\n",
        "    for analyse in [\"max\",\"mean\",\"std\"]:\n",
        "        if typeData == \"fft\":\n",
        "            for magnangle in [\"magn\",\"angle\"]:\n",
        "                generate_functions(\"abs_diff_%s_%s\"%(magnangle,analyse),typeData,[\"r_%s\"%magnangle,magnangle],\"K.%s(K.abs(r_%s-%s))\"%(analyse,magnangle,magnangle))\n",
        "        else:\n",
        "            i = \"0\" if typeData == \"rgb\" else \"1\"\n",
        "            generate_functions(\"abs_diff_%s\"%(analyse),typeData,[],\"K.%s(K.abs(y_true-y_pred))\"%(analyse))\n",
        "print(\"[%s]\"%(\",\".join(Lmetrics)))\n",
        "print(\"[%s]\"%(\",\".join(list(map(lambda x:\"'\"+x+\"'\",Lnames)))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca6_nAS4SvRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics_rgb_abs_diff_max():\n",
        "\tdef rgb_abs_diff_max(y_true,y_pred):\n",
        "\t\treturn K.max(K.abs(y_true-y_pred))\n",
        "\treturn rgb_abs_diff_max\n",
        "\n",
        "def metrics_rgb_abs_diff_mean():\n",
        "\tdef rgb_abs_diff_mean(y_true,y_pred):\n",
        "\t\treturn K.mean(K.abs(y_true-y_pred))\n",
        "\treturn rgb_abs_diff_mean\n",
        "\n",
        "def metrics_rgb_abs_diff_std():\n",
        "\tdef rgb_abs_diff_std(y_true,y_pred):\n",
        "\t\treturn K.std(K.abs(y_true-y_pred))\n",
        "\treturn rgb_abs_diff_std\n",
        "\n",
        "# [metrics_rgb_abs_diff_max(),metrics_rgb_abs_diff_mean(),metrics_rgb_abs_diff_std()]\n",
        "# ['K.max(K.abs(rgb_true-rgb_pred))','K.mean(K.abs(rgb_true-rgb_pred))','K.std(K.abs(rgb_true-rgb_pred))']\n",
        "\n",
        "def metrics_probability_abs_diff_max():\n",
        "\tdef probability_abs_diff_max(y_true,y_pred):\n",
        "\t\treturn K.max(K.abs(y_true-y_pred))\n",
        "\treturn probability_abs_diff_max\n",
        "\n",
        "def metrics_probability_abs_diff_mean():\n",
        "\tdef probability_abs_diff_mean(y_true,y_pred):\n",
        "\t\treturn K.mean(K.abs(y_true-y_pred))\n",
        "\treturn probability_abs_diff_mean\n",
        "\n",
        "def metrics_probability_abs_diff_std():\n",
        "\tdef probability_abs_diff_std(y_true,y_pred):\n",
        "\t\treturn K.std(K.abs(y_true-y_pred))\n",
        "\treturn probability_abs_diff_std\n",
        "\n",
        "# [metrics_probability_abs_diff_max(),metrics_probability_abs_diff_mean(),metrics_probability_abs_diff_std()]\n",
        "# ['K.max(K.abs(rgb_true-rgb_pred))','K.mean(K.abs(rgb_true-rgb_pred))','K.std(K.abs(rgb_true-rgb_pred))']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knjYDnSCSygd",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UTrJ2EXkIse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_binary_MSE(inpt_gen_output,gen):\n",
        "    def loss(y_true, y_pred):\n",
        "        b_c = tf.keras.losses.binary_crossentropy(y_true[1], y_pred)\n",
        "        MSE = K.mean(K.pow(gen- inpt_gen_output,2))\n",
        "        return b_c+MSE*10**-2\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVL2rt_XSyDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_progressive_model(index_model,taille_image,taille_batch,max_epoch=200, max_it=2000):\n",
        "    train_data,validation_data,test_data = create_datasets()\n",
        "    max_total_it = (max_step-1)*3*max_it\n",
        "    tot_iteration = 0\n",
        "    base_name = \"Pretrain_fft_gan_%d\"%index_model\n",
        "    reprise_epoch = 50# @param {type:'number'}\n",
        "    reprise_iteration = 0# @param {type:'number'}\n",
        "    for epoch in range(max_epoch):\n",
        "        object_dataset = Dataset(localisation_annexe_txt=\"./list_files.txt\")\n",
        "        graph_name = base_name\n",
        "        graph_name = graph_name + \"_%d\"%(step)\n",
        "        backup_path = graph_name + \".h5\"\n",
        "        model_gen,model_disc,model_gan,inpt_gen_output,gen = generate_models(name=graph_name)\n",
        "        \n",
        "        metrics_list_gen = [metrics_rgb_abs_diff_max(),metrics_rgb_abs_diff_mean(),metrics_rgb_abs_diff_std()]\n",
        "        metrics_list_names_gen = ['MSE','K.max(K.abs(rgb_true-rgb_pred))','K.mean(K.abs(rgb_true-rgb_pred))','K.std(K.abs(rgb_true-rgb_pred))']\n",
        "\n",
        "        metrics_list_disc = [metrics_probability_abs_diff_max(),metrics_probability_abs_diff_mean(),metrics_probability_abs_diff_std()]\n",
        "        metrics_list_names_disc = ['Binary_crossentropy','K.max(K.abs(rgb_true-rgb_pred))','K.mean(K.abs(rgb_true-rgb_pred))','K.std(K.abs(rgb_true-rgb_pred))']\n",
        "        adam_lr_neg_pow = 6#@param {type:'slider',min:0,max:10,step:1}\n",
        "        adam_beta_1 = 0.99#@param {type:'slider',min:0.8,max:1.5,step:0.001}\n",
        "        adam_beta_2 = 0.9405#@param {type:'slider',min:0.8,max:1.5,step:0.0001}\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=10**-adam_lr_neg_pow, beta_1=adam_beta_1, beta_2=adam_beta_2, amsgrad=False)\n",
        "        model_gen.compile(loss=\"MSE\",metrics=metrics_list_gen,optimizer=opt)\n",
        "        \n",
        "        model_disc.compile(loss=\"binary_crossentropy\",metrics=metrics_list_disc,optimizer=opt)\n",
        "        model_gan.compile(loss=loss_binary_MSE(inpt_gen_output,gen),optimizer=opt)\n",
        "        search_restore_pt(index_model,model_gan,base_name=graph_name)\n",
        "        Ldisc,Lgan = [],[]\n",
        "        for iteration in range(0 if step !=reprise_epoch else reprise_iteration,max_it):\n",
        "            gen_input, gen_output,disc_input,disc_output,gan_input,gan_output = object_dataset.next_gan_batch(dataset='tr')\n",
        "            metrics_list_values_gan = model_gan.train_on_batch([gen_input,gen_output],gan_output)\n",
        "            loss_gan = metrics_list_values_gan\n",
        "            \n",
        "            metrics_list_values_disc = model_disc.train_on_batch(inpt_disc,outpt_disc)\n",
        "            loss_disc = metrics_list_values_disc\n",
        "            Ldisc.append(loss_disc[0])\n",
        "            Lgan.append(loss_gan)\n",
        "            compteur = 0\n",
        "            max_repetitions_corrections = 100#@param {type:'number'}\n",
        "            while abs(loss_disc[0]-loss_gan) > 0.5 and compteur < max_repetitions_corrections:\n",
        "                image,imageBruitee = next_batch_bruit_example(train_data,iteration,max_it,taille_image,taille_batch)\n",
        "                inpt_disc,outpt_disc,inpt_gen,outpt_gen,outpt_gan_disc = generate_inpts_outpts(image,imageBruitee)\n",
        "                if loss_gan > loss_disc[0]:\n",
        "                    metrics_list_values_gan = model_gan.train_on_batch([inpt_gen,outpt_gen],outpt_gan_disc)\n",
        "                    loss_gan = metrics_list_values_gan\n",
        "\n",
        "                    metrics_list_values_disc = model_disc.test_on_batch(inpt_disc,outpt_disc)\n",
        "                    loss_disc = metrics_list_values_disc\n",
        "                    print(\"Temporaire training GAN : Etape d'entrainement %d \\t\\titeration %d \\tstatut %d \\tpourcent, \\terreur gan %.8e \\t et disc %.8e\"%(step, iteration,tot_iteration/max_total_it,loss_gan,loss_disc[0]))\n",
        "                    Lgan.append(loss_gan)\n",
        "                else:\n",
        "                    metrics_list_values_disc = model_disc.train_on_batch(inpt_disc,outpt_disc)\n",
        "                    loss_disc = metrics_list_values_disc\n",
        "\n",
        "                    metrics_list_values_gan = model_gan.test_on_batch([inpt_gen,outpt_gen],outpt_gan_disc)\n",
        "                    loss_gan = metrics_list_values_gan\n",
        "                    print(\"Temporaire training Disc : Etape d'entrainement %d \\t\\titeration %d \\tstatut %d \\tpourcent, \\terreur gan %.8e \\t et disc %.8e\"%(step, iteration,tot_iteration/max_total_it,loss_gan,loss_disc[0]))\n",
        "                    Ldisc.append(loss_disc[0])\n",
        "                compteur += 1\n",
        "                plt.clf()\n",
        "                plt.subplot(1,2,1)\n",
        "                plt.plot(list(range(len(Ldisc))),Ldisc)\n",
        "                plt.yscale(\"log\")\n",
        "                plt.subplot(1,2,2)\n",
        "                plt.plot(list(range(len(Lgan))),Lgan)\n",
        "                plt.yscale(\"log\")\n",
        "                plt.show()\n",
        "            backup_fft(num_model=index_model,model_gen=model_gen,model_disc=model_disc,model_gan=model_gan,image=image,imageBruitee=imageBruitee,graph_name=graph_name,step=step,iteration=iteration,metrics_list_names_gen=metrics_list_names_gen,metrics_list_names_disc=metrics_list_names_disc,model_state_backup_int=5)\n",
        "            \n",
        "            print(\"Etape d'entrainement %d \\t\\titeration %d \\tstatut %d \\tpourcent, \\terreur gan %.8e \\t et disc %.8e\"%(step, iteration,tot_iteration/max_total_it,loss_gan,loss_disc[0]))\n",
        "            tot_iteration += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEfOWQUqCepb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "taille_image = 256# @param {type:'number'}\n",
        "taille_batch = 5# @param {type:'number'}\n",
        "numero_model = 12# @param {type:'number'}\n",
        "#train_progressive_model(index_model=numero_model,taille_image=taille_image,taille_batch=taille_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjYUPWp029xH",
        "colab_type": "text"
      },
      "source": [
        "| lr | Beta 1 | Beta 2 |Erreur finale disc|\n",
        "|:---:|:---:|:---:|:---:|\n",
        "|$10^{-5}$|0.99|0.939|0.3|\n",
        "|$10^{-6}$|0.99|0.944|4|\n",
        "|$10^{-6}$|0.99|0.940|0.05 (non fini)|\n",
        "|$10^{-6}$|0.99|0.941|4|\n",
        "|$10^{-6}$|0.99|0.945||"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g55rZPH2Onf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWwIUKkWlRSO",
        "colab_type": "code",
        "outputId": "7a9b4826-50b0-4c64-d931-dcb9e6166f84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "base_name = \"Pretrain_fft_gan_12\"\n",
        "graph_name = base_name\n",
        "graph_name = graph_name + \"_%d\"%(50)\n",
        "backup_path = graph_name + \".h5\"\n",
        "model_gen,model_disc,model_gan,inpt_gen_output,gen = generate_models(name=graph_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building 129_Image\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS5-axXVlmI8",
        "colab_type": "code",
        "outputId": "9c98fc5d-fc7b-4935-ae4e-9129a05dd200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(model_gan.layers)\n",
        "print([l.name for l in model_gan.layers])\n",
        "print(model_gan.outputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7fda8350bac8>, <tensorflow.python.keras.engine.training.Model object at 0x7fda8147f908>, <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7fda806bb400>, <tensorflow.python.keras.engine.training.Model object at 0x7fda806bbfd0>]\n",
            "['129_Image', 'gen', 'input_1', 'disc']\n",
            "Tensor(\"disc/dense/BiasAdd:0\", shape=(?, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}