{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modèle000001.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "R4se1VpyGHLS"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rob174/Astronomy/blob/Astronomy/Mod%C3%A8le000001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k_hAT1OO2lUN"
      },
      "source": [
        "# Fonctions de base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "daqyahLS1u1Z",
        "colab": {}
      },
      "source": [
        "\n",
        "##Python / Colab\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import os\n",
        "from IPython.display import Image as imgIPython\n",
        "from IPython.display import clear_output,display\n",
        "## Tensorflow keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from tensorflow.python import debug as tf_debug\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense,Conv2D,Convolution2D,Activation,Conv2DTranspose\n",
        "from tensorflow.keras.layers import MaxPooling2D,AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout,Reshape,BatchNormalization\n",
        "from tensorflow.keras.layers import concatenate,Concatenate,Subtract,Multiply,Average,Add\n",
        "from tensorflow.keras.layers import UpSampling2D, Reshape,Flatten\n",
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.losses\n",
        "\n",
        "## Math libraries\n",
        "import numpy as np\n",
        "import scipy\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "##Images\n",
        "from PIL import Image\n",
        "import cv2\n",
        "## Graph\n",
        "from graphviz import render\n",
        "from graphviz import Digraph,Graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3feuWSKFaZF",
        "colab_type": "code",
        "outputId": "b4b876dc-248b-4eb3-8d14-96ab5072b569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/My Drive/TIPE'\n",
        "print(\"Utilise le\",str(device_lib.list_local_devices()[0])[15:18])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/TIPE\n",
            "Utilise le CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkxjSEhBBxUY",
        "colab_type": "text"
      },
      "source": [
        "# Setup input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GAlaegm-PV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_datasets(train_rate=0.7,validation_rate=0.2,test_rate=0.1):\n",
        "    images = [\"Galaxies_resized/\"+f for f in os.listdir(\"Galaxies_resized/\")]\n",
        "    indexes = np.arange(len(images))\n",
        "    np.random.shuffle(indexes)\n",
        "    if os.path.exists(\"Datasets_list.txt\"):\n",
        "        with open(\"Datasets_list.txt\",\"r\") as file:\n",
        "            train_dataset = file.readline().strip().split(\",\")\n",
        "            validation_dataset = file.readline().strip().split(\",\")\n",
        "            test_dataset = file.readline().strip().split(\",\")\n",
        "    else:\n",
        "        train_dataset = [images[i] for i in indexes[:int(train_rate*len(images))+1]]\n",
        "        validation_dataset = [images[i] for i in indexes[int(train_rate*len(images))+1:int((train_rate+validation_rate)*len(images))+1]]\n",
        "        test_dataset = [images[i] for i in indexes[int((train_rate+validation_rate)*len(images))+1:]]\n",
        "        with open(\"Datasets_list.txt\",\"w\") as file:\n",
        "            file.write(\",\".join(train_dataset))\n",
        "            file.write(\",\".join(validation_dataset))\n",
        "            file.write(\",\".join(test_dataset))\n",
        "    return train_dataset,validation_dataset,test_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fxnCGK3I2hpP",
        "colab": {}
      },
      "source": [
        "def next_batch(batch_size, images,tailleAttendue,formatArray):#ATTENTION : pr tenter d'améliorer l'apprentissage, on augmente la taille minimale d'image prise\n",
        "    \"\"\"\n",
        "    formatArray : format de sortie des données de l'image ; utiliser numpy\n",
        "    \"\"\"\n",
        "    imageEntreeTensor = []\n",
        "    imageSortieTensor = []\n",
        "    while len(imageEntreeTensor) < batch_size:\n",
        "        try:\n",
        "            np.random.shuffle(images)#choix aléatoire de l'image\n",
        "            image = cv2.imread(images[0])#Ouvre en rgb l'image nettoyée\n",
        "            resizedImage = cv2.resize(image,(tailleAttendue,tailleAttendue))\n",
        "            imageSortieTensor.append(np.array(resizedImage,dtype=formatArray))\n",
        "            imageEntreeTensor.append(np.array(resizedImage,dtype=formatArray))\n",
        "        except:\n",
        "            print(\"Error in next_batch\")\n",
        "    imageEntreeTensor = np.array(imageEntreeTensor,formatArray)\n",
        "    return [imageEntreeTensor,imageEntreeTensor]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "49nICh8X2kOb",
        "colab": {}
      },
      "source": [
        "def next_batch_bruit_voile(batch_size, images,tailleAttendue,formatArray,facteursVoile,bruitParam,plageVal=[0,255]):#ATTENTION : pr tenter d'améliorer l'apprentissage, on augmente la taille minimale d'image prise\n",
        "    \"\"\"\n",
        "    formatArray : format de sortie des données de l'image ; utiliser numpy\n",
        "    facteurVoile : liste de valeur entre 0 et 1 contenant l'atténuation pour chaque couche de l'image\n",
        "    bruitParam : liste avec dans l'ordre moyenne et écart type\n",
        "    \"\"\"\n",
        "    assert plageVal[0] <= plageVal[1]\n",
        "    assert plageVal[0] <= bruitParam[0] <= plageVal[1]\n",
        "    assert plageVal[0] <= bruitParam[0]-bruitParam[1] <= plageVal[1]\n",
        "    assert plageVal[0] <= bruitParam[0]+bruitParam[1] <= plageVal[1]\n",
        "    assert formatArray == np.float32\n",
        "    \n",
        "    imageEntreeTensor,imageSortieTensor = next_batch(batch_size,  images,tailleAttendue,formatArray)\n",
        "    imageEntreeTensor,imageSortieTensor = imageEntreeTensor/255,imageSortieTensor/255\n",
        "    imageSortieTensorCopy = np.array(imageSortieTensor,dtype=np.float32)\n",
        "    for image in range(imageSortieTensorCopy.shape[0]):\n",
        "        for rgbIndex in range(3):\n",
        "            imageSortieTensorCopy[image,:,:,rgbIndex] *= facteursVoile[rgbIndex]\n",
        "    imageSortieTensorCopy = np.clip(imageSortieTensorCopy + np.random.normal(bruitParam[0],bruitParam[1],imageSortieTensorCopy.shape),plageVal[0],plageVal[1])\n",
        "    return [imageEntreeTensor,np.array(imageSortieTensorCopy,dtype=formatArray)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KZrLh69g3Eel",
        "colab": {}
      },
      "source": [
        "def next_batch_bruit_voile_2(batch_size, images,tailleAttendue,formatArray,facteursVoile,bruitParam,plageVal=[0,255]):#ATTENTION : pr tenter d'améliorer l'apprentissage, on augmente la taille minimale d'image prise\n",
        "    \"\"\"\n",
        "    La versison 2 fait les  modification sélectives de couleurs après avoir ajouté le bruit\n",
        "    formatArray : format de sortie des données de l'image ; utiliser numpy\n",
        "    facteurVoile : liste de valeur entre 0 et 1 contenant l'atténuation pour chaque couche de l'image\n",
        "                   OU liste de liste specifiant la plage de valeurs aléatoire entre lesquelles prendre l'atténuation\n",
        "    bruitParam : liste avec dans l'ordre moyenne et écart type\n",
        "    \"\"\"\n",
        "    assert plageVal[0] <= plageVal[1]\n",
        "    assert plageVal[0] <= bruitParam[0] <= plageVal[1]\n",
        "    assert plageVal[0] <= bruitParam[0]-bruitParam[1] <= plageVal[1]\n",
        "    assert plageVal[0] <= bruitParam[0]+bruitParam[1] <= plageVal[1]\n",
        "    assert formatArray == np.float32\n",
        "    assert type(facteursVoile) == list, \"Data type required : list not %s\"%(type(facteursVoile))\n",
        "    assert len(facteursVoile) == 3, \"Not the correct amount of data : expected 3 'data' (list or floats) in a list not %d\"%(len(facteursVoile))\n",
        "    facteursVoileImg = []\n",
        "    if type(facteursVoile[0]) == list:\n",
        "        if len(facteursVoile[0]) != 2 or len(facteursVoile[1]) != 2 or len(facteursVoile[2]) != 2:\n",
        "            print(\"facteursVoile currently used : \",facteursVoile)\n",
        "            raise ValueError(\"Expected each facteursVoile subelement to be 3 lists of 2 elements\")\n",
        "        for rgbIndex in range(3):\n",
        "            colorFactor = []\n",
        "            for imgIndex in range(batch_size):\n",
        "                random_value = np.random.rand(1)[0]*(facteursVoile[rgbIndex][1]-facteursVoile[rgbIndex][0])+facteursVoile[rgbIndex][0]\n",
        "                # print(\"Random voile with offset : %f\"%random_value)\n",
        "                colorFactor += [random_value]\n",
        "            facteursVoileImg.append(colorFactor)\n",
        "    imageEntreeTensor,imageSortieTensor = next_batch(batch_size,  images,tailleAttendue,formatArray)\n",
        "    imageEntreeTensor,imageSortieTensor = imageEntreeTensor/255,imageSortieTensor/255\n",
        "    imageSortieTensorCopy = np.array(imageSortieTensor,dtype=np.float32)\n",
        "    imageSortieTensorCopy = np.array(imageSortieTensorCopy,dtype=formatArray)\n",
        "    for image in range(imageSortieTensorCopy.shape[0]):\n",
        "        for rgbIndex in range(3):\n",
        "            imageSortieTensorCopy[image,:,:,rgbIndex] *= facteursVoileImg[rgbIndex][image]\n",
        "            imageSortieTensorCopy[image,:,:,rgbIndex] += np.random.normal(bruitParam[0],bruitParam[1])\n",
        "            # print(\"Max imageSortieTensorCopy : %d et Max imageEntreeTensor : %d pour img d'index %d et de couleur %d\"%(np.max(imageSortieTensorCopy),np.max(imageEntreeTensor),image,rgbIndex))\n",
        "    # print(\"Max imageSortieTensorCopy : %d et Max imageEntreeTensor : %d\"%(np.max(imageSortieTensorCopy),np.max(imageEntreeTensor)))\n",
        "    imageSortieTensorCopy /= np.max(imageEntreeTensor)\n",
        "    imageSortieTensorCopy *= plageVal[1]\n",
        "    imageSortieTensorCopy = np.clip(imageSortieTensorCopy,plageVal[0],plageVal[1])\n",
        "    imageEntreeTensor /= np.max(imageEntreeTensor)\n",
        "    imageEntreeTensor *= plageVal[1]\n",
        "    imageEntreeTensor = np.clip(imageEntreeTensor,plageVal[0],plageVal[1])\n",
        "    return [imageEntreeTensor,imageSortieTensorCopy]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6nuvaDIBDp3G",
        "colab": {}
      },
      "source": [
        "def normalisation(arrayL,plageEntree=[0,255],plageSortie=[0,1]):\n",
        "    assert plageEntree != plageSortie\n",
        "    assert plageEntree[1]>0 and plageSortie[1] > 0\n",
        "    formatArray = [array.dtype for array in arrayL]\n",
        "    L = [np.array(array,dtype=np.float) for array in arrayL]\n",
        "    for i in range(len(L)):\n",
        "        L[i] = np.array((L[i]-plageEntree[0])/(plageEntree[1]-plageEntree[0])*(plageSortie[1]-plageSortie[0])+plageSortie[0],formatArray[i])\n",
        "    return L"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTiBXKguCD7c",
        "colab_type": "text"
      },
      "source": [
        "# Conversions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AO_NSKJdVoS9",
        "colab": {}
      },
      "source": [
        "def LarrayFloatToUint(L):\n",
        "    return [np.array(array,np.uint) for array in L]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hp8DhcxaEKU-",
        "colab": {}
      },
      "source": [
        "def convertToUintL(L):\n",
        "    Lresult = []\n",
        "    print(\"Entree : \",len(L))\n",
        "    for i in range(len(L)):\n",
        "        Lresult.append(np.array(normalisation(L[i],[0,1],[0,255]),dtype=np.uint8))\n",
        "    print(\"Sortie : \",len(Lresult))\n",
        "    return Lresult"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drdUov_iYAZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convertToUint(array):\n",
        "    return np.array(normalisation(array,[0,1],[0,255]),dtype=np.uint8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkYZQGhzV3ff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cf https://stackoverflow.com/questions/43784921/how-to-display-custom-images-in-tensorboard-using-keras?noredirect=1#comment85726690_43784921\n",
        "def make_image(tensor):\n",
        "    \"\"\"\n",
        "    Convert an numpy representation image to Image protobuf.\n",
        "    Copied from https://github.com/lanpa/tensorboard-pytorch/\n",
        "    \"\"\"\n",
        "    from PIL import Image\n",
        "    tensor = np.stack((tensor,tensor,tensor),axis=-1)\n",
        "    height, width, channel = tensor.shape # numpy.ndarray\n",
        "    image = Image.fromarray(tensor)\n",
        "    import io\n",
        "    output = io.BytesIO()\n",
        "    image.save(output, format='PNG')\n",
        "    image_string = output.getvalue()\n",
        "    output.close()\n",
        "    CHANNEL = 1\n",
        "    var = tf.Summary.Image(height=height,\n",
        "                         width=width,\n",
        "                         colorspace=CHANNEL,\n",
        "                         encoded_image_string=image_string)\n",
        "    print(\"var : \",var)\n",
        "    return var"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZODUIlFtaG3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_shape(x):\n",
        "    print(\"Shape : \",x.get_shape().as_list())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOZZ2bGdMLEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tauxApprentissage(epoch,ampl,tau,lim):\n",
        "    taux = ampl*10**-((epoch)/tau)\n",
        "    return taux if taux > lim else lim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wSOGJdQj6dR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def differenceAcceptee(epoch,ampl,tau,lim):\n",
        "    taux = ampl*10**-((epoch)/tau)\n",
        "    return taux if taux > lim else lim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7lAEUgOBRzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_img(img_array_normalized,path):\n",
        "    assert type(img_array_normalized) == np.ndarray, \"Pass a numpy array\"\n",
        "    assert len(img_array_normalized.shape) == 4, \"Pass a tensor, a batch of images with 4 dimensions\"\n",
        "    assert img_array_normalized.shape[-1] == 3, \"Pass a tensor with 3 channels at the end to build rgb images\"\n",
        "    assert np.max(img_array_normalized) <=1, \"Max tensor value must be 1 no %f\"%np.max(img_array_normalized)\n",
        "    individual_path = path\n",
        "    for i in range(img_array_normalized.shape[0]):\n",
        "        individual_path = path + \"_batchIndex_%d\"%(i)\n",
        "        cv2.imwrite(path+'.jpg', np.uint8(img_array_normalized[i,:,:,:]*255))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmBhBUPBCSMm",
        "colab_type": "text"
      },
      "source": [
        "# Couches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXZasifPjmid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# source : https://stackoverflow.com/questions/46418373/how-to-resize-interpolate-a-tensor-in-keras\n",
        "def interpolation(h,w,inputTensor):\n",
        "    def resize_like(inputTensor,h,w):\n",
        "        return tf.image.resize_nearest_neighbor(inputTensor, [h, w])\n",
        "\n",
        "    return Lambda(resize_like, arguments={'h':h,'w':w})(inputTensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL1IECK2CYZ8",
        "colab_type": "text"
      },
      "source": [
        "## SELU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Fq-UnF8CH2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Optimizer\n",
        "\n",
        "\n",
        "# Ported from https://github.com/LiyuanLucasLiu/RAdam/blob/master/radam.py\n",
        "class RectifiedAdam(Optimizer):\n",
        "    \"\"\"RectifiedAdam optimizer.\n",
        "    Default parameters follow those provided in the original paper.\n",
        "    # Arguments\n",
        "        lr: float >= 0. Learning rate.\n",
        "        final_lr: float >= 0. Final learning rate.\n",
        "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
        "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
        "        gamma: float >= 0. Convergence speed of the bound function.\n",
        "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
        "        decay: float >= 0. Learning rate decay over each update.\n",
        "        weight_decay: Weight decay weight.\n",
        "        amsbound: boolean. Whether to apply the AMSBound variant of this\n",
        "            algorithm.\n",
        "    # References\n",
        "        - [On the Variance of the Adaptive Learning Rate and Beyond]\n",
        "          (https://arxiv.org/abs/1908.03265)\n",
        "        - [Adam - A Method for Stochastic Optimization]\n",
        "          (https://arxiv.org/abs/1412.6980v8)\n",
        "        - [On the Convergence of Adam and Beyond]\n",
        "          (https://openreview.net/forum?id=ryQu7f-RZ)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
        "                 epsilon=None, decay=0., weight_decay=0.0, **kwargs):\n",
        "        super(RectifiedAdam, self).__init__(**kwargs)\n",
        "\n",
        "        with K.name_scope(self.__class__.__name__):\n",
        "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
        "            self.lr = K.variable(lr, name='lr')\n",
        "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
        "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
        "            self.decay = K.variable(decay, name='decay')\n",
        "\n",
        "        if epsilon is None:\n",
        "            epsilon = K.epsilon()\n",
        "        self.epsilon = epsilon\n",
        "        self.initial_decay = decay\n",
        "\n",
        "        self.weight_decay = float(weight_decay)\n",
        "\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [K.update_add(self.iterations, 1)]\n",
        "\n",
        "        lr = self.lr\n",
        "        if self.initial_decay > 0:\n",
        "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n",
        "                                                      K.dtype(self.decay))))\n",
        "\n",
        "        t = K.cast(self.iterations, K.floatx()) + 1\n",
        "\n",
        "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
        "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
        "        self.weights = [self.iterations] + ms + vs\n",
        "\n",
        "        for p, g, m, v in zip(params, grads, ms, vs):\n",
        "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
        "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
        "\n",
        "            beta2_t = self.beta_2 ** t\n",
        "            N_sma_max = 2 / (1 - self.beta_2) - 1\n",
        "            N_sma = N_sma_max - 2 * t * beta2_t / (1 - beta2_t)\n",
        "\n",
        "            # apply weight decay\n",
        "            if self.weight_decay != 0.:\n",
        "                p_wd = p - self.weight_decay * lr * p\n",
        "            else:\n",
        "                p_wd = None\n",
        "\n",
        "            if p_wd is None:\n",
        "                p_ = p\n",
        "            else:\n",
        "                p_ = p_wd\n",
        "\n",
        "            def gt_path():\n",
        "                step_size = lr * K.sqrt(\n",
        "                    (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max /\n",
        "                    (N_sma_max - 2)) / (1 - self.beta_1 ** t)\n",
        "\n",
        "                denom = K.sqrt(v_t) + self.epsilon\n",
        "                p_t = p_ - step_size * (m_t / denom)\n",
        "\n",
        "                return p_t\n",
        "\n",
        "            def lt_path():\n",
        "                step_size = lr / (1 - self.beta_1 ** t)\n",
        "                p_t = p_ - step_size * m_t\n",
        "\n",
        "                return p_t\n",
        "\n",
        "            p_t = K.switch(N_sma > 5, gt_path, lt_path)\n",
        "\n",
        "            self.updates.append(K.update(m, m_t))\n",
        "            self.updates.append(K.update(v, v_t))\n",
        "            new_p = p_t\n",
        "\n",
        "            # Apply constraints.\n",
        "            if getattr(p, 'constraint', None) is not None:\n",
        "                new_p = p.constraint(new_p)\n",
        "\n",
        "            self.updates.append(K.update(p, new_p))\n",
        "        return self.updates\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'lr': float(K.get_value(self.lr)),\n",
        "                  'beta_1': float(K.get_value(self.beta_1)),\n",
        "                  'beta_2': float(K.get_value(self.beta_2)),\n",
        "                  'decay': float(K.get_value(self.decay)),\n",
        "                  'epsilon': self.epsilon,\n",
        "                  'weight_decay': self.weight_decay}\n",
        "        base_config = super(RectifiedAdam, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exgFsJIgpdt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "def SELU(x):\n",
        "    return 1.0507*K.elu(x,alpha=1.67326)\n",
        "\n",
        "get_custom_objects().update({'custom_activation': Activation(SELU)})\n",
        "\n",
        "# A mettre pour le modèle : Activation(SELU)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqgd144CCdt_",
        "colab_type": "text"
      },
      "source": [
        "## LRN2D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDebcyPfqaow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LRN2D(Layer):#Normalisation de réponse locale\n",
        "    \"\"\"\n",
        "    This code is adapted from pylearn2.\n",
        "    License at: https://github.com/lisa-lab/pylearn2/blob/master/LICENSE.txt\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=1e-4, k=2, beta=0.75, n=5, **kwargs):\n",
        "        if n % 2 == 0:\n",
        "            raise NotImplementedError(\"LRN2D only works with odd n. n provided: \" + str(n))\n",
        "        super(LRN2D, self).__init__(**kwargs)\n",
        "        self.alpha = alpha\n",
        "        self.k = k\n",
        "        self.beta = beta\n",
        "        self.n = n\n",
        "\n",
        "    def get_output(self, train):\n",
        "        X = self.get_input(train)\n",
        "        b, ch, r, c = K.shape(X)\n",
        "        half_n = self.n // 2\n",
        "        input_sqr = K.square(X)\n",
        "        extra_channels = K.zeros((b, ch + 2 * half_n, r, c))\n",
        "        input_sqr = K.concatenate([extra_channels[:, :half_n, :, :],\n",
        "                                   input_sqr,\n",
        "                                   extra_channels[:, half_n + ch:, :, :]],\n",
        "                                  axis=1)\n",
        "        scale = self.k\n",
        "        for i in range(self.n):\n",
        "            scale += self.alpha * input_sqr[:, i:i + ch, :, :]\n",
        "        scale = scale ** self.beta\n",
        "        return X / scale\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\"name\": self.__class__.__name__,\n",
        "                  \"alpha\": self.alpha,\n",
        "                  \"k\": self.k,\n",
        "                  \"beta\": self.beta,\n",
        "                  \"n\": self.n}\n",
        "        base_config = super(LRN2D, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JpfoJ7mDnZo",
        "colab_type": "text"
      },
      "source": [
        "## Layers graph mode implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpQxOwTGECUQ",
        "colab_type": "text"
      },
      "source": [
        "### Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvwUwURzKW84",
        "colab_type": "text"
      },
      "source": [
        "#### Global managment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O43lCBXxG-U1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def g_get_current_id():\n",
        "\tglobal index_couches\n",
        "\treturn str(index_couches)\n",
        "def g_get_past_id(back=1):\n",
        "\tglobal index_couches\n",
        "\treturn str(index_couches-back)\n",
        "def g_new_id():\n",
        "\tglobal index_couches\n",
        "\tindex_couches += 1\n",
        "\treturn str(index_couches)\n",
        "Llink = []\n",
        "def g_link(graph,id1,id2):\n",
        "    global Llink\n",
        "    if not([id1,id2] in Llink):\n",
        "        graph.edge(id1,id2)\n",
        "        Llink += [id1,id2]\n",
        "    else:\n",
        "        print(\"Link already present : %s\"%[id1,id2])\n",
        "index_graph = 0\n",
        "index_couches = 0\n",
        "def new_graph(bgcolor='transparent'):\n",
        "    \"\"\"Return the graph\"\"\"\n",
        "    global index_graph\n",
        "    global index_couches\n",
        "    index_couches = 0\n",
        "    graph = Digraph(name=\"cluster_Graph%d\"%(index_graph),format='png')\n",
        "    graph.attr(bgcolor=bgcolor)\n",
        "    graph.attr(rankdir=\"LR\")\n",
        "    index_graph += 1\n",
        "    return graph\n",
        "def end_graph(graph,name):\n",
        "    \"\"\"Create the png file of the graph\"\"\"\n",
        "    graph.render(name)\n",
        "def begin_cluster(past_couche,name,color):\n",
        "\torig_graph = past_couche[1]\n",
        "\tpast_couche[1] = Digraph(name=\"cluster_%s\"%(name))\n",
        "\tpast_couche[1].attr(style='filled', color=color, label=name)\n",
        "\treturn orig_graph, past_couche\n",
        "def end_cluster(last_couche,orig_graph):\n",
        "\torig_graph.subgraph(last_couche[1])\n",
        "\tlast_couche[1] = orig_graph\n",
        "\treturn last_couche"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTlOakw2KbEU",
        "colab_type": "text"
      },
      "source": [
        "#### Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIwc3MChF0K8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def g_conv(graph,prev,noyau, filtres, strides = 1, auto_connect = True, identifier=False):\n",
        "\tlabel = None\n",
        "\tif identifier == True:\n",
        "\t\tprint(\"Received identifier\")\n",
        "\tif type(noyau) == int:\n",
        "\t\tlabel = \"{Convolution %s | {Noyau | %d} | {Filtres | %d} | {Strides | %d}}\"%(g_get_current_id(),noyau,filtres,strides)\n",
        "\telse:\n",
        "\t\tlabel = \"{Convolution %s | {Noyau | %dx%d} | {Filtres | %d} | {Strides | %d}}\"%(g_get_current_id(),noyau[0],noyau[1],filtres,strides)\n",
        "\tgraph.node(g_get_current_id(),shape=\"record\",label=label,color=\"black\",fillcolor=\"white\" if identifier==False else \"red\",style=\"filled\")\n",
        "\tif auto_connect == True:\n",
        "\t\tg_link(graph,id1=prev,id2=g_get_current_id())\n",
        "def g_max_p(graph,prev,noyau, auto_connect = True, identifier=False):\n",
        "\tlabel = \"{MaxPooling | {Noyau | %d}}\"%(noyau)\n",
        "\tgraph.node(g_get_current_id(),shape=\"record\",label=label,color=\"black\",fillcolor=\"white\" if identifier==False else \"red\",style=\"filled\")\n",
        "\tif auto_connect == True:\n",
        "\t\tg_link(graph,id1=prev,id2=g_get_current_id())\n",
        "def g_deconv(graph,prev,noyau,filtres, auto_connect = True, identifier=False):\n",
        "    label = \"{Déconvolution %s | {Noyau | %d} | {Filtres | %d}}\"%(g_get_current_id(),noyau,filtres)\n",
        "    graph.node(g_get_current_id(),shape=\"record\",label=label,color=\"black\",fillcolor=\"white\" if identifier==False else \"red\",style=\"filled\")\n",
        "    if auto_connect == True:\n",
        "        g_link(graph,id1=prev,id2=g_get_current_id())\n",
        "def g_dense(graph,prev,filtres, strides = 1, auto_connect = True, identifier=False):\n",
        "\tlabel = \"{Dense | {Filtres | %d} | {Strides | %d}}\"%(filtres,strides)\n",
        "\tgraph.node(g_get_current_id(),shape=\"record\",label=label,color=\"black\",fillcolor=\"white\" if identifier==False else \"red\",style=\"filled\")\n",
        "\tif auto_connect == True:\n",
        "\t\tg_link(graph,id1=prev,id2=g_get_current_id())\n",
        "\n",
        "def g_dropout(graph,prev,taux, auto_connect = True, identifier=False):\n",
        "\tlabel = \"\"\n",
        "\tif type(taux) == float:\n",
        "\t\tlabel = \"{Dropout %s | {Rate\\n(taux désactivation) | %.3f}}\"%(g_get_current_id(),taux)\n",
        "\telse:\n",
        "\t\tlabel = \"{Dropout %s | {Rate\\n(taux désactivation) | Adapté}}\"%(g_get_current_id())\n",
        "\n",
        "\tgraph.node(g_get_current_id(),shape=\"record\",label=label,color=\"black\",fillcolor=\"white\" if identifier==False else \"red\",style=\"filled\")\n",
        "\tif auto_connect == True:\n",
        "\t\tg_link(graph,id1=prev,id2=g_get_current_id())\n",
        "def g_regLoc(graph,prev,noyau = 20, k = 2, alpha= 10**-4,beta = 0.75, auto_connect = True, identifier=False):\n",
        "\tlabel = \"{Regularisation\\nRéponse\\nLocale\\n%s | {Noyau | %d} | {k | %.2f} | {alpha | %.2e} | {beta | %.2e}}\"%(g_get_current_id(),noyau,k,alpha,beta)\n",
        "\tif identifier == True:\n",
        "\t\tprint(identifier)\n",
        "\tgraph.node(g_get_current_id(),shape=\"record\",label=label,color=\"black\",fillcolor=\"white\" if identifier==False else \"red\",style=\"filled\")\n",
        "\tif auto_connect == True:\n",
        "\t\tg_link(graph,id1=prev,id2=g_get_current_id())\n",
        "def g_activation(graph,prev,type = \"SELU\", auto_connect = True, identifier=False):\n",
        "\tlabel = \"{Activation %s | {Type | %s}}\"%(g_get_current_id(),type)\n",
        "\tgraph.node(g_get_current_id(),shape=\"record\",label=label,color=\"black\",fillcolor=\"white\" if identifier==False else \"red\",style=\"filled\")\n",
        "\tif auto_connect == True:\n",
        "\t\tg_link(graph,id1=prev,id2=g_get_current_id())\n",
        "def g_batch_norm(graph,prev, auto_connect = True, identifier=False):\n",
        "\tlabel = \"{Normalisation\\nPar\\nBatch\\n%s}\"%(g_get_current_id())\n",
        "\tgraph.node(g_get_current_id(),shape=\"record\",label=label,color=\"black\",fillcolor=\"white\" if identifier==False else \"red\",style=\"filled\")\n",
        "\tif auto_connect == True:\n",
        "\t\tg_link(graph,id1=prev,id2=g_get_current_id())\n",
        "def g_flat(graph,prev, auto_connect = True, identifier=False):\n",
        "\tlabel = \"{Flatten}\"\n",
        "\tgraph.node(g_get_current_id(),shape=\"record\",label=label,color=\"black\",fillcolor=\"white\" if identifier==False else \"red\",style=\"filled\")\n",
        "\tif auto_connect == True:\n",
        "\t\tg_link(graph,id1=prev,id2=g_get_current_id())\n",
        "def g_concat(graph,index, identifier=False):\n",
        "    graph.node(g_get_current_id(),\"Concatenate\\n%s\"%(g_get_current_id()),fillcolor=\"white\" if identifier==False else \"red\")\n",
        "    for id in index:\n",
        "        graph.edge(id,g_get_current_id())\n",
        "def g_add(graph,index, identifier=False):\n",
        "    graph.node(g_get_current_id(),\"+\",fillcolor=\"white\" if identifier==False else \"red\")\n",
        "    for id in index:\n",
        "        graph.edge(id,g_get_current_id())\n",
        "def g_subtract(graph,index, identifier=False):\n",
        "    graph.node(g_get_current_id(),\"-\",fillcolor=\"white\" if identifier==False else \"red\")\n",
        "    for id in index:\n",
        "        graph.edge(id,g_get_current_id())\n",
        "def g_proba(graph,prev, auto_connect = True, identifier=False):\n",
        "\tgraph.node(g_get_current_id(),\"Probabilite\",fillcolor=\"white\" if identifier==False else \"red\")\n",
        "\tif auto_connect == True:\n",
        "\t\tg_link(graph,id1=prev,id2=g_get_current_id())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90I4K2JIEE6A",
        "colab_type": "text"
      },
      "source": [
        "### Graph, Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI79KzCJ0OYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_inpt(inpt):\n",
        "    inpt,graph,index,improvements = inpt[0],inpt[1],inpt[2],inpt[3]\n",
        "    return inpt,graph,index,improvements\n",
        "\n",
        "def g_input(graph, improvements, shape,content_name,draw=True):\n",
        "    global index_couches\n",
        "    inpt = Input(shape=shape, name=\"%d_%s\"%(index_couches,content_name))\n",
        "    print(\"Building %s\"%(\"%d_%s\"%(index_couches,content_name)))\n",
        "    if draw == True:\n",
        "        graph.node(str(index_couches),content_name)\n",
        "    index_couches += 1\n",
        "    return [inpt,graph,g_get_past_id(),improvements]\n",
        "def conv(inpt,k,f,s,operation=False, identifier=False,draw=True,always=False,step=None):\n",
        "    inpt_tmp,graph,index,improvements = extract_inpt(inpt)\n",
        "    if identifier == True:\n",
        "        print(\"Received identifier at conv level\")\n",
        "    global index_couches\n",
        "    if improvements != None and improvements == 0 and always == False:\n",
        "        index_couches += 1\n",
        "        return [inpt_tmp,graph,index,0]\n",
        "    if always == True:\n",
        "        improvements += 1\n",
        "    \n",
        "    if operation == True:\n",
        "        raise Exception(\"Operation option to check !\")\n",
        "    couche = conv_unit(inpt,k,f,s,operation=operation, identifier=identifier,draw=draw,first=True,step=step)\n",
        "    couche = conv_unit(couche,k,f,s,operation=operation, identifier=identifier,draw=draw,first=False,step=step)\n",
        "    couche,graph,index,improvements = extract_inpt(couche)\n",
        "    if improvements != None:\n",
        "        improvements = max(improvements-1,0)\n",
        "    return [couche,graph,g_get_past_id(),improvements]\n",
        "    \n",
        "    \n",
        "def conv_unit(inpt,k,f,s,operation=False, identifier=False,draw=True,first=True,step=None):\n",
        "    inpt,graph,index,improvements = extract_inpt(inpt)\n",
        "    global index_couches\n",
        "    couche = None\n",
        "    if first == True:\n",
        "        couche = Convolution2D(filters=f,kernel_size=(k,1),activation=None,strides=(s,s),padding='SAME',name=\"%d%s_conv_k%d_f%d\"%(index_couches,\"_step%d\"%step if step != None else \"\",k,f),trainable=True)(inpt)\n",
        "    else:\n",
        "        couche = Convolution2D(filters=f,kernel_size=(k,1),activation=None,strides=(s,s),padding='SAME',name=\"%d%s_conv_k%d_f%d\"%(index_couches,\"_step%d\"%step if step != None else \"\",k,f),trainable=True)(inpt)\n",
        "    if identifier == True:\n",
        "        print(\"Received identifier at conv_unit level\")\n",
        "    if draw == True:\n",
        "        if operation == False:\n",
        "            g_conv(graph,prev=index,noyau=(k,1) if first == True else (1,k),filtres=f,strides=s, identifier=identifier)\n",
        "        else:\n",
        "            g_conv(graph,prev=index,noyau=k,filtres=f,strides=s,auto_connect=False, identifier=identifier)\n",
        "    if identifier == True:\n",
        "        print(\"Previous index %s current %s\"%(index,g_get_current_id()))\n",
        "    index_couches += 1\n",
        "    return [couche,graph,g_get_past_id(),improvements]\n",
        "def dropout(inpt,r,operation=False, identifier=False,draw=True,always=False):\n",
        "    name = \"\"\n",
        "    global index_couches\n",
        "    if type(r) != float:\n",
        "        r = K.cast(r[0,0],K.floatx())\n",
        "        name = '%d_dropout_r_adaptative'%(index_couches)\n",
        "    else:\n",
        "        name = '%d_dropout_r%.2f'%(index_couches,r)\n",
        "    inpt,graph,index,improvements = extract_inpt(inpt)\n",
        "    if improvements != None and improvements == 0 and always == False:\n",
        "        index_couches += 1\n",
        "        return [inpt,graph,index,0]\n",
        "    if always == True:\n",
        "        improvements += 1    \n",
        "    trainable = True if improvements == None or max(improvements-1,0) > 0 else False\n",
        "    couche = Dropout(name=name,rate=r,trainable=trainable)(inpt)\n",
        "    if draw == True:\n",
        "        if operation == False:\n",
        "            g_dropout(graph,prev=index,taux=r, identifier=identifier)\n",
        "        else:\n",
        "            g_dropout(graph,prev=index,taux=r,auto_connect=False, identifier=identifier)\n",
        "    index_couches += 1\n",
        "    return [couche,graph,g_get_past_id(),None if improvements == None else max(improvements-1,0)]\n",
        "def lrn(inpt,n,k,a,b,operation=False, identifier=False,draw=True):\n",
        "    inpt,graph,index,improvements = extract_inpt(inpt)\n",
        "    global index_couches\n",
        "    if improvements != None and improvements == 0:\n",
        "        index_couches += 1\n",
        "        return [inpt,graph,index,0]\n",
        "    couche = LRN2D(name='%d_lrn_n%d_k%d_a%.2e_b%.2f'%(index_couches,n,k,a,b),n=n,k=k,alpha=a,beta=b)(inpt)\n",
        "    if draw == True:\n",
        "        if operation == False:\n",
        "            g_regLoc(graph,prev=index, identifier=identifier)\n",
        "        else:\n",
        "            g_regLoc(graph,prev=index,auto_connect=False, identifier=identifier,always=False)\n",
        "    index_couches += 1\n",
        "    return [couche,graph,g_get_past_id(),None if improvements == None else max(improvements-1,0)]\n",
        "def activ(inpt,act_type,operation=False, identifier=False,draw=True,always=False):\n",
        "    inpt,graph,index,improvements = extract_inpt(inpt)\n",
        "    \n",
        "    global index_couches\n",
        "    if improvements != None and improvements == 0 and always == False:\n",
        "        index_couches += 1\n",
        "        return [inpt,graph,index,0]\n",
        "    if always == True:\n",
        "        improvements += 1\n",
        "    assert act_type==\"SELU\", \"other activation functions than SELU are currently not supported\"\n",
        "    couche = Activation(SELU,name='%d_activation_%s'%(index_couches,act_type))(inpt)\n",
        "    if draw == True:\n",
        "        if operation == False:\n",
        "            g_activation(graph,prev=index, identifier=identifier)\n",
        "        else:\n",
        "            g_activation(graph,prev=index,auto_connect=False, identifier=identifier)\n",
        "    index_couches += 1\n",
        "    return [couche,graph,g_get_past_id(),None if improvements == None else max(improvements-1,0)]\n",
        "def b_norm(inpt,operation=False, identifier=False,draw=True):\n",
        "    inpt,graph,index,improvements = extract_inpt(inpt)\n",
        "    global index_couches\n",
        "    if improvements != None and improvements == 0:\n",
        "        index_couches += 1\n",
        "        return [inpt,graph,index,0]\n",
        "    trainable = True if improvements == None or max(improvements-1,0) > 0 else False\n",
        "    couche = BatchNormalization(name='%d_batchnorm'%(index_couches),trainable=trainable)(inpt)\n",
        "    if draw == True:\n",
        "        if operation == False:\n",
        "            g_batch_norm(graph,prev=index, identifier=identifier)\n",
        "        else:\n",
        "            g_batch_norm(graph,prev=index,auto_connect=False, identifier=identifier)\n",
        "    index_couches += 1\n",
        "    return [couche,graph,g_get_past_id(),None if improvements == None else max(improvements-1,0)]\n",
        "def dense(inpt,f,operation=False, identifier=False,draw=True):\n",
        "    global index_couches\n",
        "    inpt,graph,index,improvements = extract_inpt(inpt)\n",
        "    if improvements != None and improvements == 0:\n",
        "        index_couches += 1\n",
        "        return [inpt,graph,index,0]\n",
        "    trainable = True if improvements == None or max(improvements-1,0) > 0 else False\n",
        "    couche = Dense(f,activation=None,name='%d_dense_f%d'%(index_couches,f),trainable=trainable)(inpt)\n",
        "    if draw == True:\n",
        "        if operation == False:\n",
        "            g_dense(graph,prev=index,filtres=f, identifier=identifier)\n",
        "        else:\n",
        "            g_dense(graph,prev=index,filtres=f,auto_connect=False, identifier=identifier)\n",
        "    index_couches += 1\n",
        "    return [couche,graph,g_get_past_id(),None if improvements == None else max(improvements-1,0)]\n",
        "def extract_inpt_L(L):\n",
        "    L_inpt = [L[i][0] for i in range(len(L))]\n",
        "    graph = L[0][1]\n",
        "    ids = [inpt[2] for inpt in L]\n",
        "\n",
        "    return L_inpt,graph,ids,min([L[i][3] for i in range(len(L))])\n",
        "def concat(L_inpt,operation=False, identifier=False,draw=True):\n",
        "    L_inpt,graph,index,improvements = extract_inpt_L(L_inpt)\n",
        "    improvements += 1\n",
        "    global index_couches\n",
        "    couche = Concatenate(axis=-1,name='%d_merge'%(index_couches))(L_inpt)\n",
        "    if draw == True:\n",
        "        if operation == False:\n",
        "            g_concat(graph,index, identifier=identifier)\n",
        "        else:\n",
        "            g_concat(graph,index, auto_connect=False, identifier=identifier)\n",
        "\n",
        "    index_couches += 1\n",
        "    return [couche,graph,g_get_past_id(),None if improvements == None else max(improvements-1,0)]\n",
        "def subtract(L_inpt,operation=False, identifier=False):\n",
        "    L_inpt,graph,index,improvements = extract_inpt_L(L_inpt)\n",
        "    improvements += 1\n",
        "    global index_couches\n",
        "    couche = Subtract(name='%d_subtract'%(index_couches))(L_inpt)\n",
        "    if draw == True:\n",
        "        if operation == False:\n",
        "            g_subtract(graph,index, identifier=identifier)\n",
        "        else:\n",
        "            g_subtract(graph,index, auto_connect=False, identifier=identifier)\n",
        "\n",
        "    index_couches += 1\n",
        "    return [couche,graph,g_get_past_id(),None if improvements == None else max(improvements-1,0)]\n",
        "def max_p(inpt,k,operation=False, identifier=False,draw=True):\n",
        "    inpt,graph,index,improvements = extract_inpt(inpt)\n",
        "    global index_couches\n",
        "    trainable = True\n",
        "    couche = MaxPooling2D(name='%d_max_p_k%d'%(index_couches,k),pool_size=k,padding='VALID',trainable=trainable)(inpt)\n",
        "    if draw == True:\n",
        "        if operation == False:\n",
        "            g_max_p(graph,prev=index,noyau=k, identifier=identifier)\n",
        "        else:\n",
        "            g_max_p(graph,prev=index,noyau=k,auto_connect=False, identifier=identifier)\n",
        "    index_couches += 1\n",
        "    return [couche,graph,g_get_past_id(),improvements]\n",
        "def deconv(inpt,k,f,strides,tmp=False,operation=False, identifier=False,draw=True):\n",
        "    inpt,graph,index,improvements = extract_inpt(inpt)\n",
        "    global index_couches\n",
        "    trainable = True \n",
        "    couche = Conv2DTranspose(filters=f,kernel_size=k, strides=strides,name='%d_deconv_k%d_f%d'%(index_couches,k,f),padding='VALID',trainable=trainable)(inpt)\n",
        "    if tmp == True:\n",
        "        print(\"WARNING : this is a temporary layer, please not forget to destroy it when you find the right kernel parameter and recreate one deconv with tmp at False\")\n",
        "        return [couche,graph,g_get_current_id(),improvements]\n",
        "        \n",
        "    if draw == True:\n",
        "        if operation == False:\n",
        "            g_deconv(graph,prev=index,noyau=k,filtres=f, identifier=identifier)\n",
        "        else:\n",
        "            g_deconv(graph,prev=index,noyau=k,auto_connect=False, identifier=identifier)\n",
        "    index_couches += 1\n",
        "    return [couche,graph,g_get_past_id(),improvements]\n",
        "def add(L_inpt,operation=False, identifier=False,draw=True):\n",
        "    global index_couches\n",
        "    L_inpt,graph,index,improvements = extract_inpt_L(L_inpt)\n",
        "    improvements += 1\n",
        "    couche = Add(name='%d_add'%(index_couches))(L_inpt) \n",
        "    if draw == True:\n",
        "        if operation == False:\n",
        "            g_add(graph,index,identifier=identifier)\n",
        "        else:\n",
        "            g_add(graph,index,auto_connect=False,identifier=identifier)\n",
        "\n",
        "    index_couches += 1\n",
        "    return [couche,graph,g_get_past_id(),None if improvements == None else max(improvements-1,0)]\n",
        "def flat(inpt,draw=True):\n",
        "    inpt,graph,index,improvements = extract_inpt(inpt)\n",
        "    global index_couches\n",
        "    if improvements != None and improvements == 0:\n",
        "        index_couches += 1\n",
        "        return [inpt,graph,g_get_past_id(),0]\n",
        "    couche = Flatten(name='%d_flatten'%(index_couches))(inpt)\n",
        "    if draw == True:\n",
        "        g_flat(graph,prev=index)\n",
        "    index_couches += 1\n",
        "    return [couche,graph,g_get_past_id(),None if improvements == None else max(improvements-1,0)]\n",
        "def proba(inpt,draw=True):\n",
        "    inpt,graph,index,improvements = extract_inpt(inpt)\n",
        "    global index_couches\n",
        "    if improvements != None and improvements == 0:\n",
        "        index_couches += 1\n",
        "        return [inpt,graph,g_get_past_id(),0]\n",
        "    couche = Activation('sigmoid',name='%d_sigmoid_proba'%(index_couches))(inpt)\n",
        "    if draw == True:\n",
        "        g_proba(graph,prev=index)\n",
        "    index_couches += 1\n",
        "    return [couche,graph,g_get_past_id(),None if improvements == None else max(improvements-1,0)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUIaa1v66tE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rgb_to_magn_angle(x,draw=True):\n",
        "    x,graph,index,improvements = x[0],x[1],x[2],x[3]\n",
        "    x = Lambda(lambda x:K.cast(x,\"complex64\"))(x)\n",
        "    x_list_magn = []\n",
        "    x_list_angle = []\n",
        "    for i in range(3):\n",
        "        fft = Lambda(lambda x: tf.fft2d(x[:,:,:,i]), output_shape=(None,199,199))(x)\n",
        "        x_list_magn.append(Lambda(lambda fft:K.expand_dims(tf.math.abs(fft),axis=-1), output_shape=(None,199,199))(fft))\n",
        "        x_list_angle.append(Lambda(lambda fft: K.expand_dims(tf.math.angle(fft),axis=-1), output_shape=(None,199,199))(fft))\n",
        "    magn = Concatenate()(x_list_magn)\n",
        "    angle = Concatenate()(x_list_angle)\n",
        "    magn = Lambda(lambda magn: K.cast(magn,dtype=tf.float32), output_shape=(None,199,199))(magn)\n",
        "    angle = Lambda(lambda angle: K.cast(angle,dtype=tf.float32), output_shape=(None,199,199))(angle)\n",
        "    if draw == True:\n",
        "#         print(\"Draw FFT at index %s\"%g_get_current_id())\n",
        "        graph.node(g_get_current_id(),\"FFT\")\n",
        "        g_link(graph,index,g_get_current_id())\n",
        "    global index_couches\n",
        "    index_couches += 1\n",
        "    return [magn,graph,g_get_past_id(),improvements],[angle,graph,g_get_past_id(),improvements]\n",
        "def to_rgb(magn,angle,magn_train,angle_train,draw=True):\n",
        "    magn,graph,magn_index,improvements = extract_inpt(magn)\n",
        "    angle,graph,angle_index,improvements = extract_inpt(angle)\n",
        "    magn = Lambda(lambda magn: K.cast(magn,dtype=tf.complex64))(magn)\n",
        "    angle = Lambda(lambda angle:K.cast(angle,dtype=tf.complex64))(angle)\n",
        "    x_list_canal = []\n",
        "    for i in range(3):\n",
        "        complx_tensor = Lambda(lambda mang_angle: mang_angle[0][:,:,:,i]*tf.exp(1j*mang_angle[1][:,:,:,i]))([magn,angle])\n",
        "        ifft = Lambda(tf.ifft2d)(complx_tensor)\n",
        "        x_list_canal.append(K.expand_dims(ifft,axis=-1))\n",
        "    images = Concatenate()(x_list_canal)\n",
        "    images = K.cast(images,dtype=tf.float32)\n",
        "    if draw == True:\n",
        "        graph.node(g_get_current_id(),\"RGB\")\n",
        "        if magn_train == True:\n",
        "            g_link(graph,magn_index,g_get_current_id())\n",
        "        if angle_train == True:\n",
        "            g_link(graph,angle_index,g_get_current_id())\n",
        "    global index_couches\n",
        "    index_couches += 1\n",
        "    return [images,graph,g_get_past_id(),improvements]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPEtd5pfNtVp",
        "colab_type": "text"
      },
      "source": [
        "###Couches composites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYfoDfstTrFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unit(inpt,k,f,dropout_r=None,activation=False,draw=False,init_index=None,identifier=False):\n",
        "    if identifier == True:\n",
        "        print(\"Identifier : \",identifier)\n",
        "    if init_index != None:\n",
        "        global index_couches\n",
        "        index_couches = init_index\n",
        "    couche = conv(inpt,k=k,f=f,s=1,draw=draw,identifier=identifier)\n",
        "    couche = b_norm(couche,draw=draw,identifier=identifier)\n",
        "    couche = lrn(couche,n=21,k=2,a=10**-4,b=0.75,draw=draw,identifier=identifier)\n",
        "    if activation == True:\n",
        "        couche = activ(couche,act_type=\"SELU\",draw=draw,identifier=identifier)\n",
        "    if dropout_r != None:\n",
        "        couche = dropout(couche,r=dropout_r,draw=draw,identifier=identifier)\n",
        "    return couche"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jueETQ6eSLbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compare_shape(t1,t2):\n",
        "    if len(t1) != len(t2):\n",
        "        return False\n",
        "    for elem1,elem2 in zip(t1,t2):\n",
        "        if elem1 != elem2:\n",
        "            return False\n",
        "    return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVQP3hmzBd8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Because we need to have both the former layer and the layer resulting of the next floor we have to split the \"etage\" in an down function and an up function that will create respectively the left and the right part \n",
        "def etage_down(inpt_before,pool_k,k,f,dropout_r=None,activation=False,draw=False,init_index=None):\n",
        "    \"\"\"We consider that a treatment has been already done on the inpt_before layer ;\n",
        "    on the other side the etage function will restitute a layer with dimensions that match with inpt_before ;\n",
        "    if we are at the last etage we will only call one / multiple units with the two ouputs layers\n",
        "    In order to keep mostly the same weights we will always keep max pool and deconv layers in the process with deconv non trainable --> in the corresponding layers\"\"\"\n",
        "    reduced = max_p(inpt_before,pool_k,draw=draw)\n",
        "    after_process = unit(reduced,k,f,dropout_r=dropout_r,activation=True,draw=draw,init_index=init_index)\n",
        "    return after_process\n",
        "\n",
        "def etage_up(inpt_before,inpt_after,after_process,k,f,dropout_r=None,activation=False,draw=False,init_index=None):\n",
        "    _,_,_,improvements_size_modif = extract_inpt(inpt_before)\n",
        "    _,_,_,improvements_processing = extract_inpt(after_process)\n",
        "    \n",
        "    inpt_after[3] = improvements_processing\n",
        "    after_process[3] = improvements_processing\n",
        "    inpt_after = concat([inpt_after,after_process],operation=False, identifier=False,draw=draw)\n",
        "    inpt_after_processed = unit(inpt_after,k,f,dropout_r=dropout_r,activation=activation,draw=draw,init_index=init_index)\n",
        "    \n",
        "    output = deconv(inpt_after_processed,k=2,f=inpt_before[0].get_shape().as_list()[-1],strides=(2,2),\n",
        "                    tmp=False,draw=draw)\n",
        "    if compare_shape(output[0].get_shape().as_list(),inpt_before[0].get_shape().as_list()) != True:\n",
        "        raise Exception(\"No appropriate deconvolution kernel found with desired shape %s\\nand input of deconvolution %s\"%(inpt_before[0].get_shape().as_list(),output[0].get_shape().as_list()))\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTnMwNPiD0zO",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2ghwubNSbaf",
        "colab_type": "text"
      },
      "source": [
        "## Global model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG6ZvYqrBhRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inception(inpt,draw=True):\n",
        "    orig_graph, inpt = begin_cluster(past_couche=inpt,name=\"Inception%s\"%(g_get_current_id()),color='red')\n",
        "    \n",
        "    T0 = unit(inpt,k=1,f=100,activation=True,dropout_r=0.5,draw=draw)\n",
        "    T1 = unit(inpt,k=1,f=100,activation=True,dropout_r=0.5,draw=draw)\n",
        "    T1 = unit(T1,k=1,f=100,activation=True,dropout_r=0.5,draw=draw)\n",
        "\n",
        "    couche = concat([T0,T1],draw=draw)\n",
        "    couche = conv(couche,k=1,f=100,s=1,draw=draw)\n",
        "    couche = add([couche,inpt],draw=draw)\n",
        "    end_cluster(last_couche=couche, orig_graph=orig_graph)\n",
        "    return couche"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o352RNtxhvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sauvegardeModele(entree_pure,entree_deterioree,model,iteration_entrainement,summary_writer,batch_size=5):\n",
        "    global Llayers\n",
        "    for canal_image in range(3):\n",
        "        summary_image = tf.Summary(value=[tf.Summary.Value(tag=\"input_pure_canal_\"+str(canal_image), \n",
        "                                image=make_image(np.array(entree_pure[0][0,:,:,canal_image]*255,dtype=np.uint8)))])\n",
        "        summary_writer.add_summary(summary_image,iteration_entrainement)\n",
        "        summary_image = tf.Summary(value=[tf.Summary.Value(tag=\"input_deterioree_canal_\"+str(canal_image), \n",
        "                                image=make_image(np.array(entree_deterioree[0][0,:,:,canal_image]*255,dtype=np.uint8)))])\n",
        "        summary_writer.add_summary(summary_image,iteration_entrainement)\n",
        "        \n",
        "    for p,entree in enumerate([entree_pure,entree_deterioree]):\n",
        "        print(\"p value %d\"%(p))\n",
        "        layer_outputs,layer_names = [Llayers[i][0] for i in range(len(Llayers))],[Llayers[i][1] for i in range(len(Llayers))]\n",
        "        model_calcul_image = Model(inputs=model.input,outputs=[model.output]+layer_outputs)\n",
        "        sorties_couches = model_calcul_image.predict(entree, batch_size=batch_size)[1:] if len(layer_outputs) > 0 else [model_calcul_image.predict(entree, batch_size=batch_size)][1:]\n",
        "        \n",
        "        for index_couche,sortie_couche in enumerate(sorties_couches):\n",
        "            layer_name = layer_names[index_couche]\n",
        "            dim_sortie = sortie_couche.shape\n",
        "            if len(dim_sortie) == 4:\n",
        "                for canal_image in range(dim_sortie[-1]):\n",
        "                    tag = layer_name\n",
        "                    tag += 'pure' if p == 0 else 'deterioree'\n",
        "                    tag += \"_canal_\" + str(canal_image)\n",
        "                    summary_image = tf.Summary(value=[tf.Summary.Value(tag=tag, \n",
        "                                            image=make_image(np.array(sortie_couche[0,:,:,canal_image]*255,dtype=np.uint8)))])\n",
        "                    summary_writer.add_summary(summary_image,iteration_entrainement)\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1xaVw_4SfiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import output\n",
        "def beep():\n",
        "    output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3fMqsjEMJTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets\n",
        "import time\n",
        "continuer = None\n",
        "def block_or_continue():\n",
        "    beep()\n",
        "    print(\"Would you resume training ? True or False ?\")\n",
        "    return bool(input())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB7xsmOEThUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backup(image,imageBruitee,gen_disc,disc,index,summary_writer):\n",
        "    x = [np.concatenate((image[0],imageBruitee[0])),np.concatenate((image[1],imageBruitee[1]))]\n",
        "    sauvegardeModele(image,imageBruitee,gen_disc,index,summary_writer)\n",
        "    p_gen_disc = gen_disc.predict(x)[0]\n",
        "    p_disc = disc.predict(x)[0]\n",
        "\n",
        "    summary_loss = tf.Summary(value=[tf.Summary.Value(tag=\"Probabilité gen-disc img 0 (orig pure) pure : \", \n",
        "                                        simple_value=p_gen_disc) ])\n",
        "    summary_writer.add_summary(summary_loss,index)\n",
        "    summary_loss = tf.Summary(value=[tf.Summary.Value(tag=\"Probabilité disc img 0 (orig pure) pure : \", \n",
        "                                        simple_value=p_disc) ])\n",
        "    summary_writer.add_summary(summary_loss,index)\n",
        "    x_proba = [np.concatenate((imageBruitee[0],imageBruitee[0])),np.concatenate((imageBruitee[1],imageBruitee[1]))]\n",
        "    p_gen_disc = gen_disc.predict(x_proba)[0]\n",
        "    p_disc = disc.predict(x_proba)[0]\n",
        "    summary_loss = tf.Summary(value=[tf.Summary.Value(tag=\"Probabilité gen-disc img 0 (orig détériorée) pure : \", \n",
        "                                        simple_value=p_gen_disc) ])\n",
        "    summary_writer.add_summary(summary_loss,index)\n",
        "    summary_loss = tf.Summary(value=[tf.Summary.Value(tag=\"Probabilité disc img 0 (orig détériorée) pure : \", \n",
        "                                        simple_value=p_disc) ])\n",
        "    summary_writer.add_summary(summary_loss,index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39XpimsoQARk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_loss(y_true,y_pred):\n",
        "    return K.mean(K.square(y_pred - y_true),axis=-1)+K.max(K.square(y_pred - y_true),axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDAX-aHtopZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_fft(img_tensor):\n",
        "    L = []\n",
        "    print(img_tensor.shape[0],img_tensor.get_shape().as_list())\n",
        "    for index_image in range(img_tensor.get_shape().as_list()[0]):\n",
        "        L.append(to_fft_single(img_tensor[index_image,:,:,:]))\n",
        "    return np.stack(L,axis=0)\n",
        "\n",
        "def to_img(fft_cplx):\n",
        "    return to_uint8_array(np.fft.ifft2(fft_cplx))\n",
        "def to_fft_single(img):\n",
        "    fft_cplx = np.fft.fft2(img)\n",
        "    fft = 20*np.log(np.abs(fft_cplx)+10**-5*np.ones(img.shape))\n",
        "    fft = fft + np.abs(np.min(fft))*np.ones(fft.shape)\n",
        "    fft = fft / (np.max(fft)*np.ones(fft.shape))*(255*np.ones(fft.shape))\n",
        "    fft_uint8 = to_uint8_array(fft)\n",
        "    print(np.max(fft_uint8),np.min(fft_uint8),fft_uint8.shape,fft_uint8.dtype)\n",
        "    return fft_cplx\n",
        "\n",
        "def to_uint8_array(array):\n",
        "    return array.astype(np.uint8)\n",
        "def next_batch_bruit_voile_fft():\n",
        "    image,imageBruitee = next_batch_bruit_voile_2(10,images,199,np.float32,[1,1,1],[50,50])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH193F6WSWyo",
        "colab_type": "text"
      },
      "source": [
        "## FFT Pretraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odSEq99jmnDl",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq4QVh3eceLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def analyse_progressive_training_model(couche,step=0,init=2,draw=True):\n",
        "    #input size has to be 2**4=16 min but we wil take 2**8=256\n",
        "    level0_in = unit(couche,k=2,f=100,activation=True,dropout_r=0.1,init_index=init,draw=draw)#Output size around 256\n",
        "    down1 = etage_down(level0_in,pool_k=2,k=2,f=100,dropout_r=0.1,activation=True,draw=draw)#Output size around 128\n",
        "    down2 = etage_down(down1,pool_k=2,k=2,f=100,dropout_r=0.1,activation=True,draw=draw)#Output size around 64\n",
        "    down3 = etage_down(down2,pool_k=2,k=2,f=100,dropout_r=0.1,activation=True,draw=draw)#Output size around 32\n",
        "    down4 = etage_down(down3,pool_k=2,k=2,f=100,dropout_r=0.1,activation=True,draw=draw)#Output size around 16\n",
        "    \n",
        "    final_analysis = unit(down4,k=2,f=100,activation=True,draw=draw)#Output size around 16\n",
        "    \n",
        "    up4 = etage_up(inpt_before=down3,inpt_after=final_analysis,after_process=down4,k=2,f=100,dropout_r=0.1,activation=True,draw=draw)#Output size around 32\n",
        "    up3 = etage_up(inpt_before=down2,inpt_after=up4,after_process=down3,k=2,f=100,dropout_r=0.1,activation=True,draw=draw)#Output size around 64\n",
        "    up2 = etage_up(inpt_before=down1,inpt_after=up3,after_process=down2,k=2,f=100,dropout_r=0.1,activation=True,draw=draw)#Output size around 128\n",
        "    up1 = etage_up(inpt_before=level0_in,inpt_after=up2,after_process=down1,k=2,f=100,dropout_r=0.1,activation=True,draw=draw)#Output size around 256\n",
        "    level0_out = concat([level0_in,up1],draw=draw)\n",
        "    level0_out[3] = couche[3]\n",
        "    level0_out = unit(level0_out,k=2,f=3,activation=True,dropout_r=0.1,draw=draw)#Bonne taille\n",
        "    return  level0_out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocq7NRIS6Egq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index_couches = -12\n",
        "def model_fft_training(step,half_batch_size,name,magn_select=True,angle_select=True):#else angle\n",
        "    global index_disc\n",
        "    global index_couches\n",
        "    index_couches = 1\n",
        "    graph = new_graph()\n",
        "    inpt = g_input(graph=graph, improvements=step, shape=(256, 256, 3),content_name=\"Image\")\n",
        "    inpt_real = g_input(graph=graph, improvements=step, shape=(256, 256, 3),content_name=\"Image_real\",draw=False)\n",
        "\n",
        "    magn,angle = rgb_to_magn_angle(inpt,draw=True)\n",
        "    r_magn,r_angle = rgb_to_magn_angle(inpt_real,draw=False)\n",
        "\n",
        "    if magn_select == True:\n",
        "        magn = analyse_progressive_training_model(magn,step=step,init=3)\n",
        "    if angle_select == True:\n",
        "        angle = analyse_progressive_training_model(angle,step=step,init=150)\n",
        "    img_rgb = to_rgb(magn=magn,angle=angle,magn_train=magn_select,angle_train=angle_select) \n",
        "    model = Model(inputs=[inpt[0],inpt_real[0]],outputs=img_rgb[0],name='gen')\n",
        "    end_graph(img_rgb[1],name=name)\n",
        "    return model, r_magn[0],r_angle[0], magn[0], angle[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsoRVtreIg6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# validation = None\n",
        "# for step in range(38,100):#21\n",
        "#     for training_L in [[False,True],[True,False],[True,True]]:\n",
        "#         base_name = \"Pretrain_test_fft_%d\"%10\n",
        "#         graph_name = base_name\n",
        "#         if training_L[0] == True:\n",
        "#             graph_name += \"_magn\"\n",
        "#         if training_L[1] == True:\n",
        "#             graph_name += \"_angle\"\n",
        "#         graph_name = graph_name + \"_%d\"%(step)\n",
        "#         backup_path = graph_name + \".h5\"\n",
        "#         global index_couches\n",
        "#         model_fft_training(step,half_batch_size=5,name=graph_name,magn_select=training_L[0],angle_select=training_L[1])\n",
        "#         print(\"Is it a correct graph for step %d angle %r et magn %r ?\"%(step,training_L[0],training_L[1]))\n",
        "#         print(\"path : %s\"%('%s.png'%(graph_name)))\n",
        "#         graph = imgIPython(filename='%s.png'%(graph_name)) \n",
        "#         display(graph)\n",
        "#         validation = input()\n",
        "#         if validation == \"e\":\n",
        "#             break\n",
        "#         if validation != \"t\":\n",
        "#             raise Exception(\"The graph is not correct\")\n",
        "    \n",
        "#     if validation == \"e\":\n",
        "#         break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PK0jE-_mijZ",
        "colab_type": "text"
      },
      "source": [
        "### Backup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuRmrZ47b9aP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def restore_modified_network(past_model,new_model):\n",
        "    layers = [layer.name for layer in new_model.layers]\n",
        "    for layer in past_model.layers:\n",
        "        if layer.name in layers:\n",
        "            print(\"Restoring layer %s\"%layer.name)\n",
        "            try:\n",
        "                new_model.get_layer(layer.name).set_weights(layer.get_weights())\n",
        "            except Exception as e:\n",
        "                print(\"Can't load weights because \")\n",
        "                print(str(e))\n",
        "                print(\"So, nothing done for the layer %s\"%(layer.name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwGgsIqMUEQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def search_restore_pt(index_model,model,magn_trained,angle_trained,base_name):\n",
        "    best_angle_training = None\n",
        "    best_magn_training = None\n",
        "    max_step_angle = 0\n",
        "    max_step_magn = 0\n",
        "    folder = \"Model%d/\"%index_model\n",
        "    if os.path.exists(folder) == False:\n",
        "        print(\"No restore point found\")\n",
        "        return None\n",
        "    for file in os.listdir(folder):\n",
        "        if file.endswith(\".h5\") and base_name in file:\n",
        "            if \"magn\" in file and int(file.split(\".\")[0].split(\"_\")[-1]) >= max_step_magn:\n",
        "                best_magn_training = folder + file\n",
        "                max_step_magn = int(file.split(\".\")[0].split(\"_\")[-1])\n",
        "            if \"angle\" in file and int(file.split(\".\")[0].split(\"_\")[-1]) >= max_step_angle:\n",
        "                best_angle_training = folder + file\n",
        "                max_step_angle = int(file.split(\".\")[0].split(\"_\")[-1])\n",
        "    # Pour cette étape d'entrainement, \n",
        "    #       - soit uniquement l'un des deux a été entrainé séparément : cas où à l'iteration step on arrive à magn = True et angle = True \n",
        "    #               -> magn déjà entrainé pour step mais angle en est encore à step-1\n",
        "    #           --> on restaure d'abord le fichier contenant angle et magn de l'étape précédante puis celui d'entrainement particulier\n",
        "    #           --> soit d'abord dans tous les cas le fichier de angle puis celui de magn\n",
        "    #       - soit les deux ont été entrainés séparément\n",
        "    #           --> on restaure les deux fichier sans ordre préférentiel car chaque fichier ne contient pas d'info sur un reseau analysant l'autre partie de l'image\n",
        "    #       - soit les deux ont été entrainés en commun \n",
        "    #           --> on restaure d'abord le fichier contenant angle et magn de l'étape précédante puis celui d'entrainement particulier\n",
        "    # Dans l'ordre on fait angle1 -> magn1 -> magn_angle1 -> angle2 -> magn2 -> magn_angle2 -> ...\n",
        "    if (angle_trained == True and magn_trained == False) or (angle_trained == False and magn_trained == True) and (best_angle_training != None and best_magn_training != None):\n",
        "        # Cas angle2 ou magn2\n",
        "        print(\"Restoring weights...\")\n",
        "        model_prec,_,_, _,_ = model_fft_training(step=max_step_magn,#To avoid problems after training angle of this step\n",
        "                                                                           half_batch_size=5,\n",
        "                                                                  name=graph_name,magn_select=True,\n",
        "                                                                  angle_select=True)\n",
        "        model_prec.load_weights(best_angle_training,by_name=True)\n",
        "        restore_modified_network(past_model=model_prec,new_model=model)\n",
        "    elif (angle_trained == True and magn_trained == True) and (best_angle_training != None and best_magn_training != None):\n",
        "        print(\"Restoring weights from models angle_magn into magn_angle...\")\n",
        "        # Cas magn_angle1 : on construit angle1 et magn1 et on met les variables dans le nouveau modèle\n",
        "        model_prec_angle,_,_, _,_ = model_fft_training(step=max_step_magn,half_batch_size=5,\n",
        "                                                                  name=base_name,magn_select=True,\n",
        "                                                                  angle_select=True)\n",
        "        model_prec_magn,_,_, _,_ = model_fft_training(step=max_step_magn,half_batch_size=5,\n",
        "                                                                  name=base_name,magn_select=True,\n",
        "                                                                  angle_select=True)\n",
        "        model_prec_angle.load_weights(best_angle_training,by_name=True)\n",
        "        restore_modified_network(past_model=model_prec_angle,new_model=model)\n",
        "        model_prec_magn.load_weights(best_magn_training,by_name=True)\n",
        "        restore_modified_network(past_model=model_prec_magn,new_model=model)\n",
        "    else:\n",
        "        print(\"%s restore point found for angle and %s restore point found for magn\"%(\"No\" if best_angle_training == None else \"One\",\"no\" if best_magn_training == None else \"one\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfvAihJ3eQwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backup_fft(num_model,model,image,imageBruitee,graph_name,step,iteration,metrics_list_names,model_state_backup_int=5):\n",
        "    if os.path.exists(\"Model%d\"%num_model) == False:\n",
        "        os.mkdir(\"Model%d\"%num_model)\n",
        "    folder = \"Model%d/\"%num_model\n",
        "    metrics_list_values_clean = model.test_on_batch([image,image],image)\n",
        "    metrics_list_values_noise = model.test_on_batch([imageBruitee,image],image)\n",
        "    if len(metrics_list_values_clean) != len(metrics_list_names) and len(metrics_list_values_noise) != len(metrics_list_names):\n",
        "        raise Exception(\"Metrics names number and model outputs not matching with %d values for model output and %d values for names\"%(len(metrics_list_values_clean),len(metrics_list_names)))\n",
        "    metrics_names = []\n",
        "    metrics_list_values = []\n",
        "    for i,name in enumerate(metrics_list_names):\n",
        "        metrics_names += [name+\"_clean\",name+\"_noise\"]\n",
        "        metrics_list_values += [metrics_list_values_clean[i],metrics_list_values_noise[i]]\n",
        "    metrics_names = [\"Iteration\"] + metrics_names\n",
        "    metrics_list_values = [iteration] + metrics_list_values\n",
        "    if iteration % model_state_backup_int == 0:\n",
        "        model.save_weights(folder + graph_name+\".h5\")\n",
        "        save_img(image,folder + graph_name+\"_%d_img\"%iteration)\n",
        "        save_img(imageBruitee,folder + graph_name+\"_%d_img_bruit\"%iteration)\n",
        "        img_gen = model.predict([image,image])\n",
        "        img_bruit_gen = model.predict([imageBruitee,image])\n",
        "        save_img(np.clip(img_gen,0,1),folder + graph_name+\"_%d_img_gen\"%iteration)\n",
        "        save_img(np.clip(img_bruit_gen,0,1),folder + graph_name+\"_%d_img_bruit_gen\"%iteration)\n",
        "\n",
        "    append_write = \"w\"\n",
        "    backup_file_path = folder + graph_name+\"_step%d_metrics.txt\"%step\n",
        "    if os.path.exists(backup_file_path):\n",
        "        append_write = 'a' # append if already exists\n",
        "    with open(backup_file_path,append_write) as file:\n",
        "        if os.stat(backup_file_path).st_size == 0:\n",
        "            file.write(\",\".join(list(map(lambda x:str(x),metrics_names)))+\"\\n\")\n",
        "        file.write(\",\".join(list(map(lambda x:str(x),metrics_list_values)))+\"\\n\")\n",
        "    print(\"\".join([metrics_names[i] + \" : \" + str(metrics_list_values[i]) + \"; \" for i in range(len(metrics_list_values))]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc6r3TZnmq1V",
        "colab_type": "text"
      },
      "source": [
        "### Loss & Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfMItCyYTF6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fft_loss(r_magn,r_angle, gen_magn,gen_angle):\n",
        "    def loss(y_true,y_pred):\n",
        "        return K.mean(K.square(r_magn - gen_magn), axis=-1) + K.mean(K.square(r_angle - gen_angle), axis=-1)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3Jha5tSOztJT",
        "outputId": "b35bb5a5-c2a1-4681-c5f4-8e6368824f8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "Lmetrics = []\n",
        "Lnames = []\n",
        "def generate_functions(name,type_data,input_first_function,expression):\n",
        "    global Lmetrics\n",
        "    global Lnames\n",
        "    Lnames.append(\"%s\"%(expression).replace(\"y\",\"rgb\"))\n",
        "    Lmetrics.append(\"metrics_%s_%s(%s)\"%(type_data,name,\",\".join(input_first_function)))\n",
        "    print(\"def metrics_%s_%s(%s):\\n\\tdef %s_%s(y_true,y_pred):\\n\\t\\treturn %s\\n\\treturn %s_%s\"%(type_data,name,\",\".join(input_first_function),type_data,name,expression,type_data,name))\n",
        "    print()\n",
        "for minmax in [\"min\",\"max\"]:\n",
        "    for magnangle in [\"magn\",\"angle\"]:\n",
        "        generate_functions(\"%s_%s\"%(minmax,magnangle),\"fft\",[\"r_%s\"%(magnangle)],\"K.%s(r_%s)\"%(minmax,magnangle))\n",
        "for magnangle in [\"magn\",\"angle\"]:\n",
        "    generate_functions(\"max_abs_diff_%s\"%(magnangle),\"fft\",[\"r_%s\"%(magnangle),\"gen_%s\"%(magnangle)],\"K.max(K.abs(r_%s-gen_%s))\"%(magnangle,magnangle))\n",
        "for magnangle in [\"magn\",\"angle\"]:\n",
        "    generate_functions(\"mean_abs_diff_%s\"%(magnangle),\"fft\",[\"r_%s\"%(magnangle),\"gen_%s\"%(magnangle)],\"K.mean(K.abs(r_%s-gen_%s))\"%(magnangle,magnangle))\n",
        "for magnangle in [\"magn\",\"angle\"]:\n",
        "    generate_functions(\"var_abs_diff_%s\"%(magnangle),\"fft\",[\"r_%s\"%(magnangle),\"gen_%s\"%(magnangle)],\"K.var(K.abs(r_%s-gen_%s))\"%(magnangle,magnangle))\n",
        "generate_functions(\"max_abs_diff_%s\"%(magnangle),\"rgb\",[],\"K.mean(K.abs(y_true-y_pred))\")\n",
        "print(\"[%s]\"%(\",\".join(Lmetrics)))\n",
        "print(\"[%s]\"%(\",\".join(list(map(lambda x:\"'\"+x+\"'\",Lnames)))))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "def metrics_fft_min_magn(r_magn):\n",
            "\tdef fft_min_magn(y_true,y_pred):\n",
            "\t\treturn K.min(r_magn)\n",
            "\treturn fft_min_magn\n",
            "\n",
            "def metrics_fft_min_angle(r_angle):\n",
            "\tdef fft_min_angle(y_true,y_pred):\n",
            "\t\treturn K.min(r_angle)\n",
            "\treturn fft_min_angle\n",
            "\n",
            "def metrics_fft_max_magn(r_magn):\n",
            "\tdef fft_max_magn(y_true,y_pred):\n",
            "\t\treturn K.max(r_magn)\n",
            "\treturn fft_max_magn\n",
            "\n",
            "def metrics_fft_max_angle(r_angle):\n",
            "\tdef fft_max_angle(y_true,y_pred):\n",
            "\t\treturn K.max(r_angle)\n",
            "\treturn fft_max_angle\n",
            "\n",
            "def metrics_fft_max_abs_diff_magn(r_magn,gen_magn):\n",
            "\tdef fft_max_abs_diff_magn(y_true,y_pred):\n",
            "\t\treturn K.max(K.abs(r_magn-gen_magn))\n",
            "\treturn fft_max_abs_diff_magn\n",
            "\n",
            "def metrics_fft_max_abs_diff_angle(r_angle,gen_angle):\n",
            "\tdef fft_max_abs_diff_angle(y_true,y_pred):\n",
            "\t\treturn K.max(K.abs(r_angle-gen_angle))\n",
            "\treturn fft_max_abs_diff_angle\n",
            "\n",
            "def metrics_fft_mean_abs_diff_magn(r_magn,gen_magn):\n",
            "\tdef fft_mean_abs_diff_magn(y_true,y_pred):\n",
            "\t\treturn K.mean(K.abs(r_magn-gen_magn))\n",
            "\treturn fft_mean_abs_diff_magn\n",
            "\n",
            "def metrics_fft_mean_abs_diff_angle(r_angle,gen_angle):\n",
            "\tdef fft_mean_abs_diff_angle(y_true,y_pred):\n",
            "\t\treturn K.mean(K.abs(r_angle-gen_angle))\n",
            "\treturn fft_mean_abs_diff_angle\n",
            "\n",
            "def metrics_fft_var_abs_diff_magn(r_magn,gen_magn):\n",
            "\tdef fft_var_abs_diff_magn(y_true,y_pred):\n",
            "\t\treturn K.var(K.abs(r_magn-gen_magn))\n",
            "\treturn fft_var_abs_diff_magn\n",
            "\n",
            "def metrics_fft_var_abs_diff_angle(r_angle,gen_angle):\n",
            "\tdef fft_var_abs_diff_angle(y_true,y_pred):\n",
            "\t\treturn K.var(K.abs(r_angle-gen_angle))\n",
            "\treturn fft_var_abs_diff_angle\n",
            "\n",
            "def metrics_rgb_max_abs_diff_angle():\n",
            "\tdef rgb_max_abs_diff_angle(y_true,y_pred):\n",
            "\t\treturn K.mean(K.abs(y_true-y_pred))\n",
            "\treturn rgb_max_abs_diff_angle\n",
            "\n",
            "[metrics_fft_min_magn(r_magn),metrics_fft_min_angle(r_angle),metrics_fft_max_magn(r_magn),metrics_fft_max_angle(r_angle),metrics_fft_max_abs_diff_magn(r_magn,gen_magn),metrics_fft_max_abs_diff_angle(r_angle,gen_angle),metrics_fft_mean_abs_diff_magn(r_magn,gen_magn),metrics_fft_mean_abs_diff_angle(r_angle,gen_angle),metrics_fft_var_abs_diff_magn(r_magn,gen_magn),metrics_fft_var_abs_diff_angle(r_angle,gen_angle),metrics_rgb_max_abs_diff_angle()]\n",
            "['K.min(r_magn)','K.min(r_angle)','K.max(r_magn)','K.max(r_angle)','K.max(K.abs(r_magn-gen_magn))','K.max(K.abs(r_angle-gen_angle))','K.mean(K.abs(r_magn-gen_magn))','K.mean(K.abs(r_angle-gen_angle))','K.var(K.abs(r_magn-gen_magn))','K.var(K.abs(r_angle-gen_angle))','K.mean(K.abs(rgb_true-rgb_pred))']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fBvWsQPHz0br",
        "colab": {}
      },
      "source": [
        "def metrics_fft_min_magn(r_magn):\n",
        "\tdef fft_min_magn(y_true,y_pred):\n",
        "\t\treturn K.min(r_magn)\n",
        "\treturn fft_min_magn\n",
        "\n",
        "def metrics_fft_min_angle(r_angle):\n",
        "\tdef fft_min_angle(y_true,y_pred):\n",
        "\t\treturn K.min(r_angle)\n",
        "\treturn fft_min_angle\n",
        "\n",
        "def metrics_fft_max_magn(r_magn):\n",
        "\tdef fft_max_magn(y_true,y_pred):\n",
        "\t\treturn K.max(r_magn)\n",
        "\treturn fft_max_magn\n",
        "\n",
        "def metrics_fft_max_angle(r_angle):\n",
        "\tdef fft_max_angle(y_true,y_pred):\n",
        "\t\treturn K.max(r_angle)\n",
        "\treturn fft_max_angle\n",
        "\n",
        "def metrics_fft_max_abs_diff_magn(r_magn,gen_magn):\n",
        "\tdef fft_max_abs_diff_magn(y_true,y_pred):\n",
        "\t\treturn K.max(K.abs(r_magn-gen_magn))\n",
        "\treturn fft_max_abs_diff_magn\n",
        "\n",
        "def metrics_fft_max_abs_diff_angle(r_angle,gen_angle):\n",
        "\tdef fft_max_abs_diff_angle(y_true,y_pred):\n",
        "\t\treturn K.max(K.abs(r_angle-gen_angle))\n",
        "\treturn fft_max_abs_diff_angle\n",
        "\n",
        "def metrics_fft_mean_abs_diff_magn(r_magn,gen_magn):\n",
        "\tdef fft_mean_abs_diff_magn(y_true,y_pred):\n",
        "\t\treturn K.mean(K.abs(r_magn-gen_magn))\n",
        "\treturn fft_mean_abs_diff_magn\n",
        "\n",
        "def metrics_fft_mean_abs_diff_angle(r_angle,gen_angle):\n",
        "\tdef fft_mean_abs_diff_angle(y_true,y_pred):\n",
        "\t\treturn K.mean(K.abs(r_angle-gen_angle))\n",
        "\treturn fft_mean_abs_diff_angle\n",
        "\n",
        "def metrics_fft_var_abs_diff_magn(r_magn,gen_magn):\n",
        "\tdef fft_var_abs_diff_magn(y_true,y_pred):\n",
        "\t\treturn K.var(K.abs(r_magn-gen_magn))\n",
        "\treturn fft_var_abs_diff_magn\n",
        "\n",
        "def metrics_fft_var_abs_diff_angle(r_angle,gen_angle):\n",
        "\tdef fft_var_abs_diff_angle(y_true,y_pred):\n",
        "\t\treturn K.var(K.abs(r_angle-gen_angle))\n",
        "\treturn fft_var_abs_diff_angle\n",
        "\n",
        "def metrics_rgb_max_abs_diff_angle():\n",
        "\tdef rgb_max_abs_diff_angle(y_true,y_pred):\n",
        "\t\treturn K.mean(K.abs(y_true-y_pred))\n",
        "\treturn rgb_max_abs_diff_angle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42Z12kIwmvJS",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy-tE33133y9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_progressive_model(index_model,max_step=200, max_it=2000):\n",
        "    train_data,validation_data,test_data = create_datasets()\n",
        "    max_total_it = (max_step-1)*3*max_it\n",
        "    tot_iteration = 0\n",
        "    for step in range(40,max_step):\n",
        "        magn_choice, angle_choice = True,True\n",
        "        if magn_choice == False and angle_choice == False:\n",
        "            continue\n",
        "\n",
        "        base_name = \"Pretrain_fft_%d\"%index_model\n",
        "        graph_name = base_name\n",
        "        if magn_choice == True:\n",
        "            graph_name += \"_magn\"\n",
        "        if angle_choice == True:\n",
        "            graph_name += \"_angle\"\n",
        "        graph_name = graph_name + \"_%d\"%(step)\n",
        "        backup_path = graph_name + \".h5\"\n",
        "        model,r_magn,r_angle, gen_magn,gen_angle = model_fft_training(step=step,half_batch_size=5,\n",
        "                                                                        name=graph_name,magn_select=magn_choice,angle_select=angle_choice)\n",
        "        \n",
        "        metrics_list = [metrics_fft_min_magn(r_magn),metrics_fft_min_angle(r_angle),\n",
        "                        metrics_fft_max_magn(r_magn),metrics_fft_max_angle(r_angle),\n",
        "                        metrics_fft_max_abs_diff_magn(r_magn,gen_magn),metrics_fft_max_abs_diff_angle(r_angle,gen_angle),\n",
        "                        metrics_fft_mean_abs_diff_magn(r_magn,gen_magn),metrics_fft_mean_abs_diff_angle(r_angle,gen_angle),\n",
        "                        metrics_fft_var_abs_diff_magn(r_magn,gen_magn),metrics_fft_var_abs_diff_angle(r_angle,gen_angle),\n",
        "                        metrics_rgb_max_abs_diff_angle()]\n",
        "        metrics_list_names = [\"Loss\",'K.min(r_magn)','K.min(r_angle)',\n",
        "                                'K.max(r_magn)','K.max(r_angle)',\n",
        "                                'K.max(K.abs(r_magn-gen_magn))','K.max(K.abs(r_angle-gen_angle))',\n",
        "                                'K.mean(K.abs(r_magn-gen_magn))','K.mean(K.abs(r_angle-gen_angle))',\n",
        "                                'K.var(K.abs(r_magn-gen_magn))','K.var(K.abs(r_angle-gen_angle))',\n",
        "                                'K.mean(K.abs(rgb_true-rgb_pred))']\n",
        "        optimizer = Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "        model.compile(loss=fft_loss(r_magn,r_angle, gen_magn,gen_angle),metrics=metrics_list,optimizer=\"Adam\")\n",
        "        search_restore_pt(index_model,model=model, magn_trained=magn_choice,angle_trained=angle_choice,\n",
        "                            base_name=graph_name)\n",
        "        for iteration in range(max_it):\n",
        "            if step == 40 and iteration < 1406:\n",
        "                continue\n",
        "            image,imageBruitee = next_batch_bruit_voile_2(5,train_data,256,np.float32,\n",
        "                                                            [[0,1.-iteration/(max_it+1)],[0,1.-iteration/(max_it+1)],[0,1.-iteration/(max_it+1)]],\n",
        "                                                            [iteration//100*100/max_it/2,iteration//100*100/max_it/2],[0,1])\n",
        "            x = np.concatenate((image,imageBruitee),axis=0)\n",
        "            y = np.concatenate((image,image),axis=0)\n",
        "            metrics_list_values = model.train_on_batch([x,y],y)\n",
        "            loss = metrics_list_values[0]\n",
        "            backup_fft(index_model,model,image,imageBruitee,graph_name,step,iteration,metrics_list_names,model_state_backup_int=5)\n",
        "            whose_is_training = \"\"\n",
        "            if magn_choice == True and angle_choice == True:\n",
        "                whose_is_training += \"magn et angle\" \n",
        "            elif magn_choice == True:\n",
        "                whose_is_training += \"magn\" \n",
        "            elif angle_choice == True:\n",
        "                whose_is_training += \"angle\" \n",
        "            print(\"Etape d'entrainement %d de %s iteration %d statut %d pourcent, erreur %.5e\"%(step,whose_is_training, iteration,tot_iteration/max_total_it,loss))\n",
        "            tot_iteration += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBzmhOxJ_qvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_progressive_model(11)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJGaD3wJglbR",
        "colab_type": "text"
      },
      "source": [
        "# Discriminator pretraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w4Y7R97jM1H",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4xgaXRpjOhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_model():\n",
        "    inpt = Input(shape=(256, 256, 3), name=\"Input\")\n",
        "    xception_model = tensorflow.keras.applications.xception.Xception(include_top=False, weights=None, input_tensor=inpt, input_shape=(256,256,3), pooling='max', classes=1)\n",
        "    filterModification = Dense(units=1,activation=None)(xception_model.output)\n",
        "    return Model(inputs=inpt,outputs=filterModification, name='disc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW_c4jsRgqJv",
        "colab_type": "text"
      },
      "source": [
        "## Losses & metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWfpXwi1eFfG",
        "colab_type": "code",
        "outputId": "d288da58-3206-41fe-f559-70a31575c9d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "Lmetrics = []\n",
        "Lnames = []\n",
        "generate_functions(\"max_abs_diff_%s\"%(magnangle),\"rgb\",[],\"K.max(K.abs(y_true-y_pred))\")\n",
        "generate_functions(\"mean_abs_diff_%s\"%(magnangle),\"rgb\",[],\"K.mean(K.abs(y_true-y_pred))\")\n",
        "generate_functions(\"std_abs_diff_%s\"%(magnangle),\"rgb\",[],\"K.std(K.abs(y_true-y_pred))\")\n",
        "print(\"[%s]\"%(\",\".join(Lmetrics)))\n",
        "print(\"[%s]\"%(\",\".join(list(map(lambda x:\"'\"+x+\"'\",Lnames)))))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "def metrics_rgb_max_abs_diff_angle():\n",
            "\tdef rgb_max_abs_diff_angle(y_true,y_pred):\n",
            "\t\treturn K.max(K.abs(y_true-y_pred))\n",
            "\treturn rgb_max_abs_diff_angle\n",
            "\n",
            "def metrics_rgb_mean_abs_diff_angle():\n",
            "\tdef rgb_mean_abs_diff_angle(y_true,y_pred):\n",
            "\t\treturn K.mean(K.abs(y_true-y_pred))\n",
            "\treturn rgb_mean_abs_diff_angle\n",
            "\n",
            "def metrics_rgb_std_abs_diff_angle():\n",
            "\tdef rgb_std_abs_diff_angle(y_true,y_pred):\n",
            "\t\treturn K.std(K.abs(y_true-y_pred))\n",
            "\treturn rgb_std_abs_diff_angle\n",
            "\n",
            "[metrics_rgb_max_abs_diff_angle(),metrics_rgb_mean_abs_diff_angle(),metrics_rgb_std_abs_diff_angle()]\n",
            "['K.max(K.abs(rgb_true-rgb_pred))','K.mean(K.abs(rgb_true-rgb_pred))','K.std(K.abs(rgb_true-rgb_pred))']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Xfuu33MgBXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics_rgb_max_abs_diff_angle():\n",
        "\tdef rgb_max_abs_diff_angle(y_true,y_pred):\n",
        "\t\treturn K.max(K.abs(y_true-y_pred))\n",
        "\treturn rgb_max_abs_diff_angle\n",
        "\n",
        "def metrics_rgb_mean_abs_diff_angle():\n",
        "\tdef rgb_mean_abs_diff_angle(y_true,y_pred):\n",
        "\t\treturn K.mean(K.abs(y_true-y_pred))\n",
        "\treturn rgb_mean_abs_diff_angle\n",
        "\n",
        "def metrics_rgb_std_abs_diff_angle():\n",
        "\tdef rgb_std_abs_diff_angle(y_true,y_pred):\n",
        "\t\treturn K.std(K.abs(y_true-y_pred))\n",
        "\treturn rgb_std_abs_diff_angle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC-kuitmgwe8",
        "colab_type": "text"
      },
      "source": [
        "## Backup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JuC82QjCg7pd",
        "colab": {}
      },
      "source": [
        "def search_restore_pt(index_model,model,base_name):\n",
        "    \"\"\"\n",
        "    We only have to restore previous version of the model\"\"\"\n",
        "    folder = \"Model_disc%d/\"%index_model\n",
        "    if os.path.exists(folder) == False:\n",
        "        print(\"No restore point found\")\n",
        "        return None\n",
        "    model_path = \"\"\n",
        "    for file in os.listdir(folder):\n",
        "        if file.endswith(\".h5\") and base_name in file:\n",
        "            model_path = file\n",
        "    if model_path == \"\":\n",
        "        print(\"No restore point found\")\n",
        "        return None\n",
        "    try:\n",
        "        model.load_weights(model_path,by_name=True)\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "        print(\"Skipping restoration\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HsmNpZZkRCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def append_or_write(backup_file_path):\n",
        "    mode =\"w\"\n",
        "    if os.path.exists(backup_file_path):\n",
        "        append_write = 'a'\n",
        "    return mode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haLnBU8ulD1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_to_file(backup_file_path,titles,data_list_list):\n",
        "    with open(backup_file_path,append_or_write(backup_file_path=backup_file_path)) as file:\n",
        "        if os.stat(backup_file_path).st_size == 0:\n",
        "            file.write(\",\".join(titles)+\"\\n\")\n",
        "        for line in data_list_list:\n",
        "            file.write(\",\".join(line)+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoRyudZnpAdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_input_output(img,imgNoise):\n",
        "    \"\"\"D'après la fonction backup_fft ci-dessus on prend comme convention que l'IA prédit la probabilité que \n",
        "    l'image d'entrée soit bruitée\n",
        "    \"\"\"\n",
        "    inpt = np.concatenate((img,imgNoise),axis=0)\n",
        "    outpt = np.zeros(shape=(inpt.shape[0]))\n",
        "    return inpt,outpt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4FdEcjErg7ph",
        "colab": {}
      },
      "source": [
        "def backup_fft(num_model,model,image,imageBruitee,graph_name,step,iteration,metrics_list_names,model_state_backup_int=5):\n",
        "    folder = \"Model_disc%d/\"%num_model\n",
        "    if os.path.exists(folder) == False:\n",
        "        os.mkdir(folder)\n",
        "    \n",
        "    outpt_clean = np.zeros(shape=(image.shape[0]))\n",
        "    outpt_noise = np.zeros(shape=(imageBruitee.shape[0]))\n",
        "\n",
        "    metrics_list_values_clean = model.test_on_batch(image,outpt_clean)\n",
        "    metrics_list_values_noise = model.test_on_batch(imageBruitee,outpt_noise)\n",
        "    if len(metrics_list_values_clean) != len(metrics_list_names) and len(metrics_list_values_noise) != len(metrics_list_names):\n",
        "        print(\"metrics_list_values_clean : \",metrics_list_values_clean)\n",
        "        print(\"metrics_list_names : \",metrics_list_names)\n",
        "        raise Exception(\"Metrics names number and model outputs not matching with %d values for model output and %d values for names\"%(len(metrics_list_values_clean),len(metrics_list_names)))\n",
        "    metrics_names = []\n",
        "    metrics_list_values = []\n",
        "    for i,name in enumerate(metrics_list_names):\n",
        "        metrics_names += [name+\"_clean\",name+\"_noise\"]\n",
        "        metrics_list_values += [metrics_list_values_clean[i],metrics_list_values_noise[i]]\n",
        "    metrics_names = [\"Iteration\"] + metrics_names\n",
        "    metrics_list_values = [iteration] + metrics_list_values\n",
        "    if iteration % model_state_backup_int == 0:\n",
        "        model.save_weights(folder + graph_name+\".h5\")\n",
        "        save_img(image,folder + graph_name+\"_%d_img\"%iteration)\n",
        "        save_img(imageBruitee,folder + graph_name+\"_%d_img_bruit\"%iteration)\n",
        "        prediction_clean_gen = model.predict(image)\n",
        "        prediction_clean_bruit_gen = model.predict(imageBruitee)\n",
        "        \n",
        "        backup_prediction_path = folder + graph_name + \"_step%d_predictions.txt\"%(step)\n",
        "        titles = [\"Prediction_bruitee_orig_clean\",\"Prediction_bruitee_orig_noise\"]\n",
        "        predictions = [str(prediction_clean_gen[0]),str(prediction_clean_bruit_gen[0])]\n",
        "        write_to_file(backup_file_path=backup_prediction_path,titles=titles,data_list_list=[predictions])\n",
        "\n",
        "    backup_file_path = folder + graph_name+\"_step%d_metrics.txt\"%step\n",
        "    titles = list(map(lambda x:str(x),metrics_names))\n",
        "    data = list(map(lambda x:str(x),metrics_list_values))\n",
        "    write_to_file(backup_file_path=backup_file_path,titles=titles,data_list_list=[data])\n",
        "    print(\"\".join([metrics_names[i] + \" : \" + str(metrics_list_values[i]) + \"; \" for i in range(len(metrics_list_values))]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etsrjWLhgu9f",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqQc-ZGyc8XR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_progressive_model(index_model,max_step=200, max_it=2000):\n",
        "    train_data,validation_data,test_data = create_datasets()\n",
        "    max_total_it = (max_step-1)*3*max_it\n",
        "    tot_iteration = 0\n",
        "    base_name = \"Pretrain_fft_disc_%d\"%index_model\n",
        "    for step in range(max_step):\n",
        "        graph_name = base_name\n",
        "        graph_name = graph_name + \"_%d\"%(step)\n",
        "        backup_path = graph_name + \".h5\"\n",
        "        model = generate_model()\n",
        "            \n",
        "        metrics_list = [metrics_rgb_max_abs_diff_angle(),metrics_rgb_mean_abs_diff_angle(),metrics_rgb_std_abs_diff_angle()]\n",
        "        metrics_list_names = ['binary_crossentropy','K.max(K.abs(rgb_true-rgb_pred))','K.mean(K.abs(rgb_true-rgb_pred))','K.std(K.abs(rgb_true-rgb_pred))']\n",
        "\n",
        "        model.compile(loss=\"binary_crossentropy\",metrics=metrics_list,optimizer=\"Adam\")\n",
        "        search_restore_pt(index_model,model=model,\n",
        "                            base_name=graph_name)\n",
        "        for iteration in range(max_it):\n",
        "            image,imageBruitee = next_batch_bruit_voile_2(5,train_data,256,np.float32,\n",
        "                                                            [[0,1.-iteration/(max_it+1)],[0,1.-iteration/(max_it+1)],[0,1.-iteration/(max_it+1)]],\n",
        "                                                            [iteration//100*100/max_it/2,iteration//100*100/max_it/2],[0,1])\n",
        "            inpt,outpt = generate_input_output(image,imageBruitee)\n",
        "            metrics_list_values = model.train_on_batch(inpt,outpt)\n",
        "            loss = metrics_list_values[0]\n",
        "            backup_fft(index_model,model,image,imageBruitee,graph_name,step,iteration,metrics_list_names,model_state_backup_int=5)\n",
        "            \n",
        "            print(\"Etape d'entrainement %d iteration %d statut %d pourcent, erreur %.5e\"%(step, iteration,tot_iteration/max_total_it,loss))\n",
        "            tot_iteration += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTF5jvnOoMlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_progressive_model(index_model=11)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8trBVnWlQuzb",
        "colab_type": "text"
      },
      "source": [
        "# GAN Pretrain FFT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4bIdpp-RB63",
        "colab_type": "text"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbIGSHrHQ2Zk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "K.clear_session()\n",
        "def discriminateur():\n",
        "    return generate_model()\n",
        "def generate_models(name):\n",
        "    gen_model,r_magn,r_angle, magn, angle = model_fft_training(step=50,half_batch_size=5,name=name)\n",
        "    gen = gen_model(gen_model.input)\n",
        "    disc = discriminateur()\n",
        "    disc_gan = disc(gen)\n",
        "    return gen_model,r_magn,r_angle, magn, angle,disc,Model(inputs=gen_model.input,outputs=disc_gan)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjnLQ2eFS2yX",
        "colab_type": "text"
      },
      "source": [
        "## Backup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifFaaAQPZCfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def search_restore_pt(index_model,model_gen_disc,base_name):\n",
        "    best_training = None\n",
        "    max_step = 0\n",
        "    folder = \"Model_gan_fft%d/\"%index_model\n",
        "    if os.path.exists(folder) == False:\n",
        "        print(\"No restore point found\")\n",
        "        return None\n",
        "    for file in os.listdir(folder):\n",
        "        if file.endswith(\".h5\") and base_name in file:\n",
        "            if \"GAN\" in file and int(file.split(\".\")[0].split(\"_\")[-1]) >= max_step_magn:\n",
        "                best_magn_training = folder + file\n",
        "                max_step_magn = int(file.split(\".\")[0].split(\"_\")[-1])\n",
        "    if (best_training == None):\n",
        "        print(\"No GAN training found ; Restoring from splited files\")\n",
        "        pathDisc = \"/content/drive/My Drive/TIPE/Model_disc11/Pretrain_fft_disc_11_4.h5\"\n",
        "        pathGen = \"/content/drive/My Drive/TIPE/Model11/Pretrain_fft_11_magn_angle_43.h5\"\n",
        "        ## Restore gen part\n",
        "        model_prec_fft,_,_, _,_ = model_fft_training(step=43,half_batch_size=5,\n",
        "                                                                  name=\"Pretrain_fft_11_magn_angle_4\",magn_select=True,\n",
        "                                                                  angle_select=True)\n",
        "        model_prec_fft.load_weights(pathGen,by_name=True)\n",
        "        restore_modified_network(past_model=model_prec_fft,new_model=model_gen_disc)\n",
        "        ## Restore disc part\n",
        "        model_disc_prec = generate_model()\n",
        "        model_disc_prec.load_weights(pathDisc,by_name=True)\n",
        "        restore_modified_network(past_model=model_disc_prec,new_model=model_gen_disc)\n",
        "\n",
        "    elif (best_training != None):\n",
        "        print(\"Restoring weights...\")\n",
        "        model_gen_disc.load_weights(best_training)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdJicV07fswG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backup_fft(num_model,model_gen,model_disc,image,imageBruitee,graph_name,step,iteration,metrics_list_names_gen,metrics_list_names_disc,model_state_backup_int=5):\n",
        "    \n",
        "    folder = \"Model_gan_fft%d/\"%num_model\n",
        "    if os.path.exists(folder) == False:\n",
        "        os.mkdir(folder)\n",
        "    metrics_list_names = [\"Iterations\"]\n",
        "    metrics_list_values = [iteration]\n",
        "    def format_metrics(current_metrics_list_names,metrics_list_values_clean,metrics_list_values_noise,metrics_list_names,metrics_list_values,identifiant=''):\n",
        "        if len(metrics_list_values_clean) != len(current_metrics_list_names) and len(metrics_list_values_noise) != len(current_metrics_list_names):\n",
        "            print(\"metrics_list_values_clean : \",metrics_list_values_clean)\n",
        "            print(\"metrics_list_values_clean : \",metrics_list_values_noise)\n",
        "            print(\"metrics_list_names : \",current_metrics_list_names)\n",
        "            raise Exception(\"Metrics names number and model outputs not matching with %d values for model output and %d values for names\"%(len(metrics_list_values_clean),len(current_metrics_list_names)))\n",
        "        for i,name in enumerate(current_metrics_list_names):\n",
        "            if identifiant != '':\n",
        "                metrics_list_names += [name+\"_%s_clean\",name+\"_%s_noise\"]\n",
        "            else:\n",
        "                metrics_list_names += [name+\"_clean\",name+\"_noise\"]\n",
        "            metrics_list_values += [metrics_list_values_clean[i],metrics_list_values_noise[i]]\n",
        "        return metrics_list_names,metrics_list_values\n",
        "    metrics_list_values_clean = model_gen.test_on_batch([image,image],image)\n",
        "    metrics_list_values_noise = model_gen.test_on_batch([imageBruitee,image],image)\n",
        "    metrics_list_names, metrics_list_values = format_metrics(current_metrics_list_names=metrics_list_names_gen,metrics_list_values_clean=metrics_list_values_clean,metrics_list_values_noise=metrics_list_values_noise,metrics_list_names=metrics_list_names,metrics_list_values=metrics_list_values)\n",
        "\n",
        "    input_gen_clean = model_gen.predict([image,image])\n",
        "    input_gen_noise = model_gen.predict([imageBruitee,image])\n",
        "    oupt_disc_clean = model_disc.predict(input_gen_clean)\n",
        "    oupt_disc_noise = model_disc.predict(input_gen_noise)\n",
        "    outpt_clean = np.zeros(shape=(image.shape[0]))\n",
        "    outpt_noise = np.zeros(shape=(imageBruitee.shape[0]))\n",
        "    metrics_list_values_clean = model_disc.test_on_batch(image,outpt_clean)\n",
        "    metrics_list_values_noise = model_disc.test_on_batch(imageBruitee,outpt_noise)\n",
        "    metrics_list_names, metrics_list_values = format_metrics(current_metrics_list_names=metrics_list_names_gen,metrics_list_values_clean=metrics_list_values_clean,metrics_list_values_noise=metrics_list_values_noise,metrics_list_names=metrics_list_names,metrics_list_values=metrics_list_values)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    if len(metrics_list_values_clean_disc) != len(metrics_list_names_disc) and len(metrics_list_values_noise_disc) != len(metrics_list_names_disc):\n",
        "        print(\"metrics_list_values_clean : \",metrics_list_values_clean_disc)\n",
        "        print(\"metrics_list_names : \",metrics_list_names_disc)\n",
        "        raise Exception(\"Metrics names number and model outputs not matching with %d values for model output and %d values for names\"%(len(metrics_list_values_clean_disc),len(metrics_list_names_disc)))\n",
        "\n",
        "    for i,name in enumerate(metrics_list_names):\n",
        "        metrics_names += [name+\"_gen_clean\",name+\"_gen_noise\"]\n",
        "        metrics_list_values += [metrics_list_values_clean[i],metrics_list_values_noise[i]]\n",
        "\n",
        "    if iteration % model_state_backup_int == 0:\n",
        "        model_gen_disc.save_weights(folder + graph_name+\".h5\")\n",
        "        print(\"Weights saved at %s\"%(folder + graph_name+\".h5\"))\n",
        "        save_img(image,folder + graph_name+\"_%d_img\"%iteration)\n",
        "        save_img(imageBruitee,folder + graph_name+\"_%d_img_bruit\"%iteration)\n",
        "        print(\"Original images saved at %s\"%(folder + graph_name+\"_%d_img\"%iteration))\n",
        "        prediction_clean= model_test.predict(image)\n",
        "        prediction_noise = model_test.predict(imageBruitee)\n",
        "        save_img(prediction_clean[0],folder + graph_name+\"_%d_img_fromclean\"%iteration)\n",
        "        save_img(prediction_noise[0],folder + graph_name+\"_%d_img_fromnoise\"%iteration)\n",
        "\n",
        "        \n",
        "        backup_prediction_path = folder + graph_name + \"_step%d_predictions.txt\"%(step)\n",
        "        titles = [\"Prediction_bruitee_orig_clean\",\"Prediction_bruitee_orig_noise\"]\n",
        "        predictions = [str(prediction_clean[1]),str(prediction_noise[1])]\n",
        "        write_to_file(backup_file_path=backup_prediction_path,titles=titles,data_list_list=[predictions])\n",
        "\n",
        "    backup_file_path = folder + graph_name+\"_step%d_metrics.txt\"%step\n",
        "    titles = list(map(lambda x:str(x),metrics_names))\n",
        "    data = list(map(lambda x:str(x),metrics_list_values))\n",
        "    write_to_file(backup_file_path=backup_file_path,titles=titles,data_list_list=[data])\n",
        "    print(\"\".join([metrics_names[i] + \" : \" + str(metrics_list_values[i]) + \"; \" for i in range(len(metrics_list_values))]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX4TGSxZPcLy",
        "colab_type": "text"
      },
      "source": [
        "## Losses and metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQXVPSkjcDSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_functions(name,type_data,input_first_function,expression):\n",
        "    global Lmetrics\n",
        "    global Lnames\n",
        "    Lnames.append(\"%s\"%(expression).replace(\"y\",\"rgb\"))\n",
        "    Lmetrics.append(\"metrics_%s_%s(%s)\"%(type_data,name,\",\".join(input_first_function)))\n",
        "    print(\"def metrics_%s_%s(%s):\\n\\tdef %s_%s(y_true,y_pred):\\n\\t\\treturn %s\\n\\treturn %s_%s\"%(type_data,name,\",\".join(input_first_function),type_data,name,expression,type_data,name))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl9R5a7nPfJe",
        "colab_type": "code",
        "outputId": "4e526699-20de-43fe-f9af-7ac0e109f190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "Lmetrics = []\n",
        "Lnames = []\n",
        "for typeData in [\"probability\"]:#,\"fft\",\"probability\"]:\n",
        "    for analyse in [\"max\",\"mean\",\"std\"]:\n",
        "        if typeData == \"fft\":\n",
        "            for magnangle in [\"magn\",\"angle\"]:\n",
        "                generate_functions(\"abs_diff_%s_%s\"%(magnangle,analyse),typeData,[\"r_%s\"%magnangle,magnangle],\"K.%s(K.abs(r_%s-%s))\"%(analyse,magnangle,magnangle))\n",
        "        else:\n",
        "            i = \"0\" if typeData == \"rgb\" else \"1\"\n",
        "            generate_functions(\"abs_diff_%s\"%(analyse),typeData,[],\"K.%s(K.abs(y_true[%s]-y_pred[%s]))\"%(analyse,i,i))\n",
        "print(\"[%s]\"%(\",\".join(Lmetrics)))\n",
        "print(\"[%s]\"%(\",\".join(list(map(lambda x:\"'\"+x+\"'\",Lnames)))))"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "def metrics_probability_abs_diff_max():\n",
            "\tdef probability_abs_diff_max(y_true,y_pred):\n",
            "\t\treturn K.max(K.abs(y_true[1]-y_pred[1]))\n",
            "\treturn probability_abs_diff_max\n",
            "\n",
            "def metrics_probability_abs_diff_mean():\n",
            "\tdef probability_abs_diff_mean(y_true,y_pred):\n",
            "\t\treturn K.mean(K.abs(y_true[1]-y_pred[1]))\n",
            "\treturn probability_abs_diff_mean\n",
            "\n",
            "def metrics_probability_abs_diff_std():\n",
            "\tdef probability_abs_diff_std(y_true,y_pred):\n",
            "\t\treturn K.std(K.abs(y_true[1]-y_pred[1]))\n",
            "\treturn probability_abs_diff_std\n",
            "\n",
            "[metrics_probability_abs_diff_max(),metrics_probability_abs_diff_mean(),metrics_probability_abs_diff_std()]\n",
            "['K.max(K.abs(rgb_true[1]-rgb_pred[1]))','K.mean(K.abs(rgb_true[1]-rgb_pred[1]))','K.std(K.abs(rgb_true[1]-rgb_pred[1]))']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca6_nAS4SvRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics_rgb_abs_diff_max():\n",
        "\tdef rgb_abs_diff_max(y_true,y_pred):\n",
        "\t\treturn K.max(K.abs(y_true[0]-y_pred[0]))\n",
        "\treturn rgb_abs_diff_max\n",
        "\n",
        "def metrics_rgb_abs_diff_mean():\n",
        "\tdef rgb_abs_diff_mean(y_true,y_pred):\n",
        "\t\treturn K.mean(K.abs(y_true[0]-y_pred[0]))\n",
        "\treturn rgb_abs_diff_mean\n",
        "\n",
        "def metrics_rgb_abs_diff_std():\n",
        "\tdef rgb_abs_diff_std(y_true,y_pred):\n",
        "\t\treturn K.std(K.abs(y_true[0]-y_pred[0]))\n",
        "\treturn rgb_abs_diff_std\n",
        "\n",
        "# [metrics_rgb_abs_diff_max(),metrics_rgb_abs_diff_mean(),metrics_rgb_abs_diff_std()]\n",
        "# ['K.max(K.abs(rgb_true[0]-rgb_pred[0]))','K.mean(K.abs(rgb_true[0]-rgb_pred[0]))','K.std(K.abs(rgb_true[0]-rgb_pred[0]))']\n",
        "\n",
        "def metrics_fft_abs_diff_magn_max(r_magn,magn):\n",
        "\tdef fft_abs_diff_magn_max(y_true,y_pred):\n",
        "\t\treturn K.max(K.abs(r_magn-magn))\n",
        "\treturn fft_abs_diff_magn_max\n",
        "\n",
        "def metrics_fft_abs_diff_angle_max(r_angle,angle):\n",
        "\tdef fft_abs_diff_angle_max(y_true,y_pred):\n",
        "\t\treturn K.max(K.abs(r_angle-angle))\n",
        "\treturn fft_abs_diff_angle_max\n",
        "\n",
        "def metrics_fft_abs_diff_magn_mean(r_magn,magn):\n",
        "\tdef fft_abs_diff_magn_mean(y_true,y_pred):\n",
        "\t\treturn K.mean(K.abs(r_magn-magn))\n",
        "\treturn fft_abs_diff_magn_mean\n",
        "\n",
        "def metrics_fft_abs_diff_angle_mean(r_angle,angle):\n",
        "\tdef fft_abs_diff_angle_mean(y_true,y_pred):\n",
        "\t\treturn K.mean(K.abs(r_angle-angle))\n",
        "\treturn fft_abs_diff_angle_mean\n",
        "\n",
        "def metrics_fft_abs_diff_magn_std(r_magn,magn):\n",
        "\tdef fft_abs_diff_magn_std(y_true,y_pred):\n",
        "\t\treturn K.std(K.abs(r_magn-magn))\n",
        "\treturn fft_abs_diff_magn_std\n",
        "\n",
        "def metrics_fft_abs_diff_angle_std(r_angle,angle):\n",
        "\tdef fft_abs_diff_angle_std(y_true,y_pred):\n",
        "\t\treturn K.std(K.abs(r_angle-angle))\n",
        "\treturn fft_abs_diff_angle_std\n",
        "\n",
        "# [metrics_fft_abs_diff_magn_max(r_magn,magn),metrics_fft_abs_diff_angle_max(r_angle,angle),metrics_fft_abs_diff_magn_mean(r_magn,magn),metrics_fft_abs_diff_angle_mean(r_angle,angle),metrics_fft_abs_diff_magn_std(r_magn,magn),metrics_fft_abs_diff_angle_std(r_angle,angle)]\n",
        "# ['K.max(K.abs(r_magn-magn))','K.max(K.abs(r_angle-angle))','K.mean(K.abs(r_magn-magn))','K.mean(K.abs(r_angle-angle))','K.std(K.abs(r_magn-magn))','K.std(K.abs(r_angle-angle))']\n",
        "\n",
        "def metrics_probability_abs_diff_max():\n",
        "\tdef probability_abs_diff_max(y_true,y_pred):\n",
        "\t\treturn K.max(K.abs(y_true[1]-y_pred[1]))\n",
        "\treturn probability_abs_diff_max\n",
        "\n",
        "def metrics_probability_abs_diff_mean():\n",
        "\tdef probability_abs_diff_mean(y_true,y_pred):\n",
        "\t\treturn K.mean(K.abs(y_true[1]-y_pred[1]))\n",
        "\treturn probability_abs_diff_mean\n",
        "\n",
        "def metrics_probability_abs_diff_std():\n",
        "\tdef probability_abs_diff_std(y_true,y_pred):\n",
        "\t\treturn K.std(K.abs(y_true[1]-y_pred[1]))\n",
        "\treturn probability_abs_diff_std\n",
        "\n",
        "# [metrics_probability_abs_diff_max(),metrics_probability_abs_diff_mean(),metrics_probability_abs_diff_std()]\n",
        "# ['K.max(K.abs(rgb_true[1]-rgb_pred[1]))','K.mean(K.abs(rgb_true[1]-rgb_pred[1]))','K.std(K.abs(rgb_true[1]-rgb_pred[1]))']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oT-A1E7TROh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function():\n",
        "    def loss(y_true,y_pred):\n",
        "        return tensorflow.keras.losses.binary_crossentropy(y_true[1], y_pred[2])\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knjYDnSCSygd",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z37HYPW9FAE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_inpts_outpts(image,imageBruitee):\n",
        "    \n",
        "    img_gen_with_noise = np.concatenate((image,imageBruitee),axis=0)\n",
        "    img_gen_without_noise = np.concatenate((image,image),axis=0)\n",
        "    inpt_gen = [img_gen_with_noise,img_gen_without_noise]\n",
        "    outpt_gen = img_gen_without_noise\n",
        "    inpt_disc = img_gen_with_noise\n",
        "\n",
        "    outpt_disc = np.concatenate((np.ones(shape=img_gen_with_noise.shape[0]//2),np.zeros(shape=img_gen_with_noise.shape[0]//2)),axis=0)\n",
        "    outpt_gan_disc = np.zeros(shape=img_gen_with_noise.shape[0])\n",
        "    return inpt_disc,outpt_disc,inpt_gen,outpt_gan_disc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVL2rt_XSyDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_progressive_model(index_model,max_step=200, max_it=2000):\n",
        "    train_data,validation_data,test_data = create_datasets()\n",
        "    max_total_it = (max_step-1)*3*max_it\n",
        "    tot_iteration = 0\n",
        "    base_name = \"Pretrain_fft_gan_%d\"%index_model\n",
        "    for step in range(50,max_step):\n",
        "        graph_name = base_name\n",
        "        graph_name = graph_name + \"_%d\"%(step)\n",
        "        backup_path = graph_name + \".h5\"\n",
        "        model_gen,r_magn,r_angle, magn, angle,model_disc,model_gan = generate_models(name=graph_name)\n",
        "            \n",
        "        metrics_list_gen = []#metrics_fft_abs_diff_magn_max(r_magn,magn),metrics_fft_abs_diff_angle_max(r_angle,angle),metrics_fft_abs_diff_magn_mean(r_magn,magn),metrics_fft_abs_diff_angle_mean(r_angle,angle),metrics_fft_abs_diff_magn_std(r_magn,magn),metrics_fft_abs_diff_angle_std(r_angle,angle),metrics_rgb_abs_diff_max(),metrics_rgb_abs_diff_mean(),metrics_rgb_abs_diff_std()]\n",
        "        metrics_list_names_gen = ['MSE']#,'K.max(K.abs(r_magn-magn))','K.max(K.abs(r_angle-angle))','K.mean(K.abs(r_magn-magn))','K.mean(K.abs(r_angle-angle))','K.std(K.abs(r_magn-magn))','K.std(K.abs(r_angle-angle))','K.max(K.abs(rgb_true[0]-rgb_pred[0]))','K.mean(K.abs(rgb_true[0]-rgb_pred[0]))','K.std(K.abs(rgb_true[0]-rgb_pred[0]))']\n",
        "\n",
        "        metrics_list_disc = []#metrics_probability_abs_diff_max(),metrics_probability_abs_diff_mean(),metrics_probability_abs_diff_std()]\n",
        "        metrics_list_names_disc = ['Binary_crossentropy']#,'K.max(K.abs(rgb_true[1]-rgb_pred[1]))','K.mean(K.abs(rgb_true[1]-rgb_pred[1]))','K.std(K.abs(rgb_true[1]-rgb_pred[1]))']\n",
        "        \n",
        "        model_gen.compile(loss=\"MSE\",metrics=metrics_list_gen,optimizer=\"Adam\")\n",
        "        print(model_gen.metrics)\n",
        "        model_disc.compile(loss=\"binary_crossentropy\",metrics=metrics_list_disc,optimizer=\"Adam\")\n",
        "        model_gan.compile(loss=\"binary_crossentropy\",optimizer=\"Adam\")\n",
        "        search_restore_pt(index_model,model_gan,base_name=graph_name)\n",
        "        for iteration in range(max_it):\n",
        "            plage_val = [(1.-iteration/(max_it+1))/2,1.-iteration/(max_it+1)]\n",
        "            image,imageBruitee = next_batch_bruit_voile_2(5,train_data,256,np.float32,\n",
        "                                                            [plage_val,plage_val,plage_val],\n",
        "                                                            [iteration//100*100/max_it/2,iteration//100*100/max_it/2],[0,1])\n",
        "            \n",
        "            inpt_disc,outpt_disc,inpt_gen,outpt_gan_disc = generate_inpts_outpts(image,imageBruitee)\n",
        "            metrics_list_values_gan = model_gan.train_on_batch(inpt_gen,outpt_gan_disc)\n",
        "            loss_gan = metrics_list_values_gan\n",
        "            metrics_list_values_disc = model_disc.train_on_batch(inpt_disc,outpt_disc)\n",
        "            loss_disc = metrics_list_values_disc\n",
        "            backup_fft(num_model=index_model,model_gen=model_gen,model_disc=model_disc,image=image,imageBruitee=imageBruitee,graph_name=graph_name,step=step,iteration=iteration,metrics_list_names_gen=metrics_list_names_gen,metrics_list_names_disc=metrics_list_names_disc,model_state_backup_int=5)\n",
        "            \n",
        "            print(\"Etape d'entrainement %d \\t\\titeration %d \\tstatut %d \\tpourcent, \\terreur gan %.5e \\t et disc %.5e\"%(step, iteration,tot_iteration/max_total_it,loss_gan,loss_disc))\n",
        "            tot_iteration += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEfOWQUqCepb",
        "colab_type": "code",
        "outputId": "0636ae4a-4568-41da-b495-6cf57c562f58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        }
      },
      "source": [
        "train_progressive_model(index_model=11)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building 0_Image\n",
            "Building 1_Image_real\n",
            "WARNING:tensorflow:Model inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_11\" was not an Input tensor, it was generated by layer 1_Image_real.\n",
            "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
            "The tensor that caused the issue was: 1_Image_real_21:0\n",
            "[]\n",
            "No GAN training found ; Restoring from splited files\n",
            "Building 0_Image\n",
            "Building 1_Image_real\n",
            "Restoring layer 0_Image\n",
            "Restoring layer 1_Image_real\n",
            "[]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-131-11533b279cd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_progressive_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-130-5eba6d22fa09>\u001b[0m in \u001b[0;36mtrain_progressive_model\u001b[0;34m(index_model, max_step, max_it)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mmetrics_list_values_disc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_disc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpt_disc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutpt_disc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss_disc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics_list_values_disc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mbackup_fft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_gen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_disc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_disc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimageBruitee\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimageBruitee\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgraph_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics_list_names_gen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics_list_names_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics_list_names_disc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics_list_names_disc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_state_backup_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Etape d'entrainement %d \\t\\titeration %d \\tstatut %d \\tpourcent, \\terreur gan %.5e \\t et disc %.5e\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtot_iteration\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmax_total_it\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_gan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_disc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-122-8ec027f1b9c4>\u001b[0m in \u001b[0;36mbackup_fft\u001b[0;34m(num_model, model_gen, model_disc, image, imageBruitee, graph_name, step, iteration, metrics_list_names_gen, metrics_list_names_disc, model_state_backup_int)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmetrics_list_values_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmetrics_list_values_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimageBruitee\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmetrics_list_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_list_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_metrics_list_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics_list_names_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics_list_values_clean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics_list_values_clean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics_list_values_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics_list_values_noise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics_list_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics_list_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics_list_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics_list_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0minput_gen_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-122-8ec027f1b9c4>\u001b[0m in \u001b[0;36mformat_metrics\u001b[0;34m(current_metrics_list_names, metrics_list_values_clean, metrics_list_values_noise, metrics_list_names, metrics_list_values, identifiant)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmetrics_list_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mformat_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_metrics_list_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics_list_values_clean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics_list_values_noise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics_list_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics_list_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midentifiant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_list_values_clean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_metrics_list_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_list_values_noise\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_metrics_list_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"metrics_list_values_clean : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics_list_values_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"metrics_list_values_clean : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics_list_values_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float32' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g55rZPH2Onf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvA9Twa4ZEXb",
        "colab_type": "text"
      },
      "source": [
        "## Test fonction bruitage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDYKieWgAjCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.rand(1)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGNItN2GVxk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_step=200\n",
        "max_it=2000\n",
        "iteration = 100\n",
        "train_data,validation_data,test_data = create_datasets()\n",
        "# image,imageBruitee = next_batch_bruit_voile_2(5,train_data,256,np.float32,\n",
        "#                                                             [[0,1.-iteration/(max_it+1)],[0,1.-iteration/(max_it+1)],[0,1.-iteration/(max_it+1)]],\n",
        "#                                                             [iteration//100*100/max_it/2,iteration//100*100/max_it/2],[0,1])\n",
        "\n",
        "plage_val = [(1.-iteration/(max_it+1))/2,1.-iteration/(max_it+1)]\n",
        "print(\"Facteur voile entre %f et %f\"%(plage_val[0],plage_val[1]))\n",
        "image,imageBruitee = next_batch_bruit_voile_2(5,train_data,256,np.float32,\n",
        "                                                            [plage_val,plage_val,plage_val],\n",
        "                                                            [0.01,0.01],[0,1])\n",
        "plt.figure(figsize=(30,15))\n",
        "it = 1\n",
        "for statut in range(2):\n",
        "    for img in range(5):\n",
        "        plt.subplot(2,5,it)\n",
        "        plt.imshow(image[img,:,:,:] if statut == 0 else imageBruitee[img,:,:,:])\n",
        "        it += 1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}