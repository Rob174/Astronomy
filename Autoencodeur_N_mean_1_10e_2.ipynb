{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencodeur_N_mean_1_10e-2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rob174/Astronomy/blob/Astronomy/Autoencodeur_N_mean_1_10e_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "VR_MLSTmPtFX",
        "colab_type": "code",
        "outputId": "fc7fd577-fb76-4df2-8e87-aa66fef24c4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a2h-mQi7Pyfy",
        "colab_type": "code",
        "outputId": "9159320c-a4a6-4908-9cd6-59f66ce06c3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/My Drive/TIPE'\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/TIPE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KAK6WTwgP2rW",
        "colab_type": "code",
        "outputId": "13d324ea-a287-486d-8354-48d8b56b5efb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Vérification GPU\")\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "print()\n",
        "print()\n",
        "from google.colab import files\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy\n",
        "from PIL import Image\n",
        "\n",
        "# Ajoute images\n",
        "entrainement = 60/100\n",
        "test = 20/100\n",
        "# validation = le reste : 20%\n",
        "tailleImage = 399\n",
        "\n",
        "images = []\n",
        "noises = []\n",
        "\n",
        "i = 1\n",
        "while os.path.isfile(\"Images_source/clean/\"+str(i)+\".jpg\"):\n",
        "    images.append(\"Images_source/clean/\"+str(i)+\".jpg\")\n",
        "    i += 1\n",
        "\n",
        "i = 1\n",
        "while os.path.isfile(\"Images_source/noise/\"+str(i)+\".jpg\"):\n",
        "    noises.append(\"Images_source/noise/\"+str(i)+\".jpg\")\n",
        "    i += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vérification GPU\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 14685801412873095619\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 3882054694422592756\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            "]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ty58ld9kP7jr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "def next_batch(batch_size, noises, images,tailleAttendue):#ATTENTION : pr tenter d'améliorer l'apprentissage, on augmente la taille minimale d'image prise\n",
        "    '''\n",
        "    Return a total of `num` random samples and labels. \n",
        "    '''\n",
        "#     print(\"Size of {}\".format(len(noises)))\n",
        "    noiseTensor = []\n",
        "    imageTensor = []\n",
        "#     import matplotlib.pyplot as plt\n",
        "    for i in range(batch_size):\n",
        "      choix = np.random.randint(0,len(images))#choix aléatoire de l'image\n",
        "      image = cv2.imread(images[choix],1)#Ouvre en rgb l'image nettoyée\n",
        "      noise = cv2.imread(noises[choix],1)#Ouvre en rgb l'image bruitée\n",
        "      angle = np.random.randint(0,90)\n",
        "      taille = np.random.randint(int(tailleAttendue*2**0.5)*4, image.shape[0]+1)# Racine de deux pour pouvoir toujours récupérer la taille désirée\n",
        "      ##ATENTION : ci-dessus peut porter à erreur si dépasse image.shape[0]+1\n",
        "      resizedImage = cv2.resize(image,(taille,taille))\n",
        "      resizedNoise = cv2.resize(noise,(taille,taille))\n",
        "      \n",
        "      rows,cols = resizedImage.shape[:2]\n",
        "#       print('CentreRow : ')\n",
        "#       print((taille//2,rows-taille//2))\n",
        "#       print('CentreCols : ')\n",
        "#       print((taille//2,cols-taille//2))\n",
        "#       tmp = taille//2\n",
        "      \n",
        "      centre = (np.random.randint(taille//2,rows-taille//2) if taille//2<rows-taille//2 else taille//2,np.random.randint(taille//2,cols-taille//2) if taille//2<cols-taille//2 else taille//2)\n",
        "      M = cv2.getRotationMatrix2D(centre,angle,1)\n",
        "\n",
        "      rotatedImage = cv2.warpAffine(resizedImage,M,(cols,rows))\n",
        "      rotatedNoise = cv2.warpAffine(resizedNoise,M,(cols,rows))\n",
        "#       print('Rotation : '+str(rotatedImage.shape))\n",
        "\n",
        "      resultImage = rotatedImage[centre[0]-tailleAttendue//2:centre[0]+tailleAttendue//2+1,centre[1]-tailleAttendue//2:centre[1]+tailleAttendue//2+1]\n",
        "      resultNoise = rotatedNoise[centre[0]-tailleAttendue//2:centre[0]+tailleAttendue//2+1,centre[1]-tailleAttendue//2:centre[1]+tailleAttendue//2+1]\n",
        "#       print('Crop : '+str(resultImage.shape))\n",
        "#       plt.imshow(resultImage)\n",
        "\n",
        "      noiseTensor.append(np.array(resultNoise,np.uint8))\n",
        "      imageTensor.append(np.array(resultImage,np.uint8))\n",
        "#       print(imageTensor)\n",
        "#     plt.show()\n",
        "    noiseTensor,imageTensor = np.array(noiseTensor,np.float32), np.array(imageTensor,np.float32)\n",
        "    return noiseTensor, imageTensor\n",
        "# next_batch(3,noises,images,200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UdNQxG7OQzz9",
        "colab_type": "code",
        "outputId": "a3685ffe-6b93-4511-eb14-865f8d38cd61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  var = next_batch(3,noises,images,200)\n",
        "  print(np.min(var),np.max(var))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 255.0\n",
            "0.0 255.0\n",
            "0.0 255.0\n",
            "0.0 253.0\n",
            "0.0 255.0\n",
            "0.0 255.0\n",
            "0.0 255.0\n",
            "0.0 255.0\n",
            "0.0 255.0\n",
            "0.0 255.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "B_tBbBzTTYJi",
        "outputId": "3cf04d6a-2dbc-49ec-b087-11c25838438c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3469
        }
      },
      "cell_type": "code",
      "source": [
        "height = 199\n",
        "width = 199\n",
        "channels = 3\n",
        "forceInit = True\n",
        "\n",
        "batch_size = 7\n",
        "learning_rate_power = 2\n",
        "learning_rate = 10**-learning_rate_power\n",
        "i = 1\n",
        "nom = 'Autodencodeur_N_'+'mean'+'_'+str(i)+'_10e-'+str(learning_rate_power)\n",
        "\n",
        "tf.reset_default_graph()\n",
        "import traceback\n",
        "\n",
        "root_logdir = \"tf_logs\"\n",
        "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
        "with tf.Graph().as_default():\n",
        "  \n",
        "    def denseLayer(input,nbNeurones,nom,start=False,end=False):\n",
        "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
        "      print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
        "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
        "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
        "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
        "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
        "      return couche,kernel,bias,kernel_saver,bias_saver\n",
        "    \n",
        "    def lrelu(x, leak=0.2): \n",
        "      return tf.maximum(x, leak * x)\n",
        "    \n",
        "    def convLayer(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start=False,end=False,generator=False):\n",
        "      def conv(inpt,kernel,num_filters,poolingSize,poolingType,nom,start=False,end=False,pooling=True):\n",
        "        couche = tf.layers.conv2d(inpt, filters=num_filters, kernel_size=kernel,\n",
        "             strides=1, padding='SAME',\n",
        "             activation=tf.nn.relu, name=nom)\n",
        "        print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
        "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
        "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
        "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
        "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
        "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
        "        if pooling == True:\n",
        "          if poolingType == 'mean':\n",
        "            couche = tf.layers.average_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
        "          else:\n",
        "            couche = tf.layers.max_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
        "        return couche,kernel,bias,kernel_saver,bias_saver\n",
        "      if type(kernelListOrNot) == list:\n",
        "        couche,kernel,bias,kernel_saver,bias_saver = conv(inpt,kernelListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',start,end,pooling=False)\n",
        "        kernelList,biasList,kernel_saverList,bias_saverList = [kernel],[bias],[kernel_saver],[bias_saver]\n",
        "        for i in range(1,len(kernelListOrNot)):\n",
        "          couche,kernel,bias,kernel_saver,bias_saver = conv(couche,kernelListOrNot[i],num_filters,poolingSize,poolingType,nom+'_split_'+str(i),False,False if i != len(kernelListOrNot)-1 else end,pooling=False if i != len(kernelListOrNot)-1 else True)\n",
        "          kernelList.append(kernel)\n",
        "          biasList.append(bias)\n",
        "          kernel_saverList.append(kernel_saver)\n",
        "          bias_saverList.append(bias_saver)\n",
        "        return couche,kernelList,biasList,kernel_saverList,bias_saverList\n",
        "      else: \n",
        "        return conv(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start,end)\n",
        "      \n",
        "    def deconvLayer(inpt,sizeListOrNot,num_filters,nom,start=False,end=False):\n",
        "      \"\"\"Ceci est une documentation\"\"\"\n",
        "      def deconv(inpt,size,num_filters,nom,start=False,end=False):\n",
        "        def calculateParameters(precDimension,outputSize, strides): # Ref : https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/\n",
        "#           print(precDimension)\n",
        "          return outputSize-(precDimension-1)*strides\n",
        "        kernelSize = calculateParameters(inpt.get_shape()[1],size,1)\n",
        "        print('Taille du noyau de deconvolution de '+nom+' : ' +str(kernelSize))\n",
        "        couche = tf.layers.conv2d_transpose(inpt, filters=num_filters, kernel_size=(kernelSize,kernelSize),\n",
        "             strides=1, padding='VALID',\n",
        "             activation=tf.nn.relu, name=nom)\n",
        "        print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
        "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
        "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
        "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
        "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
        "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
        "        \n",
        "        return couche,kernel,bias,kernel_saver,bias_saver\n",
        "      if type(sizeListOrNot) == list:\n",
        "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(inpt,sizeListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',True,False)\n",
        "        kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList = [kernel],[bias],[kernel_saver],[bias_saver],[sortieImage]\n",
        "        for i in range(1,len(sizeListOrNot)):\n",
        "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(couche,sizeListOrNot[i],num_filters,nom+'_split_'+str(i),False,False if i != len(sizeListOrNot)-1 else True)\n",
        "          kernelList.append(kernel)\n",
        "          biasList.append(bias)\n",
        "          kernel_saverList.append(kernel_saver)\n",
        "          bias_saverList.append(bias_saver)\n",
        "        return couche,kernelList,biasList,kernel_saverList,bias_saverList\n",
        "      else: \n",
        "        return deconv(inpt,sizeListOrNot,num_filters,nom,start,end)\n",
        "    def generateur(inpt,kernels,num_filters,pooling,poolingTypes):\n",
        "      \"\"\"\n",
        "      Description générale : Fonction créant le générateur, encodeur d'informations de l'image\n",
        "      Entree : \n",
        "        inpt, couche d'entrée, ici ce sera l'image bruitée\n",
        "        kernels, les différents noyaux de convolution, se présente soit sous forme de \n",
        "                liste simple soit sous forme d'une double liste. \n",
        "                Chaque sous liste représente un noyau de convolution \n",
        "                trop gros pour être réalisé en une couche et qui est donc séparé en sous-couches\n",
        "        num_filters, la taille de filtre pour la couche (en gardant le même nombre de filtre\n",
        "                pour chaque sous-couche si on a séparé la couche en sous-couches)\n",
        "        poolingSize, les tailles des noyaux des couche de pooling\n",
        "        poolingType, le type de pooling utilisé : SAME en générale\n",
        "      Sortie : \n",
        "        L'image encodée (même si ce n'est plus réellement une image maintenant)\n",
        "      \"\"\"\n",
        "      kernelBiasList,saver= [],[]\n",
        "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,kernelBiasList,saver):\n",
        "        if type(kernel) == list:\n",
        "          for k in kernel:\n",
        "            kernelBiasList.append(k)\n",
        "          for b in bias:\n",
        "            kernelBiasList.append(b)\n",
        "          for ks in kernel_saver:\n",
        "            saver.append(ks)\n",
        "          for bs in bias_saver:\n",
        "            saver.append(bs)\n",
        "        else:\n",
        "          kernelBiasList.append(kernel)\n",
        "          kernelBiasList.append(bias)\n",
        "          saver.append(kernel_saver)\n",
        "          saver.append(bias_saver)\n",
        "        return None #Utilise la mutabilité des listes\n",
        "      couche,kernel,bias,kernel_saver,bias_saver = convLayer(inpt,kernels[0],num_filters[0],pooling[0],poolingTypes[0],'convGenerator0',start=True)\n",
        "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,kernelBiasList,saver)\n",
        "      for i in range(1,len(kernels)-1):\n",
        "        couche,kernel,bias,kernel_saver,bias_saver = convLayer(couche,kernels[i],num_filters[i],pooling[i],poolingTypes[i],'convGenerator'+str(i))\n",
        "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,kernelBiasList,saver)\n",
        "\n",
        "      couche,kernel,bias,kernel_saver,bias_saver = convLayer(couche,kernels[-1],num_filters[-1],pooling[-1],poolingTypes[-1],'convGenerator'+str(len(kernels)-1),end=True)\n",
        "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,kernelBiasList,saver)\n",
        "      return couche,kernelBiasList,saver\n",
        "\n",
        "    def discriminator(inpt,size,num_filters,ID):\n",
        "      \"\"\"\n",
        "      Entrees : \n",
        "        inpt, couche d'entrée du discriminateur\n",
        "        size, liste, ou liste de liste si décomposition les tailles de couche successives pour revenir à la taille initiale\n",
        "        num_filters, le nombre de filtre par couches\n",
        "        ID, id unique pour séparer deux potentiels discriminateurs\n",
        "      Sortie :\n",
        "        Couche traitée par le discriminateur\n",
        "      \"\"\"\n",
        "      kernelBiasList,saver= [],[]\n",
        "      print(\"Taille de l'entrée du discriminateur : \",end='')\n",
        "      print(inpt.get_shape())\n",
        "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,kernelBiasList,saver):\n",
        "        if type(kernel) == list:\n",
        "          for k in kernel:\n",
        "            kernelBiasList.append(k)\n",
        "          for b in bias:\n",
        "            kernelBiasList.append(b)\n",
        "          for ks in kernel_saver:\n",
        "            saver.append(ks)\n",
        "          for bs in bias_saver:\n",
        "            saver.append(bs)\n",
        "        else:\n",
        "          kernelBiasList.append(kernel)\n",
        "          kernelBiasList.append(bias)\n",
        "          saver.append(kernel_saver)\n",
        "          saver.append(bias_saver)\n",
        "        return None #Utilise la mutabilité des listes\n",
        "      couche,kernel,bias,kernel_saver,bias_saver = deconvLayer(inpt,size[0],num_filters[0],'deconvDiscriminator'+ID+'0',start=True)\n",
        "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,kernelBiasList,saver)\n",
        "      for i in range(1,len(size)-1):\n",
        "        couche,kernel,bias,kernel_saver,bias_saver = deconvLayer(couche,size[i],num_filters[i],'deconvDiscriminator'+ID+str(i))\n",
        "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,kernelBiasList,saver)\n",
        "\n",
        "      couche,kernel,bias,kernel_saver,bias_saver = deconvLayer(couche,size[-1],num_filters[-1],'deconvDiscriminator'+ID+str(len(size)-1),end=True)\n",
        "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,kernelBiasList,saver)\n",
        "      return couche,kernelBiasList,saver\n",
        "    def discriminatorDense(inpt,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
        "        saver = []\n",
        "        kernelBiasList = []\n",
        "        sortiesImages = []\n",
        "        couche = inpt\n",
        "        for i,nb in enumerate(neurones):\n",
        "          couche,kernel,bias,kernel_saver,bias_saver = denseLayer(couche,nb,'dense'+str(ID)+'_'+str(i+1))\n",
        "          kernelBiasList.append(kernel)\n",
        "          kernelBiasList.append(bias)\n",
        "          saver.append(kernel_saver)\n",
        "          saver.append(bias_saver)\n",
        "        return couche,kernelBiasList,saver\n",
        "      \n",
        "    gen_input = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"input_noise\")\n",
        "    disc_input = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"input_clean\")\n",
        "    training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "    print('Taille de gen_input : '+str(gen_input.get_shape()))\n",
        "    \n",
        "    import numpy as np\n",
        "    \n",
        "    generator,gen_vars,gen_saver = generateur(gen_input,[[2]+[3],[3],[2]+[3]],[50,100,150],[4,4,2],['mean','mean','mean'])\n",
        "    couche,kernelBiasList,saver = discriminatorDense(generator,[150,300,150],'Lien')\n",
        "    nbCouchesDsicriminateur = 15\n",
        "    disc_faux,disc_vars,disc_saver = discriminator(generator,np.linspace(int(generator.get_shape()[1]),199,nbCouchesDsicriminateur+1,dtype=np.int)[1:],np.linspace(int(generator.get_shape()[-1]),3,nbCouchesDsicriminateur,dtype=np.int),'Faux')\n",
        "    \n",
        "    imageEntree = tf.summary.image(\"Image_entree\",tf.cast(gen_input,dtype=tf.uint8))\n",
        "    imageSortie = tf.summary.image(\"Image_sortie\",tf.cast(disc_faux,dtype=tf.uint8))\n",
        "    \n",
        "    print('Taille de sortie disc_faux : ',end='')\n",
        "    print(disc_faux.get_shape())\n",
        "    print('Taille de sortie disc_input : ',end='')\n",
        "    print(disc_input.get_shape())\n",
        "    \n",
        "    difference = disc_faux-gen_input\n",
        "    disc_loss = tf.reduce_mean(difference)\n",
        "    \n",
        "    maximumGradient = tf.reduce_max(difference)\n",
        "    minimumGradient = tf.reduce_min(difference)\n",
        "    meanGradient = tf.reduce_mean(difference)\n",
        "    maximumSortie = tf.reduce_max(disc_faux)\n",
        "    minimumSortie = tf.reduce_min(disc_faux)\n",
        "    meanSortie = tf.reduce_mean(disc_faux)\n",
        "  \n",
        "    loss_saver_disc = tf.summary.scalar(\"Loss\",disc_loss)\n",
        "    maximumGradientSaver = tf.summary.scalar(\"Maximum_difference\",maximumGradient)\n",
        "    minimumGradientSaver = tf.summary.scalar(\"Minimum_difference\",minimumGradient)\n",
        "    meanGradientSaver = tf.summary.scalar(\"Moyenne_difference\",meanGradient)\n",
        "    maximumSortieSaver = tf.summary.scalar(\"Maximum_sortie\",maximumSortie)\n",
        "    minimumSortieSaver = tf.summary.scalar(\"Minimum_sortie\",minimumSortie)\n",
        "    meanSortieSaver = tf.summary.scalar(\"Moyenne_sortie\",meanSortie)\n",
        "    \n",
        "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
        "    \n",
        "    saver =  disc_saver\n",
        "    print(\"Trainable variables : \"+str(tf.trainable_variables()))\n",
        "    train_disc = optimizer_disc.minimize(disc_loss)\n",
        "    init = tf.global_variables_initializer()\n",
        "    \n",
        "    tf_saver = tf.train.Saver()\n",
        "    summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
        "\n",
        "    import os\n",
        "    with tf.Session() as sess:\n",
        "        tf.global_variables_initializer().run()\n",
        "        if os.path.isdir('model/') == True and forceInit == False:\n",
        "          print(os.listdir('./model'))\n",
        "          tf_saver.restore(sess, 'model/model'+nom+'.ckpt')\n",
        "        else :\n",
        "          print('No previous training found...')\n",
        "          init.run()\n",
        "        print('Entrainement....')\n",
        "        for i in range(0,51):\n",
        "          gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)#Problèmes si gen_input\n",
        "          _,dl = sess.run([train_disc,disc_loss], feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: True})\n",
        "          summary_str = loss_saver_disc.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
        "          summary_writer.add_summary(summary_str,i)\n",
        "        \n",
        "          maximumGradientValue = maximumGradient.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
        "          minimumGradientValue = minimumGradient.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
        "          meanGradientValue = meanGradient.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
        "          maximumSortieValue = maximumSortie.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
        "          minimumSortieValue = minimumSortie.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
        "          meanSortieValue = meanSortie.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
        "          summary_str = maximumGradientSaver.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
        "          summary_writer.add_summary(summary_str,i)\n",
        "          summary_str = minimumGradientSaver.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
        "          summary_writer.add_summary(summary_str,i)\n",
        "          summary_str = meanGradientSaver.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
        "          summary_writer.add_summary(summary_str,i)\n",
        "          summary_str = maximumSortieSaver.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
        "          summary_writer.add_summary(summary_str,i)\n",
        "          summary_str = minimumSortieSaver.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
        "          summary_writer.add_summary(summary_str,i)\n",
        "          summary_str = meanSortieSaver.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
        "          summary_writer.add_summary(summary_str,i)\n",
        "          \n",
        "          print('Step %i: Discriminator Loss: %f' % (i, dl))\n",
        "          print('Différence : max %f, min %f, mean %f' % (maximumGradientValue, minimumGradientValue,meanGradientValue))\n",
        "          print('Sortie : max %f, min %f, mean %f' % (maximumSortieValue, minimumSortieValue,meanSortieValue))\n",
        "          if i == 50:\n",
        "            summary_str = imageEntree.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
        "            summary_writer.add_summary(summary_str,i)\n",
        "            summary_str = imageSortie.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
        "            summary_writer.add_summary(summary_str,i)\n",
        "        tf_saver.save(sess, 'model/model'+nom+'.ckpt')\n",
        "summary_writer.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Taille de gen_input : (7, 199, 199, 3)\n",
            "Taille de convGenerator0_split_0 : (7, 199, 199, 50)\n",
            "Taille de convGenerator0_split_1 : (7, 199, 199, 50)\n",
            "Taille de convGenerator1_split_0 : (7, 100, 100, 100)\n",
            "Taille de convGenerator2_split_0 : (7, 100, 100, 150)\n",
            "Taille de convGenerator2_split_1 : (7, 100, 100, 150)\n",
            "Taille de denseLien_1 : (7, 50, 50, 150)\n",
            "Taille de denseLien_2 : (7, 50, 50, 300)\n",
            "Taille de denseLien_3 : (7, 50, 50, 150)\n",
            "Taille de l'entrée du discriminateur : (7, 50, 50, 150)\n",
            "Taille du noyau de deconvolution de deconvDiscriminatorFaux0 : 10\n",
            "Taille de deconvDiscriminatorFaux0 : (7, 59, 59, 150)\n",
            "Taille du noyau de deconvolution de deconvDiscriminatorFaux1 : 11\n",
            "Taille de deconvDiscriminatorFaux1 : (7, 69, 69, 139)\n",
            "Taille du noyau de deconvolution de deconvDiscriminatorFaux2 : 11\n",
            "Taille de deconvDiscriminatorFaux2 : (7, 79, 79, 129)\n",
            "Taille du noyau de deconvolution de deconvDiscriminatorFaux3 : 11\n",
            "Taille de deconvDiscriminatorFaux3 : (7, 89, 89, 118)\n",
            "Taille du noyau de deconvolution de deconvDiscriminatorFaux4 : 11\n",
            "Taille de deconvDiscriminatorFaux4 : (7, 99, 99, 108)\n",
            "Taille du noyau de deconvolution de deconvDiscriminatorFaux5 : 11\n",
            "Taille de deconvDiscriminatorFaux5 : (7, 109, 109, 97)\n",
            "Taille du noyau de deconvolution de deconvDiscriminatorFaux6 : 11\n",
            "Taille de deconvDiscriminatorFaux6 : (7, 119, 119, 87)\n",
            "Taille du noyau de deconvolution de deconvDiscriminatorFaux7 : 11\n",
            "Taille de deconvDiscriminatorFaux7 : (7, 129, 129, 76)\n",
            "Taille du noyau de deconvolution de deconvDiscriminatorFaux8 : 11\n",
            "Taille de deconvDiscriminatorFaux8 : (7, 139, 139, 66)\n",
            "Taille du noyau de deconvolution de deconvDiscriminatorFaux9 : 11\n",
            "Taille de deconvDiscriminatorFaux9 : (7, 149, 149, 55)\n",
            "Taille du noyau de deconvolution de deconvDiscriminatorFaux10 : 11\n",
            "Taille de deconvDiscriminatorFaux10 : (7, 159, 159, 45)\n",
            "Taille du noyau de deconvolution de deconvDiscriminatorFaux11 : 11\n",
            "Taille de deconvDiscriminatorFaux11 : (7, 169, 169, 34)\n",
            "Taille du noyau de deconvolution de deconvDiscriminatorFaux12 : 11\n",
            "Taille de deconvDiscriminatorFaux12 : (7, 179, 179, 24)\n",
            "Taille du noyau de deconvolution de deconvDiscriminatorFaux13 : 11\n",
            "Taille de deconvDiscriminatorFaux13 : (7, 189, 189, 13)\n",
            "Taille du noyau de deconvolution de deconvDiscriminatorFaux14 : 11\n",
            "Taille de deconvDiscriminatorFaux14 : (7, 199, 199, 3)\n",
            "Taille de sortie disc_faux : (7, 199, 199, 3)\n",
            "Taille de sortie disc_input : (7, 199, 199, 3)\n",
            "Trainable variables : [<tf.Variable 'convGenerator0_split_0/kernel:0' shape=(2, 2, 3, 50) dtype=float32_ref>, <tf.Variable 'convGenerator0_split_0/bias:0' shape=(50,) dtype=float32_ref>, <tf.Variable 'convGenerator0_split_0_normalization/beta:0' shape=(50,) dtype=float32_ref>, <tf.Variable 'convGenerator0_split_1/kernel:0' shape=(3, 3, 50, 50) dtype=float32_ref>, <tf.Variable 'convGenerator0_split_1/bias:0' shape=(50,) dtype=float32_ref>, <tf.Variable 'convGenerator0_split_1_normalization/beta:0' shape=(50,) dtype=float32_ref>, <tf.Variable 'convGenerator1_split_0/kernel:0' shape=(3, 3, 50, 100) dtype=float32_ref>, <tf.Variable 'convGenerator1_split_0/bias:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'convGenerator1_split_0_normalization/beta:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'convGenerator2_split_0/kernel:0' shape=(2, 2, 100, 150) dtype=float32_ref>, <tf.Variable 'convGenerator2_split_0/bias:0' shape=(150,) dtype=float32_ref>, <tf.Variable 'convGenerator2_split_0_normalization/beta:0' shape=(150,) dtype=float32_ref>, <tf.Variable 'convGenerator2_split_1/kernel:0' shape=(3, 3, 150, 150) dtype=float32_ref>, <tf.Variable 'convGenerator2_split_1/bias:0' shape=(150,) dtype=float32_ref>, <tf.Variable 'convGenerator2_split_1_normalization/beta:0' shape=(150,) dtype=float32_ref>, <tf.Variable 'denseLien_1/kernel:0' shape=(150, 150) dtype=float32_ref>, <tf.Variable 'denseLien_1/bias:0' shape=(150,) dtype=float32_ref>, <tf.Variable 'denseLien_2/kernel:0' shape=(150, 300) dtype=float32_ref>, <tf.Variable 'denseLien_2/bias:0' shape=(300,) dtype=float32_ref>, <tf.Variable 'denseLien_3/kernel:0' shape=(300, 150) dtype=float32_ref>, <tf.Variable 'denseLien_3/bias:0' shape=(150,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux0/kernel:0' shape=(10, 10, 150, 150) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux0/bias:0' shape=(150,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux0_normalization/beta:0' shape=(150,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux1/kernel:0' shape=(11, 11, 139, 150) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux1/bias:0' shape=(139,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux1_normalization/beta:0' shape=(139,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux2/kernel:0' shape=(11, 11, 129, 139) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux2/bias:0' shape=(129,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux2_normalization/beta:0' shape=(129,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux3/kernel:0' shape=(11, 11, 118, 129) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux3/bias:0' shape=(118,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux3_normalization/beta:0' shape=(118,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux4/kernel:0' shape=(11, 11, 108, 118) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux4/bias:0' shape=(108,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux4_normalization/beta:0' shape=(108,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux5/kernel:0' shape=(11, 11, 97, 108) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux5/bias:0' shape=(97,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux5_normalization/beta:0' shape=(97,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux6/kernel:0' shape=(11, 11, 87, 97) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux6/bias:0' shape=(87,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux6_normalization/beta:0' shape=(87,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux7/kernel:0' shape=(11, 11, 76, 87) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux7/bias:0' shape=(76,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux7_normalization/beta:0' shape=(76,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux8/kernel:0' shape=(11, 11, 66, 76) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux8/bias:0' shape=(66,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux8_normalization/beta:0' shape=(66,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux9/kernel:0' shape=(11, 11, 55, 66) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux9/bias:0' shape=(55,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux9_normalization/beta:0' shape=(55,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux10/kernel:0' shape=(11, 11, 45, 55) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux10/bias:0' shape=(45,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux10_normalization/beta:0' shape=(45,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux11/kernel:0' shape=(11, 11, 34, 45) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux11/bias:0' shape=(34,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux11_normalization/beta:0' shape=(34,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux12/kernel:0' shape=(11, 11, 24, 34) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux12/bias:0' shape=(24,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux12_normalization/beta:0' shape=(24,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux13/kernel:0' shape=(11, 11, 13, 24) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux13/bias:0' shape=(13,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux13_normalization/beta:0' shape=(13,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux14/kernel:0' shape=(11, 11, 3, 13) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux14/bias:0' shape=(3,) dtype=float32_ref>, <tf.Variable 'deconvDiscriminatorFaux14_normalization/beta:0' shape=(3,) dtype=float32_ref>]\n",
            "No previous training found...\n",
            "Entrainement....\n",
            "Step 0: Discriminator Loss: -50.095531\n",
            "Différence : max 1137387192012767232.000000, min -255.053619, mean 22740561194647552.000000\n",
            "Sortie : max 1137387192012767232.000000, min -0.053981, mean 22740561194647552.000000\n",
            "Step 1: Discriminator Loss: -100.112045\n",
            "Différence : max 613447494533120.000000, min -255.175293, mean 16215131029504.000000\n",
            "Sortie : max 613447494533120.000000, min -0.175286, mean 16215131029504.000000\n",
            "Step 2: Discriminator Loss: -89.976006\n",
            "Différence : max 412804251648.000000, min -255.163025, mean 7924728320.000000\n",
            "Sortie : max 412804251648.000000, min -0.240825, mean 7924728320.000000\n",
            "Step 3: Discriminator Loss: -63.932610\n",
            "Différence : max 7587061760.000000, min -255.178192, mean 111175976.000000\n",
            "Sortie : max 7587061760.000000, min -0.246742, mean 111176040.000000\n",
            "Step 4: Discriminator Loss: -82.351479\n",
            "Différence : max 364623552.000000, min -255.250900, mean 7873664.500000\n",
            "Sortie : max 364623712.000000, min -0.250894, mean 7873735.000000\n",
            "Step 5: Discriminator Loss: -88.203827\n",
            "Différence : max 27828796.000000, min -248.271469, mean 930259.375000\n",
            "Sortie : max 27828964.000000, min -0.271470, mean 930347.687500\n",
            "Step 6: Discriminator Loss: -73.073273\n",
            "Différence : max 8806101.000000, min -255.209579, mean 209322.421875\n",
            "Sortie : max 8806264.000000, min -0.232882, mean 209395.359375\n",
            "Step 7: Discriminator Loss: -58.528725\n",
            "Différence : max 1001557.500000, min -248.222443, mean 15133.507812\n",
            "Sortie : max 1001726.500000, min -0.223012, mean 15191.976562\n",
            "Step 8: Discriminator Loss: -49.681381\n",
            "Différence : max 48788.199219, min -255.233643, mean 1666.306274\n",
            "Sortie : max 48903.199219, min -0.233648, mean 1715.907837\n",
            "Step 9: Discriminator Loss: -67.654510\n",
            "Différence : max 10737.281250, min -255.247391, mean 244.841187\n",
            "Sortie : max 10828.049805, min -0.247385, mean 312.405426\n",
            "Step 10: Discriminator Loss: -53.835754\n",
            "Différence : max 5235.228516, min -255.261292, mean 38.551327\n",
            "Sortie : max 5374.046875, min -0.266164, mean 92.287148\n",
            "Step 11: Discriminator Loss: -53.580379\n",
            "Différence : max 7657.242676, min -255.281631, mean 110.825195\n",
            "Sortie : max 7690.594238, min -0.281634, mean 164.295456\n",
            "Step 12: Discriminator Loss: -71.469276\n",
            "Différence : max 4271.125488, min -255.288010, mean -8.883039\n",
            "Sortie : max 4272.125488, min -0.288017, mean 62.466000\n",
            "Step 13: Discriminator Loss: -40.528374\n",
            "Différence : max 2625.919189, min -255.303665, mean -3.141481\n",
            "Sortie : max 2626.919189, min -0.303669, mean 37.256737\n",
            "Step 14: Discriminator Loss: -63.748413\n",
            "Différence : max 1164.755005, min -255.306992, mean -48.113056\n",
            "Sortie : max 1166.619141, min -0.306991, mean 15.495251\n",
            "Step 15: Discriminator Loss: -52.729073\n",
            "Différence : max 853.021179, min -255.305038, mean -40.294559\n",
            "Sortie : max 855.021179, min -0.305039, mean 12.284403\n",
            "Step 16: Discriminator Loss: -38.383862\n",
            "Différence : max 578.512756, min -255.304398, mean -30.305761\n",
            "Sortie : max 579.606323, min -0.304396, mean 7.918130\n",
            "Step 17: Discriminator Loss: -93.516907\n",
            "Différence : max 675.800354, min -255.312714, mean -83.142708\n",
            "Sortie : max 725.835205, min -0.312716, mean 10.203840\n",
            "Step 18: Discriminator Loss: -72.677917\n",
            "Différence : max 264.265350, min -255.318558, mean -69.076401\n",
            "Sortie : max 424.911987, min -0.318563, mean 3.421613\n",
            "Step 19: Discriminator Loss: -60.619411\n",
            "Différence : max 156.354996, min -255.322693, mean -59.521210\n",
            "Sortie : max 206.444946, min -0.322691, mean 0.908227\n",
            "Step 20: Discriminator Loss: -66.645096\n",
            "Différence : max 75.203857, min -255.327393, mean -65.307343\n",
            "Sortie : max 75.797371, min -0.327396, mean 1.137972\n",
            "Step 21: Discriminator Loss: -41.999855\n",
            "Différence : max 197.754929, min -255.330536, mean -37.639080\n",
            "Sortie : max 199.561935, min -0.334480, mean 4.150750\n",
            "Step 22: Discriminator Loss: -45.013420\n",
            "Différence : max 5181.161621, min -255.348648, mean -0.065195\n",
            "Sortie : max 5231.161621, min -0.348650, mean 44.728333\n",
            "Step 23: Discriminator Loss: -83.986107\n",
            "Différence : max 7014.821777, min -255.354248, mean -39.322884\n",
            "Sortie : max 7066.425781, min -0.354249, mean 44.433247\n",
            "Step 24: Discriminator Loss: -60.741501\n",
            "Différence : max 7668.753418, min -255.358383, mean 1.023594\n",
            "Sortie : max 7670.753418, min -0.358386, mean 61.524925\n",
            "Step 25: Discriminator Loss: -38.443443\n",
            "Différence : max 31519.324219, min -255.362823, mean 278.271484\n",
            "Sortie : max 31521.324219, min -0.362824, mean 316.465027\n",
            "Step 26: Discriminator Loss: -75.053421\n",
            "Différence : max 10380.967773, min -255.369827, mean 27.993422\n",
            "Sortie : max 10431.967773, min -0.369822, mean 102.786804\n",
            "Step 27: Discriminator Loss: -81.813431\n",
            "Différence : max 3415.181885, min -255.378052, mean -64.651665\n",
            "Sortie : max 3417.181885, min -0.378046, mean 16.891304\n",
            "Step 28: Discriminator Loss: -60.845966\n",
            "Différence : max 5972.699219, min -255.389313, mean -31.130619\n",
            "Sortie : max 6066.699219, min -0.389312, mean 29.435226\n",
            "Step 29: Discriminator Loss: -86.556160\n",
            "Différence : max 121.481720, min -255.398056, mean -86.285500\n",
            "Sortie : max 177.302185, min -0.398053, mean -0.018822\n",
            "Step 30: Discriminator Loss: -85.504807\n",
            "Différence : max 1.393754, min -255.406433, mean -85.578636\n",
            "Sortie : max 2.408988, min -0.406434, mean -0.373729\n",
            "Step 31: Discriminator Loss: -65.589813\n",
            "Différence : max 368.933655, min -255.412399, mean -63.686298\n",
            "Sortie : max 431.038177, min -0.412396, mean 1.593596\n",
            "Step 32: Discriminator Loss: -39.498539\n",
            "Différence : max 3395.423828, min -255.417847, mean -20.930906\n",
            "Sortie : max 3401.436768, min -0.417852, mean 18.247452\n",
            "Step 33: Discriminator Loss: -57.587036\n",
            "Différence : max 489.658508, min -255.423340, mean -55.572479\n",
            "Sortie : max 642.632874, min -0.423344, mean 1.684835\n",
            "Step 34: Discriminator Loss: -55.494778\n",
            "Différence : max 266.457123, min -255.429291, mean -53.464767\n",
            "Sortie : max 363.565582, min -0.429298, mean 1.690133\n",
            "Step 35: Discriminator Loss: -57.039776\n",
            "Différence : max 825.336792, min -255.435226, mean -49.556705\n",
            "Sortie : max 826.336792, min -0.435228, mean 7.132967\n",
            "Step 36: Discriminator Loss: -110.108467\n",
            "Différence : max 1228.064697, min -255.420410, mean -99.676460\n",
            "Sortie : max 1230.064697, min -0.444218, mean 10.072087\n",
            "Step 37: Discriminator Loss: -84.354156\n",
            "Différence : max 266.070709, min -255.462006, mean -81.243912\n",
            "Sortie : max 414.773010, min -0.462002, mean 2.740227\n",
            "Step 38: Discriminator Loss: -34.152412\n",
            "Différence : max 545.561523, min -250.437271, mean -32.550640\n",
            "Sortie : max 546.646057, min -0.472134, mean 1.221754\n",
            "Step 39: Discriminator Loss: -51.890656\n",
            "Différence : max 350.896027, min -255.482574, mean -50.999252\n",
            "Sortie : max 351.896027, min -0.482577, mean 0.501394\n",
            "Step 40: Discriminator Loss: -57.769810\n",
            "Différence : max 321.942139, min -255.498978, mean -55.205269\n",
            "Sortie : max 323.156799, min -0.498982, mean 2.165072\n",
            "Step 41: Discriminator Loss: -53.241093\n",
            "Différence : max 239.210938, min -244.510330, mean -52.645878\n",
            "Sortie : max 240.805542, min -0.510325, mean 0.185352\n",
            "Step 42: Discriminator Loss: -90.762978\n",
            "Différence : max -0.443613, min -255.521255, mean -90.773293\n",
            "Sortie : max 18.343588, min -0.521252, mean -0.430613\n",
            "Step 43: Discriminator Loss: -73.795090\n",
            "Différence : max 158.737625, min -255.531082, mean -73.469513\n",
            "Sortie : max 160.005768, min -0.531086, mean -0.104266\n",
            "Step 44: Discriminator Loss: -68.071838\n",
            "Différence : max -0.505094, min -255.545609, mean -68.140472\n",
            "Sortie : max 1.029211, min -0.545602, mean -0.508551\n",
            "Step 45: Discriminator Loss: -86.845833\n",
            "Différence : max -0.535077, min -255.559204, mean -86.914574\n",
            "Sortie : max -0.492690, min -0.559204, mean -0.518741\n",
            "Step 46: Discriminator Loss: -51.784145\n",
            "Différence : max 94.938889, min -246.501007, mean -51.633858\n",
            "Sortie : max 96.145370, min -0.568177, mean -0.309827\n",
            "Step 47: Discriminator Loss: -103.057404\n",
            "Différence : max -0.577098, min -255.578598, mean -103.123680\n",
            "Sortie : max -0.509421, min -0.578604, mean -0.536003\n",
            "Step 48: Discriminator Loss: -74.758530\n",
            "Différence : max -0.588763, min -255.588760, mean -74.823387\n",
            "Sortie : max -0.517911, min -0.588763, mean -0.544869\n",
            "Step 49: Discriminator Loss: -67.589127\n",
            "Différence : max -0.598566, min -254.598572, mean -67.652534\n",
            "Sortie : max 0.584902, min -0.598566, mean -0.553427\n",
            "Step 50: Discriminator Loss: -78.519241\n",
            "Différence : max 63.180473, min -255.609543, mean -78.309151\n",
            "Sortie : max 64.180473, min -0.609549, mean -0.290207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VNnxkBOsxrmY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# You can change the directory name\n",
        "LOG_DIR = 'tf_logs'\n",
        "\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "import os\n",
        "if not os.path.exists(LOG_DIR):\n",
        "  os.makedirs(LOG_DIR)\n",
        "  \n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6064 &'\n",
        "    .format(LOG_DIR))\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jgCvWaFc83Rm",
        "colab_type": "code",
        "outputId": "59790707-7947-4044-cb97-2f0cb949cea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1279
        }
      },
      "cell_type": "code",
      "source": [
        "height = 199\n",
        "width = 199\n",
        "channels = 3\n",
        "forceInit = False\n",
        "\n",
        "batch_size = 7\n",
        "learning_rate_power = 2\n",
        "learning_rate = 10**-learning_rate_power\n",
        "i = 1\n",
        "nom = 'Autodencodeur_N_'+'mean'+'_'+str(i)+'_10e-'+str(learning_rate_power)\n",
        "from tensorflow.python import debug as tf_debug\n",
        "\n",
        "tf.reset_default_graph()\n",
        "import traceback\n",
        "\n",
        "root_logdir = \"tf_logs\"\n",
        "logdir = \"{}/run-{}/\".format(root_logdir,nom)\n",
        "with tf.Graph().as_default():\n",
        "  \n",
        "    def denseLayer(input,nbNeurones,nom,start=False,end=False):\n",
        "      couche = tf.layers.dense(input,nbNeurones,name=nom)\n",
        "#       print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
        "      kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
        "      bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
        "      kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
        "      bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
        "      return couche,kernel,bias,kernel_saver,bias_saver\n",
        "    \n",
        "    def lrelu(x, leak=0.2): \n",
        "      return tf.maximum(x, leak * x)\n",
        "    \n",
        "    def convLayer(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start=False,end=False,generator=False):\n",
        "      def conv(inpt,kernel,num_filters,poolingSize,poolingType,nom,start=False,end=False,pooling=True):\n",
        "        couche = tf.layers.conv2d(inpt, filters=num_filters, kernel_size=kernel,\n",
        "             strides=1, padding='SAME',\n",
        "             activation=tf.nn.relu, name=nom)\n",
        "#         print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
        "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
        "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
        "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
        "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
        "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
        "        if pooling == True:\n",
        "          if poolingType == 'mean':\n",
        "            couche = tf.layers.average_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
        "          else:\n",
        "            couche = tf.layers.max_pooling2d(lrelu(couche), pool_size=(poolingSize,poolingSize), strides=(2,2), padding='same')\n",
        "        return couche,kernel,bias,kernel_saver,bias_saver\n",
        "      if type(kernelListOrNot) == list:\n",
        "        couche,kernel,bias,kernel_saver,bias_saver = conv(inpt,kernelListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',start,end,pooling=False)\n",
        "        kernelList,biasList,kernel_saverList,bias_saverList = [kernel],[bias],[kernel_saver],[bias_saver]\n",
        "        for i in range(1,len(kernelListOrNot)):\n",
        "          couche,kernel,bias,kernel_saver,bias_saver = conv(couche,kernelListOrNot[i],num_filters,poolingSize,poolingType,nom+'_split_'+str(i),False,False if i != len(kernelListOrNot)-1 else end,pooling=False if i != len(kernelListOrNot)-1 else True)\n",
        "          kernelList.append(kernel)\n",
        "          biasList.append(bias)\n",
        "          kernel_saverList.append(kernel_saver)\n",
        "          bias_saverList.append(bias_saver)\n",
        "        return couche,kernelList,biasList,kernel_saverList,bias_saverList\n",
        "      else: \n",
        "        return conv(inpt,kernelListOrNot,num_filters,poolingSize,poolingType,nom,start,end)\n",
        "      \n",
        "    def deconvLayer(inpt,sizeListOrNot,num_filters,nom,start=False,end=False):\n",
        "      \"\"\"Ceci est une documentation\"\"\"\n",
        "      def deconv(inpt,size,num_filters,nom,start=False,end=False):\n",
        "        def calculateParameters(precDimension,outputSize, strides): # Ref : https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/\n",
        "#           print(precDimension)\n",
        "          return outputSize-(precDimension-1)*strides\n",
        "        kernelSize = calculateParameters(inpt.get_shape()[1],size,1)\n",
        "#         print('Taille du noyau de deconvolution de '+nom+' : ' +str(kernelSize))\n",
        "        couche = tf.layers.conv2d_transpose(inpt, filters=num_filters, kernel_size=(kernelSize,kernelSize),\n",
        "             strides=1, padding='VALID',\n",
        "             activation=tf.nn.relu, name=nom)\n",
        "#         print('Taille de '+nom+' : '+str(couche.get_shape()))\n",
        "        kernel = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/kernel')[0]\n",
        "        bias = tf.get_collection(tf.GraphKeys.VARIABLES,  nom+'/bias')[0]\n",
        "        kernel_saver = tf.summary.histogram(\"Kernel/\"+nom,kernel)\n",
        "        bias_saver = tf.summary.histogram(\"Bias/\"+nom,bias)\n",
        "        couche = tf.contrib.layers.batch_norm(couche, is_training=training, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope=nom+'_normalization')\n",
        "        \n",
        "        return couche,kernel,bias,kernel_saver,bias_saver\n",
        "      if type(sizeListOrNot) == list:\n",
        "        couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(inpt,sizeListOrNot[0],num_filters,poolingSize,poolingType,nom+'_split_0',True,False)\n",
        "        kernelList,biasList,kernel_saverList,bias_saverList,sortieImageList = [kernel],[bias],[kernel_saver],[bias_saver],[sortieImage]\n",
        "        for i in range(1,len(sizeListOrNot)):\n",
        "          couche,kernel,bias,kernel_saver,bias_saver,sortieImage = deconv(couche,sizeListOrNot[i],num_filters,nom+'_split_'+str(i),False,False if i != len(sizeListOrNot)-1 else True)\n",
        "          kernelList.append(kernel)\n",
        "          biasList.append(bias)\n",
        "          kernel_saverList.append(kernel_saver)\n",
        "          bias_saverList.append(bias_saver)\n",
        "        return couche,kernelList,biasList,kernel_saverList,bias_saverList\n",
        "      else: \n",
        "        return deconv(inpt,sizeListOrNot,num_filters,nom,start,end)\n",
        "    def generateur(inpt,kernels,num_filters,pooling,poolingTypes):\n",
        "      \"\"\"\n",
        "      Description générale : Fonction créant le générateur, encodeur d'informations de l'image\n",
        "      Entree : \n",
        "        inpt, couche d'entrée, ici ce sera l'image bruitée\n",
        "        kernels, les différents noyaux de convolution, se présente soit sous forme de \n",
        "                liste simple soit sous forme d'une double liste. \n",
        "                Chaque sous liste représente un noyau de convolution \n",
        "                trop gros pour être réalisé en une couche et qui est donc séparé en sous-couches\n",
        "        num_filters, la taille de filtre pour la couche (en gardant le même nombre de filtre\n",
        "                pour chaque sous-couche si on a séparé la couche en sous-couches)\n",
        "        poolingSize, les tailles des noyaux des couche de pooling\n",
        "        poolingType, le type de pooling utilisé : SAME en générale\n",
        "      Sortie : \n",
        "        L'image encodée (même si ce n'est plus réellement une image maintenant)\n",
        "      \"\"\"\n",
        "      kernelBiasList,saver= [],[]\n",
        "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,kernelBiasList,saver):\n",
        "        if type(kernel) == list:\n",
        "          for k in kernel:\n",
        "            kernelBiasList.append(k)\n",
        "          for b in bias:\n",
        "            kernelBiasList.append(b)\n",
        "          for ks in kernel_saver:\n",
        "            saver.append(ks)\n",
        "          for bs in bias_saver:\n",
        "            saver.append(bs)\n",
        "        else:\n",
        "          kernelBiasList.append(kernel)\n",
        "          kernelBiasList.append(bias)\n",
        "          saver.append(kernel_saver)\n",
        "          saver.append(bias_saver)\n",
        "        return None #Utilise la mutabilité des listes\n",
        "      couche,kernel,bias,kernel_saver,bias_saver = convLayer(inpt,kernels[0],num_filters[0],pooling[0],poolingTypes[0],'convGenerator0',start=True)\n",
        "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,kernelBiasList,saver)\n",
        "      for i in range(1,len(kernels)-1):\n",
        "        couche,kernel,bias,kernel_saver,bias_saver = convLayer(couche,kernels[i],num_filters[i],pooling[i],poolingTypes[i],'convGenerator'+str(i))\n",
        "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,kernelBiasList,saver)\n",
        "\n",
        "      couche,kernel,bias,kernel_saver,bias_saver = convLayer(couche,kernels[-1],num_filters[-1],pooling[-1],poolingTypes[-1],'convGenerator'+str(len(kernels)-1),end=True)\n",
        "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,kernelBiasList,saver)\n",
        "      return couche,kernelBiasList,saver\n",
        "\n",
        "    def discriminator(inpt,size,num_filters,ID):\n",
        "      \"\"\"\n",
        "      Entrees : \n",
        "        inpt, couche d'entrée du discriminateur\n",
        "        size, liste, ou liste de liste si décomposition les tailles de couche successives pour revenir à la taille initiale\n",
        "        num_filters, le nombre de filtre par couches\n",
        "        ID, id unique pour séparer deux potentiels discriminateurs\n",
        "      Sortie :\n",
        "        Couche traitée par le discriminateur\n",
        "      \"\"\"\n",
        "      kernelBiasList,saver= [],[]\n",
        "      print(\"Taille de l'entrée du discriminateur : \",end='')\n",
        "      print(inpt.get_shape())\n",
        "      def recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,kernelBiasList,saver):\n",
        "        if type(kernel) == list:\n",
        "          for k in kernel:\n",
        "            kernelBiasList.append(k)\n",
        "          for b in bias:\n",
        "            kernelBiasList.append(b)\n",
        "          for ks in kernel_saver:\n",
        "            saver.append(ks)\n",
        "          for bs in bias_saver:\n",
        "            saver.append(bs)\n",
        "        else:\n",
        "          kernelBiasList.append(kernel)\n",
        "          kernelBiasList.append(bias)\n",
        "          saver.append(kernel_saver)\n",
        "          saver.append(bias_saver)\n",
        "        return None #Utilise la mutabilité des listes\n",
        "      couche,kernel,bias,kernel_saver,bias_saver = deconvLayer(inpt,size[0],num_filters[0],'deconvDiscriminator'+ID+'0',start=True)\n",
        "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,kernelBiasList,saver)\n",
        "      for i in range(1,len(size)-1):\n",
        "        couche,kernel,bias,kernel_saver,bias_saver = deconvLayer(couche,size[i],num_filters[i],'deconvDiscriminator'+ID+str(i))\n",
        "        recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,kernelBiasList,saver)\n",
        "\n",
        "      couche,kernel,bias,kernel_saver,bias_saver = deconvLayer(couche,size[-1],num_filters[-1],'deconvDiscriminator'+ID+str(len(size)-1),end=True)\n",
        "      recuperationDonneesCouches(kernel,bias,kernel_saver,bias_saver,kernelBiasList,saver)\n",
        "      return couche,kernelBiasList,saver\n",
        "    def discriminatorDense(inpt,neurones,ID):#ID car besoin nom unique pour chaque série de discriminator\n",
        "        saver = []\n",
        "        kernelBiasList = []\n",
        "        sortiesImages = []\n",
        "        couche = inpt\n",
        "        for i,nb in enumerate(neurones):\n",
        "          couche,kernel,bias,kernel_saver,bias_saver = denseLayer(couche,nb,'dense'+str(ID)+'_'+str(i+1))\n",
        "          kernelBiasList.append(kernel)\n",
        "          kernelBiasList.append(bias)\n",
        "          saver.append(kernel_saver)\n",
        "          saver.append(bias_saver)\n",
        "        return couche,kernelBiasList,saver\n",
        "      \n",
        "    gen_input = tf.placeholder(tf.float32, shape=[batch_size,height, width, channels],name=\"input_noise\")\n",
        "    disc_input = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels], name=\"input_clean\")\n",
        "    training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "    print('Taille de gen_input : '+str(gen_input.get_shape()))\n",
        "    \n",
        "    import numpy as np\n",
        "    \n",
        "    generator,gen_vars,gen_saver = generateur(gen_input,[[2]+[3],[3],[2]+[3]],[50,100,150],[4,4,2],['mean','mean','mean'])\n",
        "    couche,kernelBiasList,saver = discriminatorDense(generator,[150,300,150],'Lien')\n",
        "    nbCouchesDsicriminateur = 15\n",
        "    disc_faux,disc_vars,disc_saver = discriminator(generator,np.linspace(int(generator.get_shape()[1]),199,nbCouchesDsicriminateur+1,dtype=np.int)[1:],np.linspace(int(generator.get_shape()[-1]),3,nbCouchesDsicriminateur,dtype=np.int),'Faux')\n",
        "    \n",
        "    imageEntree = tf.summary.image(\"Image_entree\",gen_input)\n",
        "    imageSortie = tf.summary.image(\"Image_sortie\",disc_faux)\n",
        "    \n",
        "    print('Taille de sortie disc_faux : ',end='')\n",
        "    print(disc_faux.get_shape())\n",
        "    print('Taille de sortie disc_input : ',end='')\n",
        "    print(disc_input.get_shape())\n",
        "    \n",
        "    difference = disc_faux-gen_input\n",
        "    disc_loss = tf.reduce_mean(difference)\n",
        "    \n",
        "    maximumGradient = tf.reduce_max(difference)\n",
        "    minimumGradient = tf.reduce_min(difference)\n",
        "    meanGradient = tf.reduce_mean(difference)\n",
        "    maximumSortie = tf.reduce_max(disc_faux)\n",
        "    minimumSortie = tf.reduce_min(disc_faux)\n",
        "    meanSortie = tf.reduce_mean(disc_faux)\n",
        "  \n",
        "    loss_saver_disc = tf.summary.scalar(\"Loss\",disc_loss)\n",
        "    maximumGradientSaver = tf.summary.scalar(\"Maximum_difference\",maximumGradient)\n",
        "    minimumGradientSaver = tf.summary.scalar(\"Minimum_difference\",minimumGradient)\n",
        "    meanGradientSaver = tf.summary.scalar(\"Moyenne_difference\",meanGradient)\n",
        "    maximumSortieSaver = tf.summary.scalar(\"Maximum_sortie\",maximumSortie)\n",
        "    minimumSortieSaver = tf.summary.scalar(\"Minimum_sortie\",minimumSortie)\n",
        "    meanSortieSaver = tf.summary.scalar(\"Moyenne_sortie\",meanSortie)\n",
        "    \n",
        "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
        "    \n",
        "    saver =  disc_saver\n",
        "#     print(\"Trainable variables : \"+str(tf.trainable_variables()))\n",
        "    train_disc = optimizer_disc.minimize(disc_loss)\n",
        "    init = tf.global_variables_initializer()\n",
        "    \n",
        "    tf_saver = tf.train.Saver()\n",
        "    summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
        "\n",
        "    import os\n",
        "    with tf.Session() as sess:\n",
        "        sess = tf_debug.TensorBoardDebugWrapperSession(sess, 'ubuntu:6064')\n",
        "        tf.global_variables_initializer().run()\n",
        "        if os.path.isdir('model/') == True and forceInit == False:\n",
        "          print(os.listdir('./model'))\n",
        "          tf_saver.restore(sess, 'model/model'+nom+'.ckpt')\n",
        "        else :\n",
        "          print('No previous training found...')\n",
        "          init.run()\n",
        "        print('Entrainement....')\n",
        "        \n",
        "        gen_inpt, disc_inpt = next_batch(batch_size, noises,images,height)\n",
        "        entree = imageEntree.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
        "        print()\n",
        "        summary_writer.add_summary(entree,54)\n",
        "        sortie = imageSortie.eval(feed_dict={gen_input: gen_inpt, disc_input: disc_inpt,training: False})\n",
        "        summary_writer.add_summary(sortie,54)\n",
        "summary_writer.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Taille de gen_input : (7, 199, 199, 3)\n",
            "Taille de l'entrée du discriminateur : (7, 50, 50, 150)\n",
            "Taille de sortie disc_faux : (7, 199, 199, 3)\n",
            "Taille de sortie disc_input : (7, 199, 199, 3)\n",
            "['modelEssai3.ckpt.data-00000-of-00001', 'modelEssai3.ckpt.index', 'modelEssai3.ckpt.meta', 'modelEssai4.ckpt.data-00000-of-00001', 'modelEssai4.ckpt.index', 'modelEssai4.ckpt.meta', 'modelEssai5.ckpt.data-00000-of-00001', 'modelEssai5.ckpt.index', 'modelEssai5.ckpt.meta', 'modelEssai7T.ckpt.data-00000-of-00001', 'modelEssai7T.ckpt.index', 'modelEssai7T.ckpt.meta', 'modelEssai8.ckpt.data-00000-of-00001', 'modelEssai8.ckpt.index', 'modelEssai8.ckpt.meta', 'modelEssai9.ckpt.data-00000-of-00001', 'modelEssai9.ckpt.index', 'modelEssai9.ckpt.meta', 'modelEssai10.ckpt.data-00000-of-00001', 'modelEssai10.ckpt.index', 'modelEssai10.ckpt.meta', 'modelEssai11_test.ckpt.data-00000-of-00001', 'modelEssai11_test.ckpt.index', 'modelEssai11_test.ckpt.meta', 'modelEssai11.ckpt.data-00000-of-00001', 'modelEssai11.ckpt.index', 'modelEssai11.ckpt.meta', 'modelEssai12.ckpt.data-00000-of-00001', 'modelEssai12.ckpt.index', 'modelEssai12.ckpt.meta', 'modelEssaiC1.ckpt.data-00000-of-00001', 'modelEssaiC1.ckpt.meta', 'modelEssaiC1.ckpt.index', 'modelEssaiC2.ckpt.index', 'modelEssaiC2.ckpt.meta', 'modelEssaiC2.ckpt.data-00000-of-00001', 'modelEssaiC3.ckpt.meta', 'modelEssaiC3.ckpt.index', 'modelEssaiC3.ckpt.data-00000-of-00001', 'modelEssaiC4.ckpt.index', 'modelEssaiC4.ckpt.data-00000-of-00001', 'modelEssaiC4.ckpt.meta', 'modelEssaiC5.ckpt.index', 'modelEssaiC5.ckpt.data-00000-of-00001', 'modelEssaiC5.ckpt.meta', 'modelEssaiC56.ckpt.data-00000-of-00001', 'modelEssaiC56.ckpt.meta', 'modelEssaiC56.ckpt.index', 'modelEssaiC1 (1).ckpt.data-00000-of-00001', 'modelEssaiC1 (1).ckpt.index', 'modelEssaiC1 (1).ckpt.meta', 'modelEssaiC2 (1).ckpt.data-00000-of-00001', 'modelEssaiC2 (1).ckpt.index', 'modelEssaiC2 (1).ckpt.meta', 'modelEssaiC3 (1).ckpt.data-00000-of-00001', 'modelEssaiC3 (1).ckpt.index', 'modelEssaiC3 (1).ckpt.meta', 'modelEssaiC4 (1).ckpt.data-00000-of-00001', 'modelEssaiC4 (1).ckpt.index', 'modelEssaiC4 (1).ckpt.meta', 'modelEssaiC5 (1).ckpt.data-00000-of-00001', 'modelEssaiC5 (1).ckpt.index', 'modelEssaiC5 (1).ckpt.meta', 'modelEssaiC56 (1).ckpt.data-00000-of-00001', 'modelEssaiC56 (1).ckpt.index', 'modelEssaiC56 (1).ckpt.meta', 'modelEssaiC7.ckpt.data-00000-of-00001', 'modelEssaiC7.ckpt.index', 'modelEssaiC7.ckpt.meta', 'modelEssaiC8.ckpt.data-00000-of-00001', 'modelEssaiC8.ckpt.index', 'modelEssaiC8.ckpt.meta', 'modelEssaiCdeux0.ckpt.data-00000-of-00001', 'modelEssaiCdeux0.ckpt.index', 'modelEssaiCdeux0.ckpt.meta', 'modelEssaiCdeux1.ckpt.data-00000-of-00001', 'modelEssaiCdeux1.ckpt.index', 'modelEssaiCdeux1.ckpt.meta', 'modelEssaiCdeux2.ckpt.data-00000-of-00001', 'modelEssaiCdeux2.ckpt.index', 'modelEssaiCdeux2.ckpt.meta', 'modelEssaiCdeux3.ckpt.data-00000-of-00001', 'modelEssaiCdeux3.ckpt.index', 'modelEssaiCdeux3.ckpt.meta', 'modelEssaiCdeux4.ckpt.data-00000-of-00001', 'modelEssaiCdeux4.ckpt.index', 'modelEssaiCdeux4.ckpt.meta', 'modelEssaiCdeux5.ckpt.data-00000-of-00001', 'modelEssaiCdeux5.ckpt.index', 'modelEssaiCdeux5.ckpt.meta', 'modelEssaiCdeux6.ckpt.data-00000-of-00001', 'modelEssaiCdeux6.ckpt.index', 'modelEssaiCdeux6.ckpt.meta', 'modelEssaiCdeux7.ckpt.data-00000-of-00001', 'modelEssaiCdeux7.ckpt.index', 'modelEssaiCdeux7.ckpt.meta', 'modelEssaiCdeux8.ckpt.data-00000-of-00001', 'modelEssaiCdeux8.ckpt.index', 'modelEssaiCdeux8.ckpt.meta', 'modelEssaiCdeux9.ckpt.data-00000-of-00001', 'modelEssaiCdeux9.ckpt.index', 'modelEssaiCdeux9.ckpt.meta', 'modelEssaiCdeux10.ckpt.data-00000-of-00001', 'modelEssaiCdeux10.ckpt.index', 'modelEssaiCdeux10.ckpt.meta', 'modelEssaiCdeux11.ckpt.data-00000-of-00001', 'modelEssaiCdeux11.ckpt.index', 'modelEssaiCdeux11.ckpt.meta', 'modelEssaiCdeux12.ckpt.data-00000-of-00001', 'modelEssaiCdeux12.ckpt.index', 'modelEssaiCdeux12.ckpt.meta', 'modelEssaiCdeux13.ckpt.data-00000-of-00001', 'modelEssaiCdeux13.ckpt.index', 'modelEssaiCdeux13.ckpt.meta', 'modelEssaiCdeux14.ckpt.data-00000-of-00001', 'modelEssaiCdeux14.ckpt.index', 'modelEssaiCdeux14.ckpt.meta', 'modelEssaiCdeux15.ckpt.data-00000-of-00001', 'modelEssaiCdeux15.ckpt.index', 'modelEssaiCdeux15.ckpt.meta', 'modelEssaiCdeuxPooling0.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPooling0.ckpt.index', 'modelEssaiCdeuxPooling0.ckpt.meta', 'modelEssaiCdeuxPooling1.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPooling1.ckpt.index', 'modelEssaiCdeuxPooling1.ckpt.meta', 'modelEssaiCdeuxPooling2.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPooling2.ckpt.index', 'modelEssaiCdeuxPooling2.ckpt.meta', 'modelEssaiCdeuxPooling3.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPooling3.ckpt.index', 'modelEssaiCdeuxPooling3.ckpt.meta', 'modelEssaiCdeuxPooling4.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPooling4.ckpt.index', 'modelEssaiCdeuxPooling4.ckpt.meta', 'modelEssaiCdeuxPooling5.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPooling5.ckpt.index', 'modelEssaiCdeuxPooling5.ckpt.meta', 'modelEssaiCdeuxPooling6.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPooling6.ckpt.index', 'modelEssaiCdeuxPooling6.ckpt.meta', 'modelEssaiCdeuxPooling7.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPooling7.ckpt.index', 'modelEssaiCdeuxPooling7.ckpt.meta', 'modelEssaiCdeuxPooling8.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPooling8.ckpt.index', 'modelEssaiCdeuxPooling8.ckpt.meta', 'modelEssaiCdeuxPooling9.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPooling9.ckpt.index', 'modelEssaiCdeuxPooling9.ckpt.meta', 'modelEssaiCdeux0avecC0_2_C1_2.ckpt.data-00000-of-00001', 'modelEssaiCdeux0avecC0_2_C1_2.ckpt.index', 'modelEssaiCdeux0avecC0_2_C1_2.ckpt.meta', 'modelEssaiCdeux1avecC0_2_C1_4.ckpt.data-00000-of-00001', 'modelEssaiCdeux1avecC0_2_C1_4.ckpt.index', 'modelEssaiCdeux1avecC0_2_C1_4.ckpt.meta', 'modelEssaiCdeuxPooling10.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPooling10.ckpt.index', 'modelEssaiCdeuxPooling10.ckpt.meta', 'modelEssaiCdeuxPoolingDense0avecC0_2_C1_2_p_2.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense0avecC0_2_C1_2_p_2.ckpt.index', 'modelEssaiCdeuxPoolingDense0avecC0_2_C1_2_p_2.ckpt.meta', 'modelEssaiCdeux2avecC0_2_C1_6.ckpt.data-00000-of-00001', 'modelEssaiCdeux2avecC0_2_C1_6.ckpt.index', 'modelEssaiCdeux2avecC0_2_C1_6.ckpt.meta', 'modelEssaiCdeuxPoolingDense0avecC0_2_C1_2_p_3.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense0avecC0_2_C1_2_p_3.ckpt.index', 'modelEssaiCdeuxPoolingDense0avecC0_2_C1_2_p_3.ckpt.meta', 'modelEssaiCdeuxPoolingDense1avecC0_2_C1_4_p_2.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense1avecC0_2_C1_4_p_2.ckpt.index', 'modelEssaiCdeuxPoolingDense1avecC0_2_C1_4_p_2.ckpt.meta', 'modelEssaiCdeux3avecC0_2_C1_8.ckpt.data-00000-of-00001', 'modelEssaiCdeux3avecC0_2_C1_8.ckpt.index', 'modelEssaiCdeux3avecC0_2_C1_8.ckpt.meta', 'modelEssaiCdeuxPoolingDense1avecC0_2_C1_4_p_3.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense1avecC0_2_C1_4_p_3.ckpt.index', 'modelEssaiCdeuxPoolingDense1avecC0_2_C1_4_p_3.ckpt.meta', 'modelEssaiCdeuxPooling11.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPooling11.ckpt.index', 'modelEssaiCdeuxPooling11.ckpt.meta', 'modelEssaiCdeuxPoolingDense2avecC0_2_C1_6_p_2.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense2avecC0_2_C1_6_p_2.ckpt.index', 'modelEssaiCdeuxPoolingDense2avecC0_2_C1_6_p_2.ckpt.meta', 'modelEssaiCdeuxPoolingDense2avecC0_2_C1_6_p_3.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense2avecC0_2_C1_6_p_3.ckpt.index', 'modelEssaiCdeuxPoolingDense2avecC0_2_C1_6_p_3.ckpt.meta', 'modelEssaiCdeux4avecC0_2_C1_10.ckpt.data-00000-of-00001', 'modelEssaiCdeux4avecC0_2_C1_10.ckpt.index', 'modelEssaiCdeux4avecC0_2_C1_10.ckpt.meta', 'modelEssaiCdeuxPoolingDense3avecC0_2_C1_8_p_2.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense3avecC0_2_C1_8_p_2.ckpt.index', 'modelEssaiCdeuxPoolingDense3avecC0_2_C1_8_p_2.ckpt.meta', 'modelEssaiCdeuxPoolingDense3avecC0_2_C1_8_p_3.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense3avecC0_2_C1_8_p_3.ckpt.index', 'modelEssaiCdeuxPoolingDense3avecC0_2_C1_8_p_3.ckpt.meta', 'modelEssaiCdeux5avecC0_2_C1_12.ckpt.data-00000-of-00001', 'modelEssaiCdeux5avecC0_2_C1_12.ckpt.index', 'modelEssaiCdeux5avecC0_2_C1_12.ckpt.meta', 'modelEssaiCdeuxPoolingDense4avecC0_2_C1_10_p_2.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense4avecC0_2_C1_10_p_2.ckpt.index', 'modelEssaiCdeuxPoolingDense4avecC0_2_C1_10_p_2.ckpt.meta', 'modelEssaiCdeuxPoolingDense4avecC0_2_C1_10_p_3.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense4avecC0_2_C1_10_p_3.ckpt.index', 'modelEssaiCdeuxPoolingDense4avecC0_2_C1_10_p_3.ckpt.meta', 'modelEssaiCdeux6avecC0_2_C1_2.ckpt.data-00000-of-00001', 'modelEssaiCdeux6avecC0_2_C1_2.ckpt.index', 'modelEssaiCdeux6avecC0_2_C1_2.ckpt.meta', 'modelEssaiCdeuxPoolingDense5avecC0_2_C1_2_p_2.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense5avecC0_2_C1_2_p_2.ckpt.index', 'modelEssaiCdeuxPoolingDense5avecC0_2_C1_2_p_2.ckpt.meta', 'modelEssaiCdeuxPoolingDense5avecC0_2_C1_2_p_3.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense5avecC0_2_C1_2_p_3.ckpt.index', 'modelEssaiCdeuxPoolingDense5avecC0_2_C1_2_p_3.ckpt.meta', 'modelEssaiCdeux7avecC0_2_C1_4.ckpt.data-00000-of-00001', 'modelEssaiCdeux7avecC0_2_C1_4.ckpt.index', 'modelEssaiCdeux7avecC0_2_C1_4.ckpt.meta', 'modelEssaiCdeuxPoolingDense6avecC0_2_C1_4_p_2.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense6avecC0_2_C1_4_p_2.ckpt.index', 'modelEssaiCdeuxPoolingDense6avecC0_2_C1_4_p_2.ckpt.meta', 'modelEssaiCdeuxPooling12.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPooling12.ckpt.index', 'modelEssaiCdeuxPooling12.ckpt.meta', 'modelEssaiCdeux8avecC0_2_C1_6.ckpt.data-00000-of-00001', 'modelEssaiCdeux8avecC0_2_C1_6.ckpt.index', 'modelEssaiCdeux8avecC0_2_C1_6.ckpt.meta', 'modelEssaiCdeuxPoolingDense6avecC0_2_C1_4_p_3.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense6avecC0_2_C1_4_p_3.ckpt.index', 'modelEssaiCdeuxPoolingDense6avecC0_2_C1_4_p_3.ckpt.meta', 'modelEssaiCdeuxPoolingDense7avecC0_2_C1_6_p_2.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense7avecC0_2_C1_6_p_2.ckpt.index', 'modelEssaiCdeuxPoolingDense7avecC0_2_C1_6_p_2.ckpt.meta', 'modelEssaiCdeux9avecC0_2_C1_8.ckpt.data-00000-of-00001', 'modelEssaiCdeux9avecC0_2_C1_8.ckpt.index', 'modelEssaiCdeux9avecC0_2_C1_8.ckpt.meta', 'modelEssaiCdeuxPooling13.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPooling13.ckpt.index', 'modelEssaiCdeuxPooling13.ckpt.meta', 'modelEssaiCdeuxPoolingDense7avecC0_2_C1_6_p_3.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense7avecC0_2_C1_6_p_3.ckpt.index', 'modelEssaiCdeuxPoolingDense7avecC0_2_C1_6_p_3.ckpt.meta', 'modelEssaiCdeuxPoolingDense8avecC0_2_C1_8_p_2.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense8avecC0_2_C1_8_p_2.ckpt.index', 'modelEssaiCdeuxPoolingDense8avecC0_2_C1_8_p_2.ckpt.meta', 'modelEssaiCdeux10avecC0_2_C1_10.ckpt.data-00000-of-00001', 'modelEssaiCdeux10avecC0_2_C1_10.ckpt.index', 'modelEssaiCdeux10avecC0_2_C1_10.ckpt.meta', 'modelEssaiCdeuxPoolingDense8avecC0_2_C1_8_p_3.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense8avecC0_2_C1_8_p_3.ckpt.index', 'modelEssaiCdeuxPoolingDense8avecC0_2_C1_8_p_3.ckpt.meta', 'modelEssaiCdeuxPooling14.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPooling14.ckpt.index', 'modelEssaiCdeuxPooling14.ckpt.meta', 'modelEssaiCdeuxPoolingDense9avecC0_2_C1_10_p_2.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense9avecC0_2_C1_10_p_2.ckpt.index', 'modelEssaiCdeuxPoolingDense9avecC0_2_C1_10_p_2.ckpt.meta', 'modelEssaiCdeux11avecC0_2_C1_12.ckpt.data-00000-of-00001', 'modelEssaiCdeux11avecC0_2_C1_12.ckpt.index', 'modelEssaiCdeux11avecC0_2_C1_12.ckpt.meta', 'modelEssaiCdeuxPoolingDense9avecC0_2_C1_10_p_3.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense9avecC0_2_C1_10_p_3.ckpt.index', 'modelEssaiCdeuxPoolingDense9avecC0_2_C1_10_p_3.ckpt.meta', 'modelEssaiCdeuxPooling15.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPooling15.ckpt.index', 'modelEssaiCdeuxPooling15.ckpt.meta', 'modelEssaiCdeuxPoolingDense10avecC0_2_C1_12_p_2.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense10avecC0_2_C1_12_p_2.ckpt.index', 'modelEssaiCdeuxPoolingDense10avecC0_2_C1_12_p_2.ckpt.meta', 'modelEssaiCdeux12avecC0_2_C1_14.ckpt.data-00000-of-00001', 'modelEssaiCdeux12avecC0_2_C1_14.ckpt.index', 'modelEssaiCdeux12avecC0_2_C1_14.ckpt.meta', 'modelEssaiCdeuxPoolingDense10avecC0_2_C1_12_p_3.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense10avecC0_2_C1_12_p_3.ckpt.index', 'modelEssaiCdeuxPoolingDense10avecC0_2_C1_12_p_3.ckpt.meta', 'modelEssaiCdeuxPooling16.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPooling16.ckpt.index', 'modelEssaiCdeuxPooling16.ckpt.meta', 'modelEssaiCdeuxPoolingDense11avecC0_2_C1_14_p_2.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense11avecC0_2_C1_14_p_2.ckpt.index', 'modelEssaiCdeuxPoolingDense11avecC0_2_C1_14_p_2.ckpt.meta', 'modelEssaiCdeux13avecC0_2_C1_16.ckpt.data-00000-of-00001', 'modelEssaiCdeux13avecC0_2_C1_16.ckpt.index', 'modelEssaiCdeux13avecC0_2_C1_16.ckpt.meta', 'modelEssaiCdeux14avecC0_4_C1_2.ckpt.data-00000-of-00001', 'modelEssaiCdeux14avecC0_4_C1_2.ckpt.index', 'modelEssaiCdeux14avecC0_4_C1_2.ckpt.meta', 'modelEssaiCdeuxPoolingDense11avecC0_2_C1_14_p_3.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPoolingDense11avecC0_2_C1_14_p_3.ckpt.index', 'modelEssaiCdeuxPoolingDense11avecC0_2_C1_14_p_3.ckpt.meta', 'modelEssaiCdeuxPooling17.ckpt.data-00000-of-00001', 'modelEssaiCdeuxPooling17.ckpt.index', 'modelEssaiCdeuxPooling17.ckpt.meta', 'modelEssaiCdeux15avecC0_4_C1_4.ckpt.data-00000-of-00001', 'modelEssaiCdeux15avecC0_4_C1_4.ckpt.index', 'modelEssaiCdeux15avecC0_4_C1_4.ckpt.meta', 'modelEssaiCdeux16avecC0_4_C1_6.ckpt.data-00000-of-00001', 'modelEssaiCdeux16avecC0_4_C1_6.ckpt.index', 'modelEssaiCdeux16avecC0_4_C1_6.ckpt.meta', 'modelCtestCouches0_2.ckpt.data-00000-of-00001', 'modelCtestCouches0_2.ckpt.index', 'modelCtestCouches0_2.ckpt.meta', 'modelCtestCouches1_3.ckpt.data-00000-of-00001', 'modelCtestCouches1_3.ckpt.index', 'modelCtestCouches1_3.ckpt.meta', 'modelCtestCouches2_4.ckpt.data-00000-of-00001', 'modelCtestCouches2_4.ckpt.index', 'modelCtestCouches2_4.ckpt.meta', 'modelCtestCouches8_10.ckpt.data-00000-of-00001', 'modelCtestCouches8_10.ckpt.index', 'modelCtestCouches8_10.ckpt.meta', 'modelCtestCouchesFiltres_10filtres_8_10.ckpt.data-00000-of-00001', 'modelCtestCouchesFiltres_10filtres_8_10.ckpt.index', 'modelCtestCouchesFiltres_10filtres_8_10.ckpt.meta', 'modelCdense_2.ckpt.data-00000-of-00001', 'modelCdense_2.ckpt.index', 'modelCdense_2.ckpt.meta', 'modelCdense_3.ckpt.data-00000-of-00001', 'modelCdense_3.ckpt.index', 'modelCdense_3.ckpt.meta', 'modelCdense_4.ckpt.data-00000-of-00001', 'modelCdense_4.ckpt.index', 'modelCdense_4.ckpt.meta', 'modelCdense_5.ckpt.data-00000-of-00001', 'modelCdense_5.ckpt.index', 'modelCdense_5.ckpt.meta', 'modelCdense_5_corrige.ckpt.data-00000-of-00001', 'modelCdense_5_corrige.ckpt.index', 'modelCdense_5_corrige.ckpt.meta', 'modelEssaiLotRemanie5_corrige.ckpt.data-00000-of-00001', 'modelEssaiLotRemanie5_corrige.ckpt.index', 'modelEssaiLotRemanie5_corrige.ckpt.meta', 'modelEssaiLotRemaniePlusNeurones5.ckpt.data-00000-of-00001', 'modelEssaiLotRemaniePlusNeurones5.ckpt.index', 'modelEssaiLotRemaniePlusNeurones5.ckpt.meta', 'modelGAN_essai_0.ckpt.data-00000-of-00001', 'modelGAN_essai_0.ckpt.index', 'modelGAN_essai_0.ckpt.meta', 'modelGAN_essai_1.ckpt.data-00000-of-00001', 'modelGAN_essai_1.ckpt.index', 'modelGAN_essai_1.ckpt.meta', 'modelGAN_essai_test0.ckpt.data-00000-of-00001', 'modelGAN_essai_test0.ckpt.index', 'modelGAN_essai_test0.ckpt.meta', 'modelGAN_essai_test2.ckpt.data-00000-of-00001', 'modelGAN_essai_test2.ckpt.index', 'modelGAN_essai_test2.ckpt.meta', 'modelGAN_essai_test3.ckpt.data-00000-of-00001', 'modelGAN_essai_test3.ckpt.index', 'modelGAN_essai_test3.ckpt.meta', 'modelGAN_essai_test4.ckpt.data-00000-of-00001', 'modelGAN_essai_test4.ckpt.index', 'modelGAN_essai_test4.ckpt.meta', 'modelGAN_essai_test_7.ckpt.index', 'modelGAN_essai_test_7.ckpt.meta', 'modelGAN_essai_test_7.ckpt.data-00000-of-00001', 'modelGAN_essai_8.ckpt.data-00000-of-00001', 'modelGAN_essai_8.ckpt.index', 'modelGAN_essai_8.ckpt.meta', 'modelGAN_essai_9.ckpt.data-00000-of-00001', 'modelGAN_essai_9.ckpt.index', 'modelGAN_essai_9.ckpt.meta', 'modelGAN_essai_10.ckpt.data-00000-of-00001', 'modelGAN_essai_10.ckpt.index', 'modelGAN_essai_10.ckpt.meta', 'modelGAN_essai_11.ckpt.data-00000-of-00001', 'modelGAN_essai_11.ckpt.index', 'modelGAN_essai_11.ckpt.meta', 'modelGAN_essai_12.ckpt.index', 'modelGAN_essai_12.ckpt.meta', 'modelGAN_essai_12.ckpt.data-00000-of-00001', 'modelGAN_essai_13.ckpt.data-00000-of-00001', 'modelGAN_essai_13.ckpt.index', 'modelGAN_essai_13.ckpt.meta', 'modelGAN_essai_17_learning_rate_0.002_reduce_max.ckpt.data-00000-of-00001', 'modelGAN_essai_17_learning_rate_0.002_reduce_max.ckpt.index', 'modelGAN_essai_17_learning_rate_0.002_reduce_max.ckpt.meta', 'modelAutodencodeur_N_mean_2_10e-6.ckpt.index', 'modelAutodencodeur_N_mean_2_10e-6.ckpt.meta', 'modelAutodencodeur_N_mean_2_10e-6.ckpt.data-00000-of-00001', 'modelAutodencodeur_N_mean_1_10e-6.ckpt.index', 'modelAutodencodeur_N_mean_1_10e-6.ckpt.meta', 'modelAutodencodeur_N_mean_1_10e-6.ckpt.data-00000-of-00001', 'modelAutodencodeur_N_mean_1_10e-4.ckpt.index', 'modelAutodencodeur_N_mean_1_10e-4.ckpt.meta', 'modelAutodencodeur_N_mean_1_10e-4.ckpt.data-00000-of-00001', 'modelAutodencodeur_C_mean_1_10e-4.ckpt.index', 'modelAutodencodeur_C_mean_1_10e-4.ckpt.meta', 'modelAutodencodeur_C_mean_1_10e-4.ckpt.data-00000-of-00001', 'modelAutodencodeur_N_mean_1_10e-2.ckpt.data-00000-of-00001', 'modelAutodencodeur_N_mean_1_10e-2.ckpt.index', 'modelAutodencodeur_N_mean_1_10e-2.ckpt.meta', 'modelAutodencodeur_C_mean_1_10e-2.ckpt.data-00000-of-00001', 'modelAutodencodeur_C_mean_1_10e-2.ckpt.index', 'modelAutodencodeur_C_mean_1_10e-2.ckpt.meta', 'checkpoint']\n",
            "INFO:tensorflow:Restoring parameters from model/modelAutodencodeur_N_mean_1_10e-2.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "_Rendezvous",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_Rendezvous\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-ab7597084284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model/'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mforceInit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m           \u001b[0mtf_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model/model'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnom\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No previous training found...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1276\u001b[0;31m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1277\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m       \u001b[0;31m# There are three common conditions that might cause this error:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/debug/wrappers/grpc_wrapper.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata, callable_runner, callable_runner_args, callable_options)\u001b[0m\n\u001b[1;32m    221\u001b[0m       self._sent_graph_version = publish_traceback(\n\u001b[1;32m    222\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grpc_debug_server_urls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m           self._sent_graph_version)\n\u001b[0m\u001b[1;32m    224\u001b[0m     return super(TensorBoardDebugWrapperSession, self).run(\n\u001b[1;32m    225\u001b[0m         \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/debug/wrappers/grpc_wrapper.py\u001b[0m in \u001b[0;36mpublish_traceback\u001b[0;34m(debug_server_urls, graph, feed_dict, fetches, old_graph_version)\u001b[0m\n\u001b[1;32m     61\u001b[0m     source_remote.send_graph_tracebacks(\n\u001b[1;32m     62\u001b[0m         \u001b[0mdebug_server_urls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         send_source=True)\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/debug/lib/source_remote.py\u001b[0m in \u001b[0;36msend_graph_tracebacks\u001b[0;34m(destinations, run_key, origin_stack, graph, send_source)\u001b[0m\n\u001b[1;32m    207\u001b[0m   _send_call_tracebacks(\n\u001b[1;32m    208\u001b[0m       \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_eager_execution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m       graph=graph, send_source=send_source)\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/debug/lib/source_remote.py\u001b[0m in \u001b[0;36m_send_call_tracebacks\u001b[0;34m(destinations, origin_stack, is_eager_execution, call_key, graph, send_source)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mchannel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsecure_channel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mstub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdebug_service_pb2_grpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEventListenerStub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mstub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSendTracebacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_traceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msend_source\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m       for path, source_files in zip(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwith_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_Rendezvous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31m_Rendezvous\u001b[0m: <_Rendezvous of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"Name resolution failure\"\n\tdebug_error_string = \"{\"created\":\"@1553283398.789951737\",\"description\":\"Failed to create subchannel\",\"file\":\"src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":2636,\"referenced_errors\":[{\"created\":\"@1553283398.789947589\",\"description\":\"Name resolution failure\",\"file\":\"src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":2941,\"grpc_status\":14}]}\"\n>"
          ]
        }
      ]
    }
  ]
}